[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Alvaro Manzanas Portfolio",
    "section": "",
    "text": "Let me introduce myself, my name is Álvaro Manzanas, I am a Junior Data Analysis, Scientist, Psychologist with a background in Computer Systems Administration and Virtualization. Eager to learn and improve professionally in the world of data analysis, with curiosity and interest to know the knowledge they hide. I am characterized as an analytical person with logical thinking, as well as being creative and curious, which combines well with the organization and meticulousness with which I excel.\nI hope you’ll take a look at what I’ve been working on, you might find it interesting. And if you like to connect with me, do not hesitate to do so, here I provide some buttons where you will find how to do it."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Alvaro Manzanas",
    "section": "",
    "text": "projects"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Alvaro Manzanas",
    "section": "",
    "text": "CV"
  },
  {
    "objectID": "12_ExpDatAn.html",
    "href": "12_ExpDatAn.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "library(tidyverse, quietly = TRUE)\nlibrary(skimr, quietly = TRUE)\nlibrary(statsr, quietly = TRUE)\nlibrary(GGally, quietly = TRUE)\nlibrary(corrplot, quietly = TRUE)\n\nDataset: https://github.com/fivethirtyeight/data/tree/master/college-majors\n\n\n\n\n\n\n\nHeader\nDescription\n\n\n\n\nRank\nRank by median earnings\n\n\nMajor_code\nMajor code, FO1DP in ACS PUMS\n\n\nMajor\nMajor description\n\n\nMajor_category\nCategory of major from Carnevale et al\n\n\nTotal\nTotal number of people with major\n\n\nSample_size\nSample size (unweighted) of full-time, year-round ONLY (used for earnings)\n\n\nMen\nMale graduates\n\n\nWomen\nFemale graduates\n\n\nShareWomen\nWomen as share of total\n\n\nEmployed\nNumber employed (ESR == 1 or 2)\n\n\nFull_time\nEmployed 35 hours or more\n\n\nPart_time\nEmployed less than 35 hours\n\n\nFull_time_year_round\nEmployed at least 50 weeks (WKW == 1) and at least 35 hours (WKHP &gt;= 35)\n\n\nUnemployed\nNumber unemployed (ESR == 3)\n\n\nUnemployment_rate\nUnemployed / (Unemployed + Employed)\n\n\nMedian\nMedian earnings of full-time, year-round workers\n\n\nP25th\n25th percentile of earnings\n\n\nP75th\n75th percentile of earnings\n\n\nCollege_jobs\nNumber with job requiring a college degree\n\n\nNon_college_jobs\nNumber with job not requiring a college degree\n\n\nLow_wage_jobs\nNumber in low-wage service jobs\n\n\n\nThe intent of this section is to go over a practical project, following the steps: loading the dataset, understanding the data, treating missing values, exploring and visualizing, and the analysis report."
  },
  {
    "objectID": "12_ExpDatAn.html#univariate-analysis",
    "href": "12_ExpDatAn.html#univariate-analysis",
    "title": "Exploratory Data Analysis",
    "section": "Univariate Analysis",
    "text": "Univariate Analysis\nIt is to look one variable at a time. Let’s graph some histograms of the numeric variables with a for loop.\n\nfor (var in colnames(select_if(df.clean, is.numeric)) ) {\n    g = ggplot(df.clean) + \n        geom_histogram( aes( unlist(df.clean[, var]) ), bins=20,\n                        fill = \"darkblue\", color = \"lightblue\", alpha = 0.5) +\n        labs( title = paste(\"Histogram of\", var),\n              x = var)\n    plot(g)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd to be able to see outliers the boxplot will be our aproach.\n\nfor ( var in colnames( select_if(df.clean, is.numeric) ) ) {\n    g = ggplot(df.clean) +\n        geom_boxplot( aes( y = unlist(df.clean[, var]) ),\n                      fill = \"lightblue\", color = \"blue\") +\n        labs(title = paste(\"Boxplot of\", var),\n             y = var)\n    plot(g)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can add a QQ plot as a visual indication of normality.\n\nfor (var in colnames( select_if(df.clean, is.numeric)) ) {\n    g = ggplot(df.clean, aes( sample = unlist( df.clean[, var])) ) +\n        stat_qq() +\n        stat_qq_line() +\n        labs( title = paste(\"QQ-plot of\", var) )\n    plot(g)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAt this point we can see that the distributions are right skewed with many outliers. To finish, we can use statistic tests to be sure if a variable distribution is normal.\n\nnorm.stats &lt;- data.frame()\nfor (var in colnames( select_if( df.clean, is.numeric) ) ) {\n    ks &lt;- ks.test( unlist( df.clean[, var] ), y = pnorm) \n    sh &lt;- shapiro.test( unlist( df.clean[, var]) )\n    vals &lt;- data.frame(\n        ks_test = sprintf(\"%6.5f\" ,c(ks$p.value)),\n        shapiro_test = sprintf(\"%6.5f\", c(sh$p.value) ),\n        row.names = var\n    )\n    norm.stats &lt;- bind_rows(norm.stats, vals)\n}\n\nWarning in ks.test.default(unlist(df.clean[, var]), y = pnorm): ties should not\nbe present for the one-sample Kolmogorov-Smirnov test\nWarning in ks.test.default(unlist(df.clean[, var]), y = pnorm): ties should not\nbe present for the one-sample Kolmogorov-Smirnov test\nWarning in ks.test.default(unlist(df.clean[, var]), y = pnorm): ties should not\nbe present for the one-sample Kolmogorov-Smirnov test\nWarning in ks.test.default(unlist(df.clean[, var]), y = pnorm): ties should not\nbe present for the one-sample Kolmogorov-Smirnov test\nWarning in ks.test.default(unlist(df.clean[, var]), y = pnorm): ties should not\nbe present for the one-sample Kolmogorov-Smirnov test\nWarning in ks.test.default(unlist(df.clean[, var]), y = pnorm): ties should not\nbe present for the one-sample Kolmogorov-Smirnov test\nWarning in ks.test.default(unlist(df.clean[, var]), y = pnorm): ties should not\nbe present for the one-sample Kolmogorov-Smirnov test\nWarning in ks.test.default(unlist(df.clean[, var]), y = pnorm): ties should not\nbe present for the one-sample Kolmogorov-Smirnov test\nWarning in ks.test.default(unlist(df.clean[, var]), y = pnorm): ties should not\nbe present for the one-sample Kolmogorov-Smirnov test\nWarning in ks.test.default(unlist(df.clean[, var]), y = pnorm): ties should not\nbe present for the one-sample Kolmogorov-Smirnov test\n\nremove(ks)\nremove(sh)\nremove(vals)\nnorm.stats\n\n                     ks_test shapiro_test\nRank                 0.00000      0.00003\nTotal                0.00000      0.00000\nMen                  0.00000      0.00000\nWomen                0.00000      0.00000\nShareWomen           0.00000      0.00433\nSample_size          0.00000      0.00000\nEmployed             0.00000      0.00000\nFull_time            0.00000      0.00000\nPart_time            0.00000      0.00000\nFull_time_year_round 0.00000      0.00000\nUnemployed           0.00000      0.00000\nUnemployment_rate    0.00000      0.01973\nMedian               0.00000      0.00000\nP25th                0.00000      0.00000\nP75th                0.00000      0.00000\nCollege_jobs         0.00000      0.00000\nNon_college_jobs     0.00000      0.00000\nLow_wage_jobs        0.00000      0.00000"
  },
  {
    "objectID": "12_ExpDatAn.html#multivariate-analysis",
    "href": "12_ExpDatAn.html#multivariate-analysis",
    "title": "Exploratory Data Analysis",
    "section": "Multivariate Analysis",
    "text": "Multivariate Analysis\n\nIn a project, the idea is to explore how the explanatory variables (X) affect the response variable (y).\n\nHere we want to know how the variables affect to ‘unemployment_rate’. For this task could be useful the scatter-plot to see the pattern created when two variables are compared. And the correlation coefficient which tell us how much relation they have.\n\nggpairs(): from GGally returns a matrix with scatterplot and correlations.\n\n\ndf.clean %&gt;%\n    select(Men, Women, Part_time, Unemployment_rate, \n           Non_college_jobs, Low_wage_jobs) %&gt;%\n    ggpairs() \n\n\n\n\n\n\n\n\nThere are only select some variables, it have been discarded the character ones and cherry-pick variables with some correlation. The full correlation matrix could not be seen.\n\ncorrs &lt;- round( cor(df.clean[, -c(1, 2, 3, 7)]), 3)\ncorrplot (corrs, method = \"number\", type = \"lower\", \n          tl.cex = 0.8, number.cex = 0.6)\n\n\n\n\n\n\n\n\nLooking into ‘Unemployment_rate’ the correlations are so lightly, in this case a linear regression would not be the best approach.\nWhen two explanatory variables have high correlations means both variables can explain the same variance in the target variable, creating redundancy and making it more difficult to determine the effect of each individual explanatory feature. Then, it must be eliminated before modeling."
  },
  {
    "objectID": "12_ExpDatAn.html#exploring",
    "href": "12_ExpDatAn.html#exploring",
    "title": "Exploratory Data Analysis",
    "section": "Exploring",
    "text": "Exploring\nWhen we want to explore more deeply the dataset we can start doing questions and looking for answers:\n\nWhat are the top 10 majors with the lower unemployment rate?\nAnd with the higher unemployment rate?\nWhat are the majors with more specialized jobs (requiring college)?\nWhat are the best median-value-paying jobs?\nDo the majors with more share of women enrolled have a higher or lower unemployed rate?\nDo the majors with more share of women have higher rate of part time workers?\nDo the majors with a greater share of women enrolled have a similar salary median?\n\n\nWhat are the top 10 majors with the lower unemployment rate?\n\n# Selecting top 10\ntop10.hig.emp &lt;- df.clean %&gt;%\n  select(Major, Unemployment_rate) %&gt;%\n  arrange( desc(Unemployment_rate) ) %&gt;%\n  head(10)\n\n# Selecting top 10\ntop10.low.emp &lt;- df.clean %&gt;%\n  select(Major, Unemployment_rate) %&gt;%\n  arrange( Unemployment_rate ) %&gt;%\n  head(10)\ntop10.low.emp\n\n# A tibble: 10 × 2\n   Major                                      Unemployment_rate\n   &lt;fct&gt;                                                  &lt;dbl&gt;\n 1 MATHEMATICS AND COMPUTER SCIENCE                     0      \n 2 MILITARY TECHNOLOGIES                                0      \n 3 BOTANY                                               0      \n 4 SOIL SCIENCE                                         0      \n 5 EDUCATIONAL ADMINISTRATION AND SUPERVISION           0      \n 6 ENGINEERING MECHANICS PHYSICS AND SCIENCE            0.00633\n 7 COURT REPORTING                                      0.0117 \n 8 MATHEMATICS TEACHER EDUCATION                        0.0162 \n 9 PETROLEUM ENGINEERING                                0.0184 \n10 GENERAL AGRICULTURE                                  0.0196 \n\n\n\n# plot top 10 higher Unemployment rate\nggplot(top10.hig.emp) +\n  geom_col( aes(x = Unemployment_rate,\n                y = reorder( Major, +Unemployment_rate) ),\n            color = \"darkblue\", fill = \"lightblue\" ) +\n  labs( y = \"Major\" ) +\n  geom_text( aes( x = Unemployment_rate,\n                  y = Major, label = round(Unemployment_rate, 3) ),\n             size = 2, hjust = 1.2) +\n  ggtitle(\"Unemployment rate by Major\")\n\n\n\n\n\n\n\n\n\n# plot top 10 lower Unemployment rate\nggplot(top10.low.emp) +\n  geom_col( aes(x = Unemployment_rate,\n                y = reorder( Major, -Unemployment_rate) ),\n            color = \"darkblue\", fill = \"lightblue\" ) +\n  labs( y = \"Major\" ) +\n  geom_text( aes( x = Unemployment_rate,\n                  y = Major, label = round(Unemployment_rate, 3) ),\n             size = 2, hjust = 1.2) +\n  ggtitle(\"Unemployment rate by Major\")\n\n\n\n\n\n\n\n\nThe first graph of the top 10 unemployment rate looks fine but the second one not at all. Let’s try the same but using the proportion of total employees for ordering the data within unemployment rate.\n\ntop10.hig.prop &lt;- df.clean %&gt;%\n  mutate (prop = Total / sum(Total) ) %&gt;%\n  select (Major, Unemployment_rate, prop) %&gt;%\n  arrange(desc(prop), desc(Unemployment_rate)) %&gt;%\n  head(10)\n\n# plot top 10 higher Unemployment rate by proportion\nggplot(top10.hig.prop) +\n  geom_col( aes(x = Unemployment_rate,\n                y = reorder( Major, +Unemployment_rate) ),\n            color = \"darkblue\", fill = \"lightblue\" ) +\n  labs( y = \"Major\" ) +\n  geom_text( aes( x = Unemployment_rate,\n                  y = Major, label = round(Unemployment_rate, 3) ),\n             size = 2, hjust = 1.2) +\n  ggtitle(\"Unemployment rate by Major\")\n\n\n\n\n\n\n\n\n\ntop10.low.prop &lt;- df.clean %&gt;%\n  mutate (prop = Total / sum(Total) ) %&gt;%\n  select (Major, Unemployment_rate, prop) %&gt;%\n  arrange(desc(prop), Unemployment_rate) %&gt;%\n  head(10)\n\n# plot top 10 lower Unemployment rate by proportion\nggplot(top10.low.prop) +\n  geom_col( aes(x = Unemployment_rate,\n                y = reorder( Major, -Unemployment_rate) ),\n            color = \"darkblue\", fill = \"lightblue\" ) +\n  labs( y = \"Major\" ) +\n  geom_text( aes( x = Unemployment_rate,\n                  y = Major, label = round(Unemployment_rate, 3) ),\n             size = 2, hjust = 1.2) +\n  ggtitle(\"Unemployment rate by Major\")\n\n\n\n\n\n\n\n\nIt would not be necessary the second visualization, It have the same values because it is ordered by proportion. It show the most popular majors ordered by employment rate, and nursing with the best rate.\n\n\nMore specialized jobs\nJust like the last time, we are going to need new columns such as proportions of college jobs:\n\ndf.clean &lt;- df.clean %&gt;%\n  mutate ( College_jobs_pct = College_jobs / (College_jobs + Non_college_jobs),\n           Employees_pct = Total / sum(Total) )\n\n\ndf.clean %&gt;%\n  select(Major_category, Major, College_jobs_pct) %&gt;%\n  group_by(Major_category) %&gt;%\n  summarize(mean_college_jobs_pct = mean(College_jobs_pct)) %&gt;%\n  arrange(desc(mean_college_jobs_pct))\n\n# A tibble: 16 × 2\n   Major_category                      mean_college_jobs_pct\n   &lt;fct&gt;                                               &lt;dbl&gt;\n 1 Education                                           0.714\n 2 Engineering                                         0.680\n 3 Computers & Mathematics                             0.604\n 4 Biology & Life Science                              0.593\n 5 Interdisciplinary                                   0.570\n 6 Physical Sciences                                   0.532\n 7 Health                                              0.516\n 8 Psychology & Social Work                            0.508\n 9 Agriculture & Natural Resources                     0.406\n10 Humanities & Liberal Arts                           0.386\n11 Social Science                                      0.375\n12 Communications & Journalism                         0.348\n13 Arts                                                0.328\n14 Law & Public Policy                                 0.324\n15 Business                                            0.297\n16 Industrial Arts & Consumer Services               NaN    \n\n\n\n\nBest median-value-paying jobs\nFirst we have to add a variable for median salaries grouped by major category, and then use that to generate a boxplot visualization for comparison.\n\ndf.clean &lt;- df.clean %&gt;%\n  group_by(Major_category) %&gt;%\n  mutate( Salary_mdn = median(Median) )\n\nggplot(data = df.clean) +\n  geom_boxplot( aes( x = reorder(Major_category, Salary_mdn),\n                     y = Median),\n                fill = \"lightblue\", color = \"darkblue\") +\n  labs( x = \"Major_category\" ) +\n  ggtitle (\"Median by Major Category\") +\n  theme( axis.text.x = element_text( angle = 45, hjust = 1.2 ) ) +\n  expand_limits( x = c(0, NA), y = c(0, NA) ) +\n  scale_y_continuous( labels = scales::comma )\n\n\n\n\n\n\n\n\nWe can observe that Engineering, Maths, Business, Physics are the top paying jobs.\nOther approaching is to do multiple comparison (post-hoc) of the median.\n\npairwise.wilcox.test(df.clean$Median, df.clean$Major_category, p.adjust.method = \"holm\") %&gt;% suppressWarnings()\n\n\n    Pairwise comparisons using Wilcoxon rank sum test with continuity correction \n\ndata:  df.clean$Median and df.clean$Major_category \n\n                                    Agriculture & Natural Resources Arts \nArts                                1.000                           -    \nBiology & Life Science              1.000                           1.000\nBusiness                            0.801                           0.352\nCommunications & Journalism         1.000                           1.000\nComputers & Mathematics             0.295                           0.542\nEducation                           1.000                           1.000\nEngineering                         0.002                           0.007\nHealth                              1.000                           1.000\nHumanities & Liberal Arts           1.000                           1.000\nIndustrial Arts & Consumer Services 1.000                           1.000\nInterdisciplinary                   1.000                           1.000\nLaw & Public Policy                 1.000                           1.000\nPhysical Sciences                   1.000                           0.759\nPsychology & Social Work            1.000                           1.000\nSocial Science                      1.000                           1.000\n                                    Biology & Life Science Business\nArts                                -                      -       \nBiology & Life Science              -                      -       \nBusiness                            0.884                  -       \nCommunications & Journalism         1.000                  0.988   \nComputers & Mathematics             0.451                  1.000   \nEducation                           0.428                  0.008   \nEngineering                         0.000066               0.042   \nHealth                              1.000                  1.000   \nHumanities & Liberal Arts           0.367                  0.007   \nIndustrial Arts & Consumer Services 1.000                  1.000   \nInterdisciplinary                   1.000                  1.000   \nLaw & Public Policy                 1.000                  1.000   \nPhysical Sciences                   1.000                  1.000   \nPsychology & Social Work            0.884                  0.063   \nSocial Science                      1.000                  1.000   \n                                    Communications & Journalism\nArts                                -                          \nBiology & Life Science              -                          \nBusiness                            -                          \nCommunications & Journalism         -                          \nComputers & Mathematics             0.655                      \nEducation                           1.000                      \nEngineering                         0.143                      \nHealth                              1.000                      \nHumanities & Liberal Arts           1.000                      \nIndustrial Arts & Consumer Services 1.000                      \nInterdisciplinary                   1.000                      \nLaw & Public Policy                 1.000                      \nPhysical Sciences                   1.000                      \nPsychology & Social Work            1.000                      \nSocial Science                      1.000                      \n                                    Computers & Mathematics Education\nArts                                -                       -        \nBiology & Life Science              -                       -        \nBusiness                            -                       -        \nCommunications & Journalism         -                       -        \nComputers & Mathematics             -                       -        \nEducation                           0.004                   -        \nEngineering                         0.025                   0.000007 \nHealth                              1.000                   1.000    \nHumanities & Liberal Arts           0.005                   1.000    \nIndustrial Arts & Consumer Services 1.000                   1.000    \nInterdisciplinary                   1.000                   1.000    \nLaw & Public Policy                 1.000                   0.277    \nPhysical Sciences                   1.000                   0.014    \nPsychology & Social Work            0.057                   1.000    \nSocial Science                      1.000                   1.000    \n                                    Engineering Health\nArts                                -           -     \nBiology & Life Science              -           -     \nBusiness                            -           -     \nCommunications & Journalism         -           -     \nComputers & Mathematics             -           -     \nEducation                           -           -     \nEngineering                         -           -     \nHealth                              0.000499    -     \nHumanities & Liberal Arts           0.000011    0.867 \nIndustrial Arts & Consumer Services 0.023       1.000 \nInterdisciplinary                   1.000       1.000 \nLaw & Public Policy                 0.899       1.000 \nPhysical Sciences                   0.054       1.000 \nPsychology & Social Work            0.001       1.000 \nSocial Science                      0.004       1.000 \n                                    Humanities & Liberal Arts\nArts                                -                        \nBiology & Life Science              -                        \nBusiness                            -                        \nCommunications & Journalism         -                        \nComputers & Mathematics             -                        \nEducation                           -                        \nEngineering                         -                        \nHealth                              -                        \nHumanities & Liberal Arts           -                        \nIndustrial Arts & Consumer Services 1.000                    \nInterdisciplinary                   1.000                    \nLaw & Public Policy                 0.323                    \nPhysical Sciences                   0.017                    \nPsychology & Social Work            1.000                    \nSocial Science                      0.755                    \n                                    Industrial Arts & Consumer Services\nArts                                -                                  \nBiology & Life Science              -                                  \nBusiness                            -                                  \nCommunications & Journalism         -                                  \nComputers & Mathematics             -                                  \nEducation                           -                                  \nEngineering                         -                                  \nHealth                              -                                  \nHumanities & Liberal Arts           -                                  \nIndustrial Arts & Consumer Services -                                  \nInterdisciplinary                   1.000                              \nLaw & Public Policy                 1.000                              \nPhysical Sciences                   1.000                              \nPsychology & Social Work            1.000                              \nSocial Science                      1.000                              \n                                    Interdisciplinary Law & Public Policy\nArts                                -                 -                  \nBiology & Life Science              -                 -                  \nBusiness                            -                 -                  \nCommunications & Journalism         -                 -                  \nComputers & Mathematics             -                 -                  \nEducation                           -                 -                  \nEngineering                         -                 -                  \nHealth                              -                 -                  \nHumanities & Liberal Arts           -                 -                  \nIndustrial Arts & Consumer Services -                 -                  \nInterdisciplinary                   -                 -                  \nLaw & Public Policy                 1.000             -                  \nPhysical Sciences                   1.000             1.000              \nPsychology & Social Work            1.000             1.000              \nSocial Science                      1.000             1.000              \n                                    Physical Sciences Psychology & Social Work\nArts                                -                 -                       \nBiology & Life Science              -                 -                       \nBusiness                            -                 -                       \nCommunications & Journalism         -                 -                       \nComputers & Mathematics             -                 -                       \nEducation                           -                 -                       \nEngineering                         -                 -                       \nHealth                              -                 -                       \nHumanities & Liberal Arts           -                 -                       \nIndustrial Arts & Consumer Services -                 -                       \nInterdisciplinary                   -                 -                       \nLaw & Public Policy                 -                 -                       \nPhysical Sciences                   -                 -                       \nPsychology & Social Work            0.276             -                       \nSocial Science                      1.000             0.775                   \n\nP value adjustment method: holm \n\n\nOn the last output we look for p &lt; 0.05 meaning there are differences between the medians of that groups. With the boxplot we can see which are greater visually but with these comparisons we can actually know which is greater than another statistically.\n\n\nRelatioship between Majors with more share of women and unemployed rate\nThis task should be do it with correlations, and then we can plot to see its graphic representations.\n\n#cor(df.clean$ShareWomen, df.clean$Unemployment_rate, method = \"pearson\")\n\ncor(df.clean$ShareWomen, df.clean$Unemployment_rate,\n    method = \"spearman\")\n\n[1] 0.0663\n\n\nThe spearman correlation is non-parametric and it is suitable for when the relationship of two variables are non-linear, or do not follow the norm distribution.\n\nggplot( data = df.clean ) +\n  geom_point( aes( x = ShareWomen, y = Unemployment_rate ),\n              color = \"darkblue\") +\n  ggtitle (\"Share of women vs. Unemployment rate\") +\n  labs ( subtitle = \"There is no linear relationship, the graphic is spread\", color = \"darkgray\", size = 8) +\n  theme( plot.subtitle = element_text( color = \"darkgray\", \n                                               size = 10) )\n\n\n\n\n\n\n\n\n\n\nDo the majors with more share of women have higher rate of part time workers?\nJust like the last question, we can see the relationship of this two variables with a correlation.\n\ncor(df.clean$ShareWomen, df.clean$Part_time, method = \"spearman\")\n\n[1] 0.3348\n\n\nIt can be seen a medium-slight positive correlation between both Share of woman enrolled and workers with part time contract. This is how looks like the correlation:\n\nggplot(data = df.clean) +\n    geom_point( aes( x = ShareWomen, y = Part_time),\n                color = \"darkblue\") +\n    labs( title = \"Share of Women vs. Part-Time jobs\",\n          subtitle = \"The relationship between the variables is slightly weak [0.33],\\n siggesting that when the share of women is higher also there are more part-jobs\") +\n    theme( plot.subtitle = element_text( color = \"darkgray\", size = 8) )\n\n\n\n\n\n\n\n\n\n\nMajors with a greater share of women and salary median\n\ncor(df.clean$ShareWomen, df.clean$Median)\n\n[1] -0.6187\n\ncor(df.clean$ShareWomen, df.clean$Low_wage_jobs)\n\n[1] 0.1878\n\n\nThe first correlation show when the greater is the share of women enrolled, the less are the Median salary; and in the second correlation, almost without strength if the share of women is high there are a slight relationship (almost none) with more low wage jobs.\n\nggplot(data = df.clean) +\n    geom_point( aes( x = ShareWomen, y = Low_wage_jobs, \n                     color = \"Low Wage Jobs\"),\n                alpha = 1, shape = \"o\", size = 2) +\n    geom_point( aes( x = ShareWomen, y = Median, \n                     color = \"Median Salary\"), alpha = 0.5) +\n    labs(title = \"Share of women vs. low wage, and vs. Median Salary\",\n         y = \"\", color = \"Share of Women vs.:\") +\n    scale_color_manual(values = c(\"Low Wage Jobs\" = \"darkblue\", \n                                  \"Median Salary\" = \"darkred\") ) +\n    theme(legend.title = element_text())\n\n\n\n\n\n\n\n\nThe next step could be to test if the median of lower share of women is greater or not than higher share of women. To verify this we can run hypothesis tests. First we are going to add a column with two categories, one for higher share and other for lower share of women. Then we have to be sure which test we can use, for that we have to check the normality and if its fine, the homoscedasticity.\n\ndf.clean &lt;- df.clean %&gt;% \n    mutate( ShareWomen_cat = ifelse(ShareWomen &gt; 0.5, \"higher\", \"lower\") )\n\ndf.higher.w &lt;- df.clean %&gt;%\n    filter( ShareWomen_cat == \"higher\") %&gt;%\n    select( Major, Major_category, ShareWomen, \n            ShareWomen_cat, Low_wage_jobs, Median)\n\ndf.lower.w &lt;- df.clean %&gt;%\n    filter( ShareWomen_cat == \"lower\") %&gt;%\n    select( Major, Major_category, ShareWomen, \n            ShareWomen_cat, Low_wage_jobs, Median)\n\n\nks.test( df.higher.w$Low_wage_jobs, \"pnorm\") %&gt;% suppressWarnings()\n\n\n    Exact one-sample Kolmogorov-Smirnov test\n\ndata:  df.higher.w$Low_wage_jobs\nD = 0.99, p-value = 0.000000000000001\nalternative hypothesis: two-sided\n\nks.test( df.lower.w$Low_wage_jobs, \"pnorm\" ) %&gt;% suppressWarnings()\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  df.lower.w$Low_wage_jobs\nD = 0.95, p-value &lt;0.0000000000000002\nalternative hypothesis: two-sided\n\nks.test( df.higher.w$Median, \"pnorm\" ) %&gt;% suppressWarnings()\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  df.higher.w$Median\nD = 1, p-value &lt;0.0000000000000002\nalternative hypothesis: two-sided\n\nks.test( df.lower.w$Median, \"pnorm\" ) %&gt;% suppressWarnings()\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  df.lower.w$Median\nD = 1, p-value &lt;0.0000000000000002\nalternative hypothesis: two-sided\n\n\nIn the las output we show that every test was lower than 0.05 thus there are no normality on these groups. Let’s check homoscedasticity:\n\nwith(df.clean, car::leveneTest(Median, ShareWomen_cat) )  %&gt;% suppressWarnings()\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value    Pr(&gt;F)    \ngroup   1    25.4 0.0000012 ***\n      170                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nwith(df.clean, car::leveneTest(Low_wage_jobs, ShareWomen_cat) )  %&gt;% suppressWarnings()\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value Pr(&gt;F)\ngroup   1    1.86   0.17\n      170               \n\n\nThe first result for Median on ShareWomen_cat levels have p &lt; 0.05 then we reject the Null Hypothesis; nevertheless, the second result for Low_wage_jobs on ShareWomen_cat levels the p is 0.17, then we maintain the Null Hypothesis of Homogeneity of Variances.\nNow we know that for compare Median we will need a robust test, but with Low_wage_jobs we might use t-test because the sample is greater than 50 and that will not affect so much the results.\n\nwilcox.test(df.higher.w$Median, df.lower.w$Median)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  df.higher.w$Median and df.lower.w$Median\nW = 1142, p-value = 0.00000000000001\nalternative hypothesis: true location shift is not equal to 0\n\ncat(\"Median of Higher Share Woman: \", median(df.higher.w$Median),\n    \"\\nMedian of Lower Share Woman: \", median(df.lower.w$Median), \"\\n\" )\n\nMedian of Higher Share Woman:  34000 \nMedian of Lower Share Woman:  45000 \n\nt.test(df.higher.w$Low_wage_jobs, df.lower.w$Low_wage_jobs)\n\n\n    Welch Two Sample t-test\n\ndata:  df.higher.w$Low_wage_jobs and df.lower.w$Low_wage_jobs\nt = 1.8, df = 170, p-value = 0.07\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -162.2 3917.3\nsample estimates:\nmean of x mean of y \n     4708      2831 \n\n\nFor Median there are differences between the median of the groups, but for Low Wage seem like there are not differences between the mean of the groups with this sample.\nLet’s try another approach. Now using rep_sample_n() we are going to take 100 samples of 1000 observations allowing repetitions, and then taking the mean.\n\ndf.higher.samp &lt;- df.higher.w %&gt;%\n    rep_sample_n( size = 1000, reps = 200, replace = TRUE ) %&gt;%\n    summarize( mu_Median = mean(Median), \n               mu_Low_wage = mean(Low_wage_jobs) )\n\ndf.lower.samp &lt;- df.lower.w %&gt;%\n    rep_sample_n( size = 1000, reps = 200, replace = TRUE ) %&gt;%\n    summarize( mu_Median = mean(Median), \n               mu_Low_wage = mean(Low_wage_jobs) )\n\n\nshapiro.test(df.lower.samp$mu_Median)\n\n\n    Shapiro-Wilk normality test\n\ndata:  df.lower.samp$mu_Median\nW = 0.99, p-value = 0.6\n\nshapiro.test(df.higher.samp$mu_Median)\n\n\n    Shapiro-Wilk normality test\n\ndata:  df.higher.samp$mu_Median\nW = 1, p-value = 0.9\n\nshapiro.test(df.lower.samp$mu_Low_wage)\n\n\n    Shapiro-Wilk normality test\n\ndata:  df.lower.samp$mu_Low_wage\nW = 0.99, p-value = 0.3\n\nshapiro.test(df.higher.samp$mu_Low_wage)\n\n\n    Shapiro-Wilk normality test\n\ndata:  df.higher.samp$mu_Low_wage\nW = 0.99, p-value = 0.2\n\n\n\nt.test(df.higher.samp$mu_Median, df.lower.samp$mu_Median)\n\n\n    Welch Two Sample t-test\n\ndata:  df.higher.samp$mu_Median and df.lower.samp$mu_Median\nt = -385, df = 274, p-value &lt;0.0000000000000002\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -12489 -12362\nsample estimates:\nmean of x mean of y \n    34576     47002 \n\nt.test(df.higher.samp$mu_Low_wage, df.lower.samp$mu_Low_wage)\n\n\n    Welch Two Sample t-test\n\ndata:  df.higher.samp$mu_Low_wage and df.lower.samp$mu_Low_wage\nt = 94, df = 388, p-value &lt;0.0000000000000002\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 1863 1943\nsample estimates:\nmean of x mean of y \n     4722      2819 \n\n\nBoth p-value are close to zero, thus we can reject the null hypothesis of t-test. Then we can say the groups mean are different with this sample.\n\nWe can infer with 95% confidence that the majors with 50% or more of those enrolled being women have, on average, more low-wage jobs and less median salary.\n\n\ndf.salary.w &lt;- df.clean %&gt;%\n  group_by(ShareWomen_cat) %&gt;%\n  summarize(Salary_mean = mean(Median) )\n\n\nggplot( data = df.clean ) +\n  geom_boxplot( aes( x = ShareWomen_cat, y = Median), \n                fill = \"lightblue\", color = \"darkblue\" ) +\n  labs( title = \"Average Salary When \n        ShareWomen is lower/higher than 50%\",\n        subtitle = \"The average salary for majors with more women enrolled \\n\n        is lower than the majors with less women, reinforcing the perception \\n \n        that women are getting lower salaries.\",\n        x = \"50% Share of Women\",\n        y = \"Mean Salary\" ) +\n  geom_text( data = df.salary.w, aes( x = ShareWomen_cat,\n                                      y = Salary_mean,\n                                      label = round(Salary_mean)),\n             size = 2.2, vjust = 1, color = \"black\" ) +\n  theme(plot.subtitle = element_text( color = \"darkgray\", size = 9 ) )\n\n\n\n\n\n\n\n\nTo close this Exploratory Data Analysis we just need to compose the Analysis report."
  },
  {
    "objectID": "12_ExpDatAn.html#report",
    "href": "12_ExpDatAn.html#report",
    "title": "Exploratory Data Analysis",
    "section": "Report",
    "text": "Report\nEDA of a complete dataset, the missing values are less than 1% which was excluded to preserve the original data only without adding calculated numbers.\nThe descriptive statistics show high variances due to the presence of outliers. The women in college represents the 52%. The unemployment rate is around 6% presenting high variance.\nAre jobs more specialized which require a degree such as Education, Maths, Engineering. The engineering is the top-paying category followed by Computer and Maths, Physical Science, and Business. Nevertheless, there are no statistical differences between categories on salary median except comparing Engineering which is statistically higher.\nComparing gender it can be shown that majors with more women enrolled have more low wage jobs and also majors with more share of women seems to have lower salaries. Moreover, the unemployment rate are lower in majors with more share of women."
  },
  {
    "objectID": "12_ExpDatAn.html#next-steps",
    "href": "12_ExpDatAn.html#next-steps",
    "title": "Exploratory Data Analysis",
    "section": "Next steps",
    "text": "Next steps"
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Portfolio",
    "section": "",
    "text": "Let me introduce myself, my name is Álvaro Manzanas, I am a Junior Data Analysis, Scientist, Psychologist with a background in Computer Systems Administration and Virtualization. Eager to learn and improve professionally in the world of data analysis, with curiosity and interest to know the knowledge they hide. I am characterized as an analytical person with logical thinking, as well as being creative and curious, which combines well with the organization and meticulousness with which I excel.\nI hope you’ll take a look at what I’ve been working on, you might find it interesting. And if you like to connect with me, do not hesitate to do so, here I provide some buttons where you will find how to do it.\n\n\nLinkedIn\nGitHub\nemail"
  },
  {
    "objectID": "portfolio.html#productivity",
    "href": "portfolio.html#productivity",
    "title": "Github’s Projects",
    "section": "Productivity",
    "text": "Productivity\nmmm"
  },
  {
    "objectID": "portfolio.html#campaign-acceptance-ifood",
    "href": "portfolio.html#campaign-acceptance-ifood",
    "title": "Github’s Projects",
    "section": "Campaign Acceptance (iFood)",
    "text": "Campaign Acceptance (iFood)\nThis project focuses on a company in the food retail sector that aims to maximize their next marketing campaign and to do so, they are handing over their pilot study data. This will involve cleaning the data, analyzing it and proposing an approach to the marketing team to achieve this improvement for the next campaign. The second part focuses on getting a model that predicts customer behavior and thus selecting the most suitable customers for the next campaign.\n\nRepository\n\n\nOverview\n\nDescriptive Statistics\n\n\n\nDescriptive Statistics\n\n\nTable showing descriptive statistics for each selected quantitative variable. Several of these statistics have been calculated with custom functions such as ‘Winsorised Mean’, ‘Coefficient of Variation Centered on the Mean’ or ‘Trimmed Mean’.\n\n\nBar chart for categorical variable\n\n\n\nBar Char\n\n\nThis graph, composed of six vertical bar charts with confidence interval, shows the mean values of amount of money spent by product type for the Marital Status categories.\n\n\nCorrelation Matrix\n\n\n\nBar Char\n\n\nThe visualization represents a heat map with the correlations between the selected quantitative variables in order to observe the relationships between these variables and to guide further analysis.\n\n\nLearning Curve\n\n\n\nDecision Tree Learning Curve\n\n\nThe learning curve shows how the error decreases as we increase the size of the training set. It also helps us to identify whether our model is under-fitting or over-fitting the data. It is a useful tool for evaluating the performance of a model.\n\n\nFeature Importance\n\n\n\nFeature Importance in Random Forest model\n\n\nThis bar chart represents the importance of the features for the first decision tree in the Random Forest prediction model. It provides information about the relative importance of each feature (or variable) in the decision tree, useful for understanding which features most influence the model’s predictions."
  },
  {
    "objectID": "portfolio.html#data-analyst-journey",
    "href": "portfolio.html#data-analyst-journey",
    "title": "Portfolio",
    "section": "Data Analyst Journey",
    "text": "Data Analyst Journey\nIn this GitHub repository I progressively update my learning progress as a Data Analyst. I take care to maintain a scientific basis for the whole process, be it data cleaning, descriptive, inferential and predictive analysis. To learn the basics of different tools and languages, I have used, among others, the following books:\n\nMatthes, E. (2023). Python Crash Course, 3rd Edition. No Starch Press.\nMcKinney, W. (2022). Python for Data Analysis: Data Wrangling with Pandas, NumPy, and Jupyter. O’Reilly Media.\nArnold, J. (2023). Learning Microsoft Power Bi: Transforming Data Into Insights. O’Reilly Media.\n\nBelow I highlight some image with a small description so you can get the idea, but I encourage you to explore the repository, there is all the content I have gone through.\n\nRepository\n\n\nOverview\n\nDashboard sociodemographicDashboard Grade History\n\n\n\n\n\nSociodemographic data of university degree\n\n\nThe image shows a dashboard developed with Power BI in which you can see different graphs and data on how students are distributed throughout a course. From the number of students in the classroom, to a map showing the average scores depending on the state of residence in the USA.\n\n\n\n\n\nGrades data of university degree\n\n\nThis is also a Power BI dashboard in which the same data has been used, but this time it focuses on the grades obtained by the students. We can see what kind of test they had, how they describe their own level in Excel, as well as the average grade and what grade it corresponds to."
  },
  {
    "objectID": "portfolio.html#final-degree-project",
    "href": "portfolio.html#final-degree-project",
    "title": "Portfolio",
    "section": "Final Degree Project",
    "text": "Final Degree Project\nDevelopment of a scale to measure the use of video games as coping strategies for the user. Descriptive statistics and their graphical representation, psychometric tests, as well as Structural Equation Modelling (SEM) and Factor Analysis among other tests were used.\n\nOverview\n\nDescriptive VisualizationsHypothesis TestingConfirmatory Factor Analysis (SEM)\n\n\n\n\n\nDescriptive Graphs\n\n\nIt can be seen in the graphs how the sample of participants is distributed in the test scores for both depression and anxiety. A histogram can be seen as well as a stem-and-leaf chart which gives extra information such as mean, median, mode and range of scores.\n\n\n\n\n\nStatistical Inference\n\n\nThe table shows the test of two hypotheses comparing two groups. The first one refers to online gaming and who has a preference for it. The second one refers to the amount and variety of games they play. In addition to the contrast, the effect sizes for both are shown.\n\n\n\n\n\nStructural Equation Modelling\n\n\nThe graph shows the different factors for the CFA as well as their standardised factorial values, with those closest to 1 and -1 being the most relevant. In the table you can find the model fit data.\n\n\n\nChi-square\nRMSEA\nTLI\nCFI\n\n\n\n\n0.154\n0.03\n0.963\n0.968\n\n\n\n\nChi-square: adequate &gt; 0.05\nRMSEA: adequate &lt; 0.04 and good between 0.05 - 0.08\nTLI: adequate &gt; 0.94\nCFI: adequate &lt; 0.94"
  },
  {
    "objectID": "personal_notes/summary.html",
    "href": "personal_notes/summary.html",
    "title": "Summary",
    "section": "",
    "text": "Summary\nIn summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "personal_notes/intro.html",
    "href": "personal_notes/intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\nThis is a book created from markdown and executable code.\nSee @knuth84 for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "personal_notes/index.html",
    "href": "personal_notes/index.html",
    "title": "Preface",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "personal_notes/references.html",
    "href": "personal_notes/references.html",
    "title": "References",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "portfolio.html#productivity-prediction",
    "href": "portfolio.html#productivity-prediction",
    "title": "Portfolio",
    "section": "Productivity Prediction",
    "text": "Productivity Prediction\nmmm"
  },
  {
    "objectID": "portfolio.html#campaign-acceptance-prediction-ifood",
    "href": "portfolio.html#campaign-acceptance-prediction-ifood",
    "title": "Portfolio",
    "section": "Campaign Acceptance Prediction (iFood)",
    "text": "Campaign Acceptance Prediction (iFood)\nThis project focuses on a company in the food retail sector that aims to maximize their next marketing campaign and to do so, they are handing over their pilot study data. This will involve cleaning the data, analyzing it and proposing an approach to the marketing team to achieve this improvement for the next campaign. The second part focuses on getting a model that predicts customer behavior and thus selecting the most suitable customers for the next campaign.\n\nRepository\n\n\nOverview\n\nDescriptive StatisticsBar chart for categorical variableCorrelation MatrixLearning CurveFeature Importance\n\n\n\n\n\nDescriptive Statistics\n\n\nTable showing descriptive statistics for each selected quantitative variable. Several of these statistics have been calculated with custom functions such as ‘Winsorised Mean’, ‘Coefficient of Variation Centered on the Mean’ or ‘Trimmed Mean’.\n\n\n\n\n\nBar Char\n\n\nThis graph, composed of six vertical bar charts with confidence interval, shows the mean values of amount of money spent by product type for the Marital Status categories.\n\n\n\n\n\nCorrelation Matrix\n\n\nThe visualization represents a heat map with the correlations between the selected quantitative variables in order to observe the relationships between these variables and to guide further analysis.\n\n\n\n\n\nDecision Tree Learning Curve\n\n\nThe learning curve shows how the error decreases as we increase the size of the training set. It also helps us to identify whether our model is under-fitting or over-fitting the data. It is a useful tool for evaluating the performance of a model.\n\n\n\n\n\nFeature Importance in Random Forest model\n\n\nThis bar chart represents the importance of the features for the first decision tree in the Random Forest prediction model. It provides information about the relative importance of each feature (or variable) in the decision tree, useful for understanding which features most influence the model’s predictions."
  },
  {
    "objectID": "portfolio.html#descriptive-statistics",
    "href": "portfolio.html#descriptive-statistics",
    "title": "Github’s Projects",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\n\n\n\nDescriptive Statistics\n\n\nTable showing descriptive statistics for each selected quantitative variable. Several of these statistics have been calculated with custom functions such as ‘Winsorised Mean’, ‘Coefficient of Variation Centered on the Mean’ or ‘Trimmed Mean’."
  },
  {
    "objectID": "portfolio.html#bar-chart-for-categorical-variable",
    "href": "portfolio.html#bar-chart-for-categorical-variable",
    "title": "Github’s Projects",
    "section": "Bar chart for categorical variable",
    "text": "Bar chart for categorical variable\n\n\n\nBar Char\n\n\nThis graph, composed of six vertical bar charts with confidence interval, shows the mean values of amount of money spent by product type for the Marital Status categories."
  },
  {
    "objectID": "portfolio.html#correlation-matrix",
    "href": "portfolio.html#correlation-matrix",
    "title": "Github’s Projects",
    "section": "Correlation Matrix",
    "text": "Correlation Matrix\n\n\n\nBar Char\n\n\nThe visualization represents a heat map with the correlations between the selected quantitative variables in order to observe the relationships between these variables and to guide further analysis."
  },
  {
    "objectID": "portfolio.html#learning-curve",
    "href": "portfolio.html#learning-curve",
    "title": "Github’s Projects",
    "section": "Learning Curve",
    "text": "Learning Curve\n\n\n\nDecision Tree Learning Curve\n\n\nThe learning curve shows how the error decreases as we increase the size of the training set. It also helps us to identify whether our model is under-fitting or over-fitting the data. It is a useful tool for evaluating the performance of a model."
  },
  {
    "objectID": "portfolio.html#feature-importance",
    "href": "portfolio.html#feature-importance",
    "title": "Github’s Projects",
    "section": "Feature Importance",
    "text": "Feature Importance\n\n\n\nFeature Importance in Random Forest model\n\n\nThis bar chart represents the importance of the features for the first decision tree in the Random Forest prediction model. It provides information about the relative importance of each feature (or variable) in the decision tree, useful for understanding which features most influence the model’s predictions."
  }
]
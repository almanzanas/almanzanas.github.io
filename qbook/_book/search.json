[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Alvaro Manzanas",
    "section": "",
    "text": "About Me\nLet me introduce myself, my name is Álvaro Manzanas, I am a Junior Data Scientist, Psychologist with a background in Computer Systems Administration and Virtualization. Eager to learn and improve professionally in the world of data analysis, with curiosity and interest to know the knowledge they hide. I am characterized as an analytical person with logical thinking, as well as being creative and curious, which combines well with the organization and meticulousness with which I excel.\nI hope you’ll take a look at what I’ve been working on, you might find it interesting. And if you like to connect with me, do not hesitate to do so, here I provide some buttons where you will find how to do it.",
    "crumbs": [
      "Annotations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Alvaro Manzanas' Portfolio</span>"
    ]
  },
  {
    "objectID": "index.html#productivity-prediction",
    "href": "index.html#productivity-prediction",
    "title": "Alvaro Manzanas",
    "section": "Productivity Prediction",
    "text": "Productivity Prediction\nProject developed in R. It is oriented to productivity prediction, with the possibility of classifying whether or not the target productivity is met, or directly predicting what productivity value is expected.\nIt involves data cleaning, data exploration, statistical analysis, and predictive modeling.\nWith this project we want to understand why there are teams with lower performance that fail to meet the target productivity, and also want to be able to predict the productivity that the work teams may have.\n\n\n\n\nRepository\nAnalysis Report\nAnalysis Procedure\n\n\n\n\n\nOverview\n\nRegression TreeRandom ForestGroup ComparisonsClassificationRegression\n\n\n\n\n\nTree Decisions\n\n\nDecision tree generated by the Regression Tree algorithm. It explains in a visual way the decisions that must be made to achieve high productivity. The most direct branch indicates that, when ‘targeted_productivity’ is ‘0.75’ or ‘0.80’ and ‘incentive’ is greater than 70, the predicted productivity would be 0.913 (1 being the maximum).\n\n\n\n\n\nRandom Forst’s AUC\n\n\nROC curve plot with the AUC parameter at the bottom. It indicates how well the Random Forest classification model differentiates the categories. The larger the area under the curve with a maximum of 1, the better the predictions.\n\n\n\n\n\nTeams with high vs low productivity\n\n\nTwo statistically significant Wilcoxon comparisons can be seen. The visualization is in ggplot2 with the extension ggpubr. It shows how the work teams that are less compliant with productivity have a statistically lower incentive and the number of changes in their production process is statistically higher than the work teams that do meet the target productivity.\n\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Forest\nMC-RF\nLogistic Regression\nSVM\n\n\n\n\nAccuracy\n0.803\n0.798\n0.741\n0.735\n\n\nRecall\n0.902\n0.902\n0.887\n0.971\n\n\nPrecision\n0.839\n0.834\n0.789\n0.743\n\n\nKappa\n0.47\n0.455\n0.237\n0.106\n\n\nDetection Rate\n0.655\n0.655\n0.657\n0.706\n\n\nAUC\n0.816\n0.832\n\n0.672\n\n\n\n\n\n\n\n\n\nR^2\nRMSE\nMSE\nMAE\n\n\n\n\nStepwise *\n0.300\n\n\n\n\n\nLR Multiple *\n0.247\n0.150\n0.0226\n0.1088\n\n\nRobust LR\n0.213\n0.1499\n0.02247\n0.1066\n\n\nRF Regression\n0.4193\n0.1288\n0.01658\n0.08681\n\n\nRegression Tree\n0.4403\n0.1369\n0.01875\n0.09652\n\n\nCubist Tree *\n0.3902\n0.132\n0.01741\n0.08411",
    "crumbs": [
      "Annotations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Alvaro Manzanas' Portfolio</span>"
    ]
  },
  {
    "objectID": "index.html#campaign-acceptance-prediction-ifood",
    "href": "index.html#campaign-acceptance-prediction-ifood",
    "title": "Alvaro Manzanas",
    "section": "Campaign Acceptance Prediction (iFood)",
    "text": "Campaign Acceptance Prediction (iFood)\nThis project focuses on a company in the food retail sector that aims to maximize their next marketing campaign and to do so, they are handing over their pilot study data. This will involve cleaning the data, analyzing it and proposing an approach to the marketing team to achieve this improvement for the next campaign. The second part focuses on getting a model that predicts customer behavior and thus selecting the most suitable customers for the next campaign.\n\nRepository\n\n\nOverview\n\nDescriptive StatisticsBar chart for categorical variableCorrelation MatrixLearning CurveFeature Importance\n\n\n\n\n\nDescriptive Statistics\n\n\nTable showing descriptive statistics for each selected quantitative variable. Several of these statistics have been calculated with custom functions such as ‘Winsorised Mean’, ‘Coefficient of Variation Centered on the Mean’ or ‘Trimmed Mean’.\n\n\n\n\n\nBar Char\n\n\nThis graph, composed of six vertical bar charts with confidence interval, shows the mean values of amount of money spent by product type for the Marital Status categories.\n\n\n\n\n\nCorrelation Matrix\n\n\nThe visualization represents a heat map with the correlations between the selected quantitative variables in order to observe the relationships between these variables and to guide further analysis.\n\n\n\n\n\nDecision Tree Learning Curve\n\n\nThe learning curve shows how the error decreases as we increase the size of the training set. It also helps us to identify whether our model is under-fitting or over-fitting the data. It is a useful tool for evaluating the performance of a model.\n\n\n\n\n\nFeature Importance in Random Forest model\n\n\nThis bar chart represents the importance of the features for the first decision tree in the Random Forest prediction model. It provides information about the relative importance of each feature (or variable) in the decision tree, useful for understanding which features most influence the model’s predictions.",
    "crumbs": [
      "Annotations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Alvaro Manzanas' Portfolio</span>"
    ]
  },
  {
    "objectID": "index.html#data-analyst-journey",
    "href": "index.html#data-analyst-journey",
    "title": "Alvaro Manzanas",
    "section": "Data Analyst Journey",
    "text": "Data Analyst Journey\nIn this GitHub repository I progressively update my learning progress as a Data Analyst. I take care to maintain a scientific basis for the whole process, be it data cleaning, descriptive, inferential and predictive analysis. To learn the basics of different tools and languages, I have used, among others, the following books:\n\nMatthes, E. (2023). Python Crash Course, 3rd Edition. No Starch Press.\nMcKinney, W. (2022). Python for Data Analysis: Data Wrangling with Pandas, NumPy, and Jupyter. O’Reilly Media.\nArnold, J. (2023). Learning Microsoft Power Bi: Transforming Data Into Insights. O’Reilly Media.\n\nBelow I highlight some image with a small description so you can get the idea, but I encourage you to explore the repository, there is all the content I have gone through.\n\nRepository\n\n\nOverview\n\nDashboard sociodemographicDashboard Grade History\n\n\n\n\n\nSociodemographic data of university degree\n\n\nThe image shows a dashboard developed with Power BI in which you can see different graphs and data on how students are distributed throughout a course. From the number of students in the classroom, to a map showing the average scores depending on the state of residence in the USA.\n\n\n\n\n\nGrades data of university degree\n\n\nThis is also a Power BI dashboard in which the same data has been used, but this time it focuses on the grades obtained by the students. We can see what kind of test they had, how they describe their own level in Excel, as well as the average grade and what grade it corresponds to.",
    "crumbs": [
      "Annotations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Alvaro Manzanas' Portfolio</span>"
    ]
  },
  {
    "objectID": "index.html#final-degree-project",
    "href": "index.html#final-degree-project",
    "title": "Alvaro Manzanas",
    "section": "Final Degree Project",
    "text": "Final Degree Project\nDevelopment of a scale to measure the use of video games as coping strategies for the user. Descriptive statistics and their graphical representation, psychometric tests, as well as Structural Equation Modelling (SEM) and Factor Analysis among other tests were used.\n\nOverview\n\nDescriptive VisualizationsHypothesis TestingConfirmatory Factor Analysis (SEM)\n\n\n\n\n\nDescriptive Graphs\n\n\nIt can be seen in the graphs how the sample of participants is distributed in the test scores for both depression and anxiety. A histogram can be seen as well as a stem-and-leaf chart which gives extra information such as mean, median, mode and range of scores.\n\n\n\n\n\nStatistical Inference\n\n\nThe table shows the test of two hypotheses comparing two groups. The first one refers to online gaming and who has a preference for it. The second one refers to the amount and variety of games they play. In addition to the contrast, the effect sizes for both are shown.\n\n\n\n\n\nStructural Equation Modelling\n\n\nThe graph shows the different factors for the CFA as well as their standardised factorial values, with those closest to 1 and -1 being the most relevant. In the table you can find the model fit data.\n\n\n\nChi-square\nRMSEA\nTLI\nCFI\n\n\n\n\n0.154\n0.03\n0.963\n0.968\n\n\n\n\nChi-square: adequate &gt; 0.05\nRMSEA: adequate &lt; 0.04 and good between 0.05 - 0.08\nTLI: adequate &gt; 0.94\nCFI: adequate &lt; 0.94",
    "crumbs": [
      "Annotations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Alvaro Manzanas' Portfolio</span>"
    ]
  },
  {
    "objectID": "prefacio.html",
    "href": "prefacio.html",
    "title": "2  Annotations",
    "section": "",
    "text": "Preface\nThe documents in these pages are the notes, exercises and progress I have been making throughout my learning in R. They are in the same order in which I have practiced it and belong to the books:\n\nButtrey, S. E., & Whitaker, L. R. (2017). A Data Scientist’s Guide to Acquiring, Cleaning, and Managing Data in R. John Wiley & Sons.\nSantos, G. R. (2023). Data Wrangling with R: Load, Explore, Transform and Visualize Data for Modeling with Tidyverse Libraries. Packt Publishing.\n\nSomething that I have not yet been able to add are the notes of statistics of Antonio Pardo’s reference manuals. These manuals I studied them both in the career of Psychology, also now to deepen as Data Scientist. The title could be translated as: Data Analysis in Social and Health Sciences (I-III).\n\nMerino, A. P. (2014). Análisis de datos en ciencias sociales y de la salud I.\nMerino, A. P., & Castellanos, R. S. M. (2011). Análisis de datos en ciencias sociales y de la salud II.\nMerino, A. P., & Carnicer, M. Á. R. (2013). Análisis de datos en ciencias sociales y de la salud III.",
    "crumbs": [
      "Annotations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Annotations</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/00_basics_ndata.html",
    "href": "to_book/notebooks/00_basics_ndata.html",
    "title": "3  R: The very basics",
    "section": "",
    "text": "4 The very basics",
    "crumbs": [
      "Annotations",
      "Base R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R: The very basics</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/00_basics_ndata.html#getting-help",
    "href": "to_book/notebooks/00_basics_ndata.html#getting-help",
    "title": "3  R: The very basics",
    "section": "4.1 Getting Help",
    "text": "4.1 Getting Help\nAt the command line we can look for help through the commands:\n\nhelp(): gets information on a particular R function, e.g. help(matrix)\n?: works the same as the previous one, e.g. ?\"matrix\"\nhelp.search(): It’s useful if we know the subject rather than the name.\napropos(): returns a vector of names of objects containing that string, e.g. apropos(\"matrix\") returns every object with ‘matrix’ in its name.\nargs(): displays the set of arguments expected by a given function.\n\n\nargs(matrix)\n\nfunction (data = NA, nrow = 1, ncol = 1, byrow = FALSE, dimnames = NULL) \nNULL",
    "crumbs": [
      "Annotations",
      "Base R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R: The very basics</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/00_basics_ndata.html#vector-operations",
    "href": "to_book/notebooks/00_basics_ndata.html#vector-operations",
    "title": "3  R: The very basics",
    "section": "5.1 Vector Operations",
    "text": "5.1 Vector Operations\nArithmetic operations on vectors produce vectors.\n\n5:10\n\n[1]  5  6  7  8  9 10\n\n\n\n(5:10)^2\n\n[1]  25  36  49  64  81 100\n\n\nThere are functions which returns a single number (that is also a vector), such as: length(), sum(), mean(), sd(), min(), max(). The function range() returns the smallest and largest values, and summary() returns a vector of summary statistics.\n\n# An assigment with parentheses case print this assigment\n(th1 &lt;- c(20, 15, 10, 5, 0)^2)\n\n[1] 400 225 100  25   0\n\n\n\nth2 &lt;- 105:101\nth2 + th1\n\n[1] 505 329 203 127 101\n\n\n\nth2 / th1\n\n[1] 0.2625000 0.4622222 1.0300000 4.0800000       Inf\n\n\nIt’s Computed element by element, that’s why print ‘Inf’, because 101/0 refer to an infinite value.\n\n# Print the larger value anywhere\nmax(th1, th2)\n\n[1] 400\n\n\n\n# 'Parallel maximum' print the larger of each pair\npmax(th2, th1)\n\n[1] 400 225 103 102 101\n\n\nLoginac vectors can also be combined. The operator | for ‘or’ return TRUE if either element is TRUE; the & operator for ‘and’ return TRUE only if both elements are TRUE. This single version evaluate the condition for every pair of elements from both vectors, the double version ( || , &&) evaluate multiple TRUE/FALSE conditions from left to right, stopping as soon as possible (are useful in if() statements).\nRecycling\nWhen a vector does not match with other vector to be operated, the shorter one starts again, recycling it’s elements.\n\n# Two vector with the same length, 6\n5:10 + c(0, 10, 100, 1000, 10000, 100000)\n\n[1]      5     16    107   1008  10009 100010\n\n# The sum is element by element\n\n\n# Two vectors, 6 and 3 elements\n5:10 + c(1, 10, 100)\n\n[1]   6  16 107   9  19 110\n\n\n\n# 6-vector and 5 vector. The second recycle it's first element (10 + 3)\n5:10 + c(3, 4, 5, 6, 7)\n\nWarning in 5:10 + c(3, 4, 5, 6, 7): longer object length is not a multiple of\nshorter object length\n\n\n[1]  8 10 12 14 16 13\n\n\nRecycling a vector of length 1 ((5:10) + 4) is very common, but the other lengths ir rarer. If we see the last warning, It’s better to treat that as an error and try to resolve it.",
    "crumbs": [
      "Annotations",
      "Base R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R: The very basics</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/00_basics_ndata.html#handling-character-vectors",
    "href": "to_book/notebooks/00_basics_ndata.html#handling-character-vectors",
    "title": "3  R: The very basics",
    "section": "5.2 Handling character vectors",
    "text": "5.2 Handling character vectors\nWe can add names to a vector to identify individual entries. R accept letters, numbers, dots, and underscores:\n\nvec &lt;- c(201, 202, 203)\nnames(vec)\n\nNULL\n\n\n\n# It is a bad practice to have a vector's name invalid:\nnames(vec) &lt;- c(\"a\", \"b\", \"with space\")\nnames(vec)\n\n[1] \"a\"          \"b\"          \"with space\"\n\n\n\n# Also, we can assign names directly\nvec &lt;- c(a = 201, b = 202, with.space = 203)\nvec\n\n         a          b with.space \n       201        202        203",
    "crumbs": [
      "Annotations",
      "Base R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R: The very basics</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/00_basics_ndata.html#data-types",
    "href": "to_book/notebooks/00_basics_ndata.html#data-types",
    "title": "3  R: The very basics",
    "section": "5.3 Data Types",
    "text": "5.3 Data Types\nThe most common data types are numeric, logical and character. These are some less-common data types:\n\nintegers: values between -(231 -1) and 231 -1. Values outside this range may be displayed as integers but will be stored as doubles. To regard an item as an integer you can add L on its end ( 246L ), this only makes sense whether really is an integer.\nraw: refers to data kept in binary (hexadecimal) form. Is data from images, sound and video, R will stored as raw. It can be converted into character data with rawToChar() function. Reading raw data could be useful to handle the case of unexpected characters.\ncomplex number: R can manipulate complex numbers but almost never arise in data cleaning.\n\nWe can see what data type is a vector with typeof() which differentiates between integer and double. Other function called mode() calls them both numeric. The str() function return the data type and the first few entries. And class() is more general operator for complex types.\nThe functions: is.logical(), is.integer(), is.numeric(), is.character() returns a single logical value whether is TRUE or FALSE. A particularly useful function for more complicated or user-defined clases is is() which lets you specify the class as an argument: is(pi, \"numeric\").\n\ntypeof(vec)\n\n[1] \"double\"\n\n\n\nmode(vec)\n\n[1] \"numeric\"\n\n\n\n5.3.1 Data Type conversion\nR modifies the entire vector to be of the more complicated type. If you create a numeric vector but you adds a character, the entire vector will be converted to character:\n\nvec &lt;- c(1, 2, 3, 4, 5, 6, \"7\")\nvec\n\n[1] \"1\" \"2\" \"3\" \"4\" \"5\" \"6\" \"7\"\n\n\nTypes: logical &lt; raw &lt; numeric &lt; complex &lt; character.\nOther group of functios are as. to convert vectors:\n\nas.numeric: a character will be converted to a numeric if it has the syuntax of a number.\n\n# The element which no fit as numeric will be NA values by coercion\nas.numeric( c(\"123.4\", \"-1234e-2\", \"4,567\", \"45. 67\", \"$12\", \"45%\"))\n\nWarning: NAs introduced by coercion\n\n\n[1] 123.40 -12.34     NA     NA     NA     NA\n\n\nas.logical: Numeric values that are zero become FALSE, otherwise become TRUE. Sometimes numbers that you expect to be zero aren’t because of floating-point error.\n\n# The third is FALSE because it is outside the range of double precision.\n# The last element 'should' be zero but aren't because floating-point error.\nas.logical( c(123, 5-5, -1e-34, 1e-500, 1- 1/49 * 49) )\n\n[1]  TRUE FALSE  TRUE FALSE  TRUE\n\n\n\nNumeric, non-missing values never produce NA when converted to logical.",
    "crumbs": [
      "Annotations",
      "Base R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R: The very basics</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/00_basics_ndata.html#subsets-of-vectors",
    "href": "to_book/notebooks/00_basics_ndata.html#subsets-of-vectors",
    "title": "3  R: The very basics",
    "section": "5.4 Subsets of Vectors",
    "text": "5.4 Subsets of Vectors\nsubsetting or extracting is to pull out a piece of a vector.\n\n5.4.1 subscript: numeric\n\n# 5-vector\n(a &lt;- 101:105)\n\n[1] 101 102 103 104 105\n\n# extract element number 3\na[3]\n\n[1] 103\n\n\nWe can use a numeric expression to compute a subscript but only if we are sure that expression is an integer.\n\n# extract the fourth and second element\na[c(4, 2)]\n\n[1] 104 102\n\n# extract the element corresponding to 3+2 \na[3+2]\n\n[1] 105\n\n\nNegative numbers omit this elements, but you cannot mix negative and positive numbers. Zeros are permited but they are ignored by R.\na[-2]\n# Omit the first and second element\na[- (1:2) ]\n# mix of positive and negative, result an error\na[c(-1, 2)]\n\n\n5.4.2 Subscript: logical\nA logical subscript is a logical vector of the same length as the thing being extracted from. TRUE are returned, FALSE are not.\n\n# numeric vector with ages\nage &lt;- c(21, 32, 43, 54, 65, 76)\nage &gt; 60\n\n[1] FALSE FALSE FALSE FALSE  TRUE  TRUE\n\n\nWe can use the last logical vector to extract elements of age, or combining it with other vectors.\n\n# Which ages are greater than 60\nage[age &gt; 60]\n\n[1] 65 76\n\n\n\npeople &lt;- c(\"Yoshua\", \"Mery\", \"Alex\", \"John\", \"Lee\", \"Cindy\")\n# To which people corresponds their age greater than 60\npeople[age &gt; 60]\n\n[1] \"Lee\"   \"Cindy\"\n\n# Creating a new variable with the previous subscript\nage.gt.60 &lt;- age &gt; 60\npeople[age.gt.60]\n\n[1] \"Lee\"   \"Cindy\"\n\n\nErrors to avoid:\n\n# R convert the logical subscript to numeric\n# the extraction produces the first element of the vector two times\npeople[0 + age.gt.60]\n\n[1] \"Yoshua\" \"Yoshua\"\n\n# With the negative sign, R convert again into numeric values\n# The extraction drops the first value and rest are returned\npeople[-age.gt.60]\n\n[1] \"Mery\"  \"Alex\"  \"John\"  \"Lee\"   \"Cindy\"\n\n# This is, probably, the intended form:\n# The operator ! to print which values are not greater than 60\npeople[ !age.gt.60 ]\n\n[1] \"Yoshua\" \"Mery\"   \"Alex\"   \"John\"  \n\n\nThe which() function can be used to convert a logical vector into a numeric. It returns the indices of the element that are TRUE.\nTo find the location of every minimum value in the variable ‘y’ we can use which(y == min(y). The alternative are which.min() and which.max() but only select the first index with the minimum or maximum value.\n\n\n5.4.3 subscript: names\nA name subscript will need to be a character.\n\n(vec &lt;- c(a = 201, b = 202, with.space = 203) )\n\n         a          b with.space \n       201        202        203 \n\nvec[\"b\"]\n\n  b \n202 \n\n# Names of 'vec' distinct to 'a'\nvec[names(vec) != \"a\"]\n\n         b with.space \n       202        203 \n\n\n\n\n5.4.4 Vector length 0\nThis usually happens when all of the elements of a logical subscript are FALSE.\n\na\n\n[1] 101 102 103 104 105\n\nb &lt;- a[a &lt; 99]\nb\n\ninteger(0)\n\n\nIf a zero-length vector is used as the condition in an if() statement, an error results:\n# the sum of a numeric or logical vector of length 0 is itself zero\nsum( b + 12345)\n\nif (b &lt; 2) cat(\"yes\\n\")\n\n\n5.4.5 Replacing elements of a vector\nIf we extract elements, we can use the extraction operation on the left side of an assigment for replacing.\n\na\n\n[1] 101 102 103 104 105\n\na[3] &lt;- 203\na\n\n[1] 101 102 203 104 105\n\na[a &gt; 104] &lt;- -999\na\n\n[1]  101  102 -999  104 -999\n\n\n\n# Replacing using names\nnames(vec) &lt;- c(\"a\", \"b\", \"c\")\nvec\n\n  a   b   c \n201 202 203 \n\ncat(\"\\n\")\nvec[\"b\"] &lt;- 302\nvec\n\n  a   b   c \n201 302 203 \n\n\nIt is possible too combine two vectors (if the vectors are different types, R convert them) and also assign elements of a vector out past its end.\n\n(a &lt;- 101:103)\n\n[1] 101 102 103\n\n(b &lt;- -5:-1)\n\n[1] -5 -4 -3 -2 -1\n\ncat(\"\\n\")\na[4:8] &lt;- b\na\n\n[1] 101 102 103  -5  -4  -3  -2  -1\n\ncat(\"\\n\")\n# b[6] does not exists, will be filled with NA to add b[7]\nb[7] &lt;- 3\nb\n\n[1] -5 -4 -3 -2 -1 NA  3",
    "crumbs": [
      "Annotations",
      "Base R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R: The very basics</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/00_basics_ndata.html#missing-data-na",
    "href": "to_book/notebooks/00_basics_ndata.html#missing-data-na",
    "title": "3  R: The very basics",
    "section": "5.5 Missing Data NA",
    "text": "5.5 Missing Data NA\nWhen we try to extract an item that was never present in a vector appears missing values. NA values rule: any computation with an NA itself becomes an NA. With mathematical computations we can use na.rm = TRUE argument to compute the result after omitting missing values.\nWe can use anyNA() function to determine whether any of the values are missing (TRUE or FALSE). Also, is.na() fuction return TRUE or FALSE for every element in the vector. We can use where(is.na(vector))to find the numeric indices of the missing elements.\n\n# Creating a vector with missing values\nnax &lt;- 101:106\nnax[ c(3, 5) ] &lt;- NA\nnax\n\n[1] 101 102  NA 104  NA 106\n\ncat(\"\\n\")\nis.na(nax)\n\n[1] FALSE FALSE  TRUE FALSE  TRUE FALSE\n\ncat(\"\\n\")\nwhich(is.na(nax) )\n\n[1] 3 5\n\ncat(\"\\nMean 'nax' vector: \")\n\n\nMean 'nax' vector: \n\nmean(nax)\n\n[1] NA\n\nmean(nax, na.rm=TRUE)\n\n[1] 103.25\n\n\nWe can remove missing values with vec[!is.na(vec)] which return non-missing entries, but na.omit() function deletes the missing values but also keeps track of where in the vector they used to be. This information is stored in vector’s ‘attributes’:\n\n# To return non-missing values:\nnax[!is.na(nax)]\n\n[1] 101 102 104 106\n\ncat(\"\\n\")\n# Storing non missing values in a variable and keeping track of deleted ones\n(nay &lt;- na.omit(nax))\n\n[1] 101 102 104 106\nattr(,\"na.action\")\n[1] 3 5\nattr(,\"class\")\n[1] \"omit\"\n\nattr(nay, \"na.action\")\n\n[1] 3 5\nattr(,\"class\")\n[1] \"omit\"\n\nattr(nay, \"class\")\n\nNULL\n\n\nWe can have a vector ‘b’ with data and a vector ‘a’ as index which contain NA values. If we use ‘a’ as filter, it returns every coincident value and missing value, this behaviour can be changed with b[!is.na(a) & a == 2]\n\n(b &lt;- 101:104)\n\n[1] 101 102 103 104\n\n(a &lt;- c(1, 2, NA, 4))\n\n[1]  1  2 NA  4\n\ncat(\"\\n\")\nb[a == 2]\n\n[1] 102  NA\n\ncat(\"\\n\")\nb[!is.na(a) & a == 2]\n\n[1] 102\n\n\n\nNaN : Special value, means “Not a Number”. It si the result of specific computations such as 0/0. NaN is considered a missing value.\nInf : Appears when a positive number is accidentally divided by zero. Inf values are not missing.\n\nInf + NA = NA\nNaN + NA = NaN\nWe generally wanto to identify any of these values: the function is.finite() produces TRUE for numbers that are neither NA nor NaN nor Inf. It serves as a check on valid values. The command all(is.finite(vector) ) is to see a numeric vector elements which are not special values.\n\nNULL : is an object with zero length, no contents, no class. It often arise trying to access an element which does not exist. The function is.null() is for testing NULL values. If you index using a NULL value the result will be a vector of length 0.",
    "crumbs": [
      "Annotations",
      "Base R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R: The very basics</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/00_basics_ndata.html#the-table-function",
    "href": "to_book/notebooks/00_basics_ndata.html#the-table-function",
    "title": "3  R: The very basics",
    "section": "5.6 The Table( ) function",
    "text": "5.6 The Table( ) function\nTable() produce a table returning for each unique value the number of times that value appears.\n\nvec &lt;- rep ( c(\"red\", \"blue\", NA, \"green\"), c(4, 2, 1, 3) )\nvec\n\n [1] \"red\"   \"red\"   \"red\"   \"red\"   \"blue\"  \"blue\"  NA      \"green\" \"green\"\n[10] \"green\"\n\ncat(\"\\n\")\ntable(vec)\n\nvec\n blue green   red \n    2     3     4 \n\n\ntable() arguments:\n\nuseNA =\n\n\"no\" : default option. Exclude NA values.\n\"ifany\" : show an entry for NA if there are any.\n\"always\" : show an entry for NA whether there are any NA values or not.\n\nexclude =\n\nc(NA, NaN) : default option.\nWe can add a value of the vector to exclude, also we can pass NULL.\n\n\n\ntable(vec, useNA = \"always\")\n\nvec\n blue green   red  &lt;NA&gt; \n    2     3     4     1 \n\ncat(\"\\n\")\ntable(vec, exclude=\"green\")\n\nvec\nblue  red &lt;NA&gt; \n   2    4    1 \n\n\nAlso, we can pass to table() function two vectors to create a two-way table (cross-tabulation). in R a two-way table is treated the same as a matrix.\n\nvec2 &lt;- rep ( c(\"north\", \"south\", \"east\", \"west\"), c(2, 3, 2, 3) )\ntable(vec,vec2, useNA=\"ifany\")\n\n       vec2\nvec     east north south west\n  blue     1     0     1    0\n  green    0     0     0    3\n  red      0     2     2    0\n  &lt;NA&gt;     1     0     0    0\n\n\nA three and higher-way tables are produced when you pass three or more equal-length vectors.\nFor very large vectors, the data.table() function in the data.table package may prove more efficient than table().\nThe xtabs() function is useful for creating more complex tables.\n\n5.6.1 Operating on tables\n\nprop.table() : passing a variable with table assigned produces the proportions of the total counts in the table by cell (or row, or column). The second argument references 1 to rows, or 2 to columns.\n\n\nyear &lt;- rep (2015:2017, each=5)\nmarket &lt;- c(\"a\", \"a\", \"b\", \"a\", \"b\", \"b\", \"b\", \"a\", \"b\", \n            \"b\", \"a\", \"b\", \"a\", \"b\", \"a\")\ncost &lt;- c(64, 87, 71, 79, 79, 91, 86, 92, NA,\n          55, 37, 41, 60, 66, 82)\n\n# assign a table with the vectors we want to 'tab'\ntab &lt;- table(market, year)\ntab\n\n      year\nmarket 2015 2016 2017\n     a    3    1    3\n     b    2    4    2\n\ncat(\"\\n\")\n# proportions along rows\nprop.table(tab)\n\n      year\nmarket       2015       2016       2017\n     a 0.20000000 0.06666667 0.20000000\n     b 0.13333333 0.26666667 0.13333333\n\ncat(\"\\n\")\n# proportions throwout columns\nprop.table(tab, 2)\n\n      year\nmarket 2015 2016 2017\n     a  0.6  0.2  0.6\n     b  0.4  0.8  0.4\n\n\n\nmargin.table() : produce the marginals totals from a table.\naddmargins() : to a given table incorporates those totals producing a new row and column by default. Passing as second argument 1 or 2, you can specified if only wants rows or columns.\n\n\ncat(\"margin.table function for columns:\\n\")\n\nmargin.table function for columns:\n\nmargin.table(tab, 2)\n\nyear\n2015 2016 2017 \n   5    5    5 \n\ncat(\"\\naddmargins function in the table: \\n\")\n\n\naddmargins function in the table: \n\naddmargins(tab)\n\n      year\nmarket 2015 2016 2017 Sum\n   a      3    1    3   7\n   b      2    4    2   8\n   Sum    5    5    5  15\n\n\n\ntapply() : the arguments are, first the vector where perform the computation, second the vector with the index to group, and then the function to apply. To group with more than one vector (index) we can use a list().\n\n\ncat(\"There are NA in cost? \")\n\nThere are NA in cost? \n\nanyNA(cost)\n\n[1] TRUE\n\ncat(\"\\n\")\ntapply (cost, year, min, na.rm=TRUE)\n\n2015 2016 2017 \n  64   55   37 \n\ncat(\"\\n\")\ntapply (cost, list(market, year), mean, na.rm = TRUE)\n\n      2015     2016     2017\na 76.66667 92.00000 59.66667\nb 75.00000 77.33333 53.50000\n\n\nIt is posible to use with tapply() custom fuctions, as an example, for (x) return the sum of the squares of each entry of x:\n\ntapply (cost, list(market, year),\n        function (x) sum (x^2, na.rm = TRUE))\n\n   2015  2016  2017\na 17906  8464 11693\nb 11282 18702  6037",
    "crumbs": [
      "Annotations",
      "Base R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R: The very basics</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/00_basics_ndata.html#other-actions-on-vectors",
    "href": "to_book/notebooks/00_basics_ndata.html#other-actions-on-vectors",
    "title": "3  R: The very basics",
    "section": "5.7 Other Actions on Vectors",
    "text": "5.7 Other Actions on Vectors\n\n5.7.1 Rounding\n\nround() : to round a number with specific amount of decimals. Passing a negative number as second argument will round the nearest power of 10.\nsignif() : retain a specific number of digits.\ntrunc() : discards the decimal part returning an integer.\nfloor() ; ceiling() : round to the next lower or higher integer respectively.\n\n\n(dec &lt;- 123.4567)\n\n[1] 123.4567\n\ncat(\"\\nRound function to 3 decimals: \\n\")\n\n\nRound function to 3 decimals: \n\nround(dec, 3)\n\n[1] 123.457\n\ncat(\"\\nRound function (-1) to nearest power of 10: \\n\")\n\n\nRound function (-1) to nearest power of 10: \n\nround(dec, -1)\n\n[1] 120\n\ncat(\"\\nSignif function to 4 numbers: \\n\")\n\n\nSignif function to 4 numbers: \n\nsignif(dec, 4)\n\n[1] 123.5\n\ncat(\"\\nTrunc function: \\n\")\n\n\nTrunc function: \n\ntrunc(dec)\n\n[1] 123\n\ncat(\"\\nFloor function to round to the lower integer: \\n\")\n\n\nFloor function to round to the lower integer: \n\nfloor(dec)\n\n[1] 123\n\ncat(\"\\nRound function to round to the higher integer: \\n\")\n\n\nRound function to round to the higher integer: \n\nceiling(dec)\n\n[1] 124\n\n\n\n\n5.7.2 Sorting and Ordering\n\nsort() : sorting from smallest to largest. It will drop NA and NaN values by default.\n\ndecreasing=TRUE will reverse the order.\nna.last will add NA values, TRUE at the end and FALSE at the beginning.\n\norder() : returns the indices which can be used to sort a vector.\n\nna.last as TRUE by default. TRUE at the end and FALSE at the beginning.\n\n\n\npeople\n\n[1] \"Yoshua\" \"Mery\"   \"Alex\"   \"John\"   \"Lee\"    \"Cindy\" \n\n(scores &lt;- c(123, 456, 789, 456, 654, 987))\n\n[1] 123 456 789 456 654 987\n\ncat(\"\\nSorting 'people' by their scores (descending): \\n\")\n\n\nSorting 'people' by their scores (descending): \n\npeople[order(scores, decreasing=TRUE)]\n\n[1] \"Cindy\"  \"Alex\"   \"Lee\"    \"Mery\"   \"John\"   \"Yoshua\"\n\ncat(\"\\nOrdering people by their scores, and in case \n    of a tie using their names alphabetically.: \\n\")\n\n\nOrdering people by their scores, and in case \n    of a tie using their names alphabetically.: \n\npeople[order(scores, people[1:6])]\n\n[1] \"Yoshua\" \"John\"   \"Mery\"   \"Lee\"    \"Alex\"   \"Cindy\" \n\n\n\n\n5.7.3 Vector as Sets\nTo find values that overlap between two vectors we can use %in% function. For example, we can use a command a %in% b which will return a vector the same length as ‘a’ with a logical indicator whether each element is found in ‘b’. Witch table(a %in% b) we get the number of element in ‘a’ that were not found in ‘b’.\n\n# Looking for which letter is in 'letters':\nc(\"g\", \"5\", \"b\", \"J\", \"!\") %in% letters\n\n[1]  TRUE FALSE  TRUE FALSE FALSE\n\ncat(\"\\n\")\ntable (c(\"g\", \"5\", \"b\", \"J\", \"!\") %in% letters)\n\n\nFALSE  TRUE \n    3     2 \n\ncat(\"\\n\")\nwhich(c(\"g\", \"5\", \"b\", \"J\", \"!\") %in% letters)\n\n[1] 1 3\n\n\n\nunion() : returns the elements in either passed vectors\nintersect() : returns the elements which are in both passed vectors\nsetdiff() : returns the elements of ‘a’ not present in ‘b’\n\n\ncat(\"Union 'a' and 'b':\\n\")\n\nUnion 'a' and 'b':\n\nunion (c(\"g\", \"5\", \"b\", \"J\", \"!\"), letters)\n\n [1] \"g\" \"5\" \"b\" \"J\" \"!\" \"a\" \"c\" \"d\" \"e\" \"f\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\"\n[20] \"q\" \"r\" \"s\" \"t\" \"u\" \"v\" \"w\" \"x\" \"y\" \"z\"\n\ncat(\"\\n\")\ncat(\"Intersect 'a' with 'b':\\n\")\n\nIntersect 'a' with 'b':\n\nintersect (c(\"g\", \"5\", \"b\", \"J\", \"!\"), letters) \n\n[1] \"g\" \"b\"\n\ncat(\"\\n\")\ncat(\"Defferent elemenets of 'a' in 'b':\\n\")\n\nDefferent elemenets of 'a' in 'b':\n\nsetdiff (c(\"g\", \"5\", \"b\", \"J\", \"!\"), letters) \n\n[1] \"5\" \"J\" \"!\"",
    "crumbs": [
      "Annotations",
      "Base R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R: The very basics</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/00_basics_ndata.html#duplicates",
    "href": "to_book/notebooks/00_basics_ndata.html#duplicates",
    "title": "3  R: The very basics",
    "section": "5.8 Duplicates",
    "text": "5.8 Duplicates\n\nanyDuplicated() : returns the duplicated values in a vector\nunique() : returns the distinct values, including NA and NaN.\nduplicated() : returns a logical vector with TRUE per duplicated value (not the first value).\n\nfromLast=TRUE reads from the end to the beginning. Using union() you can combine fromLast=TRUE and fromLast=FALSE to identify all duplicates.\n\n\n\nlet &lt;- c(letters, c(\"j\", \"j\", \"x\"))\n\ncat(\"Duplicates?\\n\")\n\nDuplicates?\n\nlet[duplicated(let)]\n\n[1] \"j\" \"j\" \"x\"\n\ncat(\"\\n\")\n(tab &lt;- table (let))\n\nlet\na b c d e f g h i j k l m n o p q r s t u v w x y z \n1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 \n\ncat(\"\\n\")\ntab[which (tab != 1)]\n\nlet\nj x \n3 2 \n\ncat(\"\\n\")\nnames (tab)[tab != 1]\n\n[1] \"j\" \"x\"\n\ncat(\"\\n\")\ntable (table (let))\n\n\n 1  2  3 \n24  1  1 \n\n\nMatching is identify where in a vector we can find the values in another vector. The function match() is a more powerful version of %in%.\n\nmatch() : given two vectors returns where the element from ‘a’ is in ‘b’. By default returns NA for no matches, we can change this with nomatch=0 argument.\n\n\nnames1 &lt;- c(\"Jensen\", \"Chang\", \"Johnson\", \"Lopez\", \"McNamara\", \"Reese\")\nnames2 &lt;- c(\"Lopez\", \"Ruth\", \"Nakagawa\", \"Jensen\", \"Mays\")\n\nmatch(names1, names2)\n\n[1]  4 NA NA  1 NA NA\n\ncat(\"\\n\")\nnames2[match(names1, names2, nomatch=0)]\n\n[1] \"Jensen\" \"Lopez\" \n\n\nThe ‘Run length enconding’ function is rle() which returns the number of repetitions and length.\n\nrle( c(\"a\", \"b\", \"b\", \"a\", \"c\", \"c\", \"c\", \"a\", \"a\"))\n\nRun Length Encoding\n  lengths: int [1:5] 1 2 1 3 2\n  values : chr [1:5] \"a\" \"b\" \"a\" \"c\" \"a\"\n\n\nThe previous output show, with length 1 value ‘a’ followerd by ‘b’ with length 2, again ‘a’ length 1 and then ‘c’ with length 3, to finish with ‘a’ twice.",
    "crumbs": [
      "Annotations",
      "Base R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R: The very basics</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/00_basics_ndata.html#summary",
    "href": "to_book/notebooks/00_basics_ndata.html#summary",
    "title": "3  R: The very basics",
    "section": "5.9 Summary",
    "text": "5.9 Summary\n\n5.9.1 Conversions\n\nConverting character to numeric produces NA for things that aren’t numbers, like the character strings “TRUE” or “$199.99”.\nConverting character to logical produces NA for any string that isn’t “TRUE”, “True”, “true”, “T”, “FALSE”, “False”, “false” or “F”.\nConverting numeric to logical produces FALSE for a zero and TRUE for any non-zero entry (and watch out for floating-point error here).\n\n\n\n5.9.2 Subscripts\n\nA logical subscript returns the values that match up with its TRUE entries.\nA numeric subscript returns the values specified in the subscript.\ncharacter subscript will extract, from a named vector, elements whose names are present in the subscript.",
    "crumbs": [
      "Annotations",
      "Base R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R: The very basics</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/00_basics_ndata.html#matrices",
    "href": "to_book/notebooks/00_basics_ndata.html#matrices",
    "title": "3  R: The very basics",
    "section": "6.1 Matrices",
    "text": "6.1 Matrices\nEssentially is a vector two-dimensional like a rectangle. Every element of a matrix needs to be of the same type (numeric, logical or character).\nA matrix is filled column by column.\nUsing length() we get the total number of elements in the matrix, and using dim() we get the dimension as number of rows and columns.\n\ncbind() : combines a set of vectors into a matrix column by column.\nrbind() : combines a set of vectors into a matrix row by row.\n\nIf the vectors to combine have unequal length, R will recycle.\nWith matrix in the same way as vectors we can do arithmetic operations, that will be element by element. Also, t(A) transposes a matrix, and solve() inverts a matrix.\nTo extract and assign new values to elements the process is similar to a vector but now we have to pass two coordinates:\n\na &lt;- matrix(101:115, nrow=5, ncol=3)\na\n\n     [,1] [,2] [,3]\n[1,]  101  106  111\n[2,]  102  107  112\n[3,]  103  108  113\n[4,]  104  109  114\n[5,]  105  110  115\n\ncat(\"\\n\")\na[1,2]\n\n[1] 106\n\ncat(\"\\n\")\na[-2,]\n\n     [,1] [,2] [,3]\n[1,]  101  106  111\n[2,]  103  108  113\n[3,]  104  109  114\n[4,]  105  110  115\n\ncat(\"\\nObtaining row 4 and 2, and cols 3 and 1, in that order: \\n\")\n\n\nObtaining row 4 and 2, and cols 3 and 1, in that order: \n\na[c(4,2), c(3,1)]\n\n     [,1] [,2]\n[1,]  114  104\n[2,]  112  102\n\n\nTo create a vector from a matrix we can use c(), it will use the first column, then the second and so on. To extract data row by row, we can use t() to transpose the matrix first like c(t(a)).\n\nc(a)\n\n [1] 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115\n\ncat(\"\\n\")\nt(a)\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]  101  102  103  104  105\n[2,]  106  107  108  109  110\n[3,]  111  112  113  114  115\n\ncat(\"\\n\")\nc(t(a))\n\n [1] 101 106 111 102 107 112 103 108 113 104 109 114 105 110 115\n\n\nIf we ask to a matrix for one column we’ll get a vector, because dimensions of length 1 are usually dropped by default. We can use drop=FALSE argument to prevent that.\n\na[,2]\n\n[1] 106 107 108 109 110\n\ncat(\"\\n\")\na[,2, drop=FALSE]\n\n     [,1]\n[1,]  106\n[2,]  107\n[3,]  108\n[4,]  109\n[5,]  110\n\n\n\n6.1.1 Row and Column Names\nWith dimnames() we get rows and columns names. rownames() and colnames() to get rows and columns names respectively. As well as with names() and vectors, we can asign names to columns and rows with these functions.\n\nyear &lt;- rep (2015:2017, each = 5)\nmarket &lt;- c(2, 2, 3, 2, 3, 3, 3, 2, 3, 3, 2, 3, 2, 3, 2)\n\n(tbl &lt;- table (market, year))\n\n      year\nmarket 2015 2016 2017\n     2    3    1    3\n     3    2    4    2\n\ncat(\"\\nCollaing to colnames(): \\n\")\n\n\nCollaing to colnames(): \n\ncolnames(tbl)\n\n[1] \"2015\" \"2016\" \"2017\"\n\ncat(\"\\nAssigning new names to columns: \\n\")\n\n\nAssigning new names to columns: \n\ncolnames(tbl) &lt;- c(\"Y15\", \"Y16\", \"Y17\")\ntbl\n\n      year\nmarket Y15 Y16 Y17\n     2   3   1   3\n     3   2   4   2\n\n\nJust like before, we can call for the column name or row identifier to get them:\n\ntbl[,\"Y15\"]\n\n2 3 \n3 2 \n\ncat(\"\\n\")\ntbl[\"3\",]\n\nY15 Y16 Y17 \n  2   4   2 \n\n\n\n\n6.1.2 Applying Functions\nBuilt in functions which works across columns or rows: colSums(), rowSums(), colMeans(), rowMeans().\n\napply() : Useful to apply a custom function.\n\nfirst argument, the matrix to which apply the function.\nThe direction, 1 for across rows, 2 for down columns.\nThe function to be applied.\n\n\n\na\n\n     [,1] [,2] [,3]\n[1,]  101  106  111\n[2,]  102  107  112\n[3,]  103  108  113\n[4,]  104  109  114\n[5,]  105  110  115\n\ncat(\"\\n\")\nrowSums(a)\n\n[1] 318 321 324 327 330\n\ncat(\"\\n\")\napply(a, 1, sum, na.rm=TRUE)\n\n[1] 318 321 324 327 330\n\ncat(\"\\n\")\napply(a, 1, function (x) sum (x))\n\n[1] 318 321 324 327 330\n\n\nWe can guet a matrix result for the apply() function. If we compute across rows, sometimes will change the shape, we can use t() to prevent that.\n\napply(a, 2, range)\n\n     [,1] [,2] [,3]\n[1,]  101  106  111\n[2,]  105  110  115\n\ncat(\"\\n\")\nt( apply(a, 1, range) )\n\n     [,1] [,2]\n[1,]  101  111\n[2,]  102  112\n[3,]  103  113\n[4,]  104  114\n[5,]  105  115\n\n\nWhen we are looking for specific values we can get vectors with different lengths, in that case R returns a list.\n\ncat(\"Getting elements location where the value is greater than 109:\\n\")\n\nGetting elements location where the value is greater than 109:\n\napply(a, 2, function (x) which (x &gt; 109))\n\n[[1]]\ninteger(0)\n\n[[2]]\n[1] 5\n\n[[3]]\n[1] 1 2 3 4 5\n\n\n\n\n6.1.3 Missing Values\nThe function apply() is very useful to count the number of missing values. Next, let’s create a matrix using a semicolon to pass multiple commands in one line, also the multiple assigment operation to assign several things at once:\n\na &lt;- matrix(101:115, 5, 3); a[5, 3] &lt;- a[3, 1] &lt;- NA\na\n\n     [,1] [,2] [,3]\n[1,]  101  106  111\n[2,]  102  107  112\n[3,]   NA  108  113\n[4,]  104  109  114\n[5,]  105  110   NA\n\ncat(\"\\nSum of NA in each column: \\n\")\n\n\nSum of NA in each column: \n\napply(a, 2, function (x) sum (is.na(x)))\n\n[1] 1 0 1\n\n\nTracking NA values with which() to identify the position on the vector will return a number of the position for each NA. However, using arr.ind=TRUE argument we’ll obtain the coordinates:\n\nwhich(is.na(a))\n\n[1]  3 15\n\ncat(\"\\n\")\nwhich(is.na(a), arr.ind=TRUE)\n\n     row col\n[1,]   3   1\n[2,]   5   3\n\n\n\n\n6.1.4 Matrix Subscripts\nWith which() as a subscript we have the possibility to extract specific elements.\n\nb &lt;- matrix (1:20, nrow=4, byrow=T)\nb &lt;- cbind(b, c(3, 2, 0, 5))\ncolnames(b) &lt;- c(\"P1\", \"P2\", \"P3\", \"P4\", \"P5\", \"Use\")\nrownames(b) &lt;- c(\"Spring\", \"Summer\", \"Fall\", \"Winter\")\nb\n\n       P1 P2 P3 P4 P5 Use\nSpring  1  2  3  4  5   3\nSummer  6  7  8  9 10   2\nFall   11 12 13 14 15   0\nWinter 16 17 18 19 20   5\n\n\nGiven the previous matrix, the Use column specify which element of the row we want get. To generate this coordinates, we are going to generate a matrix, the first column will be the number of rows and the second column ‘Use’. Then we can pass that matrix as a subscript to ‘b’ and extract the elements.\n\n(filt &lt;- cbind(1:nrow(b), b[, \"Use\"]) )\n\n       [,1] [,2]\nSpring    1    3\nSummer    2    2\nFall      3    0\nWinter    4    5\n\ncat(\"\\n\")\nb[filt]\n\n[1]  3  7 20\n\n\n\n\n6.1.5 Higher-way Arrays\nA matrix with three or more ways is called Array in R. We can encounter this while constructing a higher way table():\n\nwho &lt;- rep( c(\"Mery\", \"Kelly\"), c(2, 6))\nwhen &lt;- rep( c(\"AM\", \"PM\"), 4)\nworked &lt;- c(T, T, F, T, F, T, F, T)\nsch &lt;- table(who, when, worked)\nsch\n\n, , worked = FALSE\n\n       when\nwho     AM PM\n  Kelly  3  0\n  Mery   0  0\n\n, , worked = TRUE\n\n       when\nwho     AM PM\n  Kelly  0  3\n  Mery   1  1\n\n\nThe function aperm() is like t() but for higher-way arrays. Also we can use c() to produce a vector, and apply() or prop.table() works too.",
    "crumbs": [
      "Annotations",
      "Base R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R: The very basics</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/00_basics_ndata.html#lists",
    "href": "to_book/notebooks/00_basics_ndata.html#lists",
    "title": "3  R: The very basics",
    "section": "6.2 Lists",
    "text": "6.2 Lists\nA list can have vector or diferent types and sizes, also might include matrices, lists or other R objects. Usually a list is returned by a modeling function in R but we can create a list with list():\n\nali &lt;- list( alpha = 1:4, beta = \"yes\", delta = log, 69)\nali\n\n$alpha\n[1] 1 2 3 4\n\n$beta\n[1] \"yes\"\n\n$delta\nfunction (x, base = exp(1))  .Primitive(\"log\")\n\n[[4]]\n[1] 69\n\n\n\nsplit() : divides a vector into pieces according to the value of another vector. Returns a list. Missing values in the second vector passed will be dropped.\n\n\nage &lt;- c(31, 33, 42, 54, 65, 74, 24)\ngender &lt;- c(\"F\", \"M\", \"F\", \"M\", \"M\", \"F\", \"F\")\n\ncat(\"Split age by gender:\\n\")\n\nSplit age by gender:\n\nsplit(age, gender)\n\n$F\n[1] 31 42 74 24\n\n$M\n[1] 33 54 65\n\ncat(\"\\nSplit ages when age is greater than 60\\n\")\n\n\nSplit ages when age is greater than 60\n\nsplit(age, age &gt; 60)\n\n$`FALSE`\n[1] 31 33 42 54 24\n\n$`TRUE`\n[1] 65 74\n\n\nWe can see with length() function the number of elements inside a list, and with lengths() function the length of each element.\n\nlength( split(age, gender) )\n\n[1] 2\n\ncat(\"\\nLengths: \\n\")\n\n\nLengths: \n\nlengths( split(age, gender) )\n\nF M \n4 3 \n\n\nThe str() command gives a description of every element on the list:\n\nstr(ali)\n\nList of 4\n $ alpha: int [1:4] 1 2 3 4\n $ beta : chr \"yes\"\n $ delta:function (x, base = exp(1))  \n $      : num 69\n\ncat(\"\\n\")\nclass(ali)\n\n[1] \"list\"\n\n\n\n6.2.1 Excracting from list\nWorking with a list, the rule says single backets produce a list, while double backets and dollar signs extract contents.\n\ncat(\"Extracting the content of element 4 as a vector: \\n\")\n\nExtracting the content of element 4 as a vector: \n\nali[[4]]\n\n[1] 69\n\ncat(\"\\nExtracting the element 1 as a list: \\n\")\n\n\nExtracting the element 1 as a list: \n\nali[1]\n\n$alpha\n[1] 1 2 3 4\n\ncat(\"\\nExtracting inside alpha the elements 2 and 4: \\n\")\n\n\nExtracting inside alpha the elements 2 and 4: \n\nali$alpha[c(F, T, F, T)] \n\n[1] 2 4\n\n\nThe function names() also works with lists returning the names of each element and “” if the element no have name. We can assign names with it too.\nWhen we extract an element from a list with dollar sign, we do not need to know the complete name, it is enough if it’s unambiguous:\n\nali$a\n\n[1] 1 2 3 4\n\ncat(\"\\n\")\nali$de\n\nfunction (x, base = exp(1))  .Primitive(\"log\")\n\n\nTo assign new values to an element we need the full name, otherwise will be creating a new ítem.\n\nali$delta &lt;- c(T, F, F, T)\nstr(ali)\n\nList of 4\n $ alpha: int [1:4] 1 2 3 4\n $ beta : chr \"yes\"\n $ delta: logi [1:4] TRUE FALSE FALSE TRUE\n $      : num 69\n\n\nTo remove an element from a list we can assign a NULL value:\n\nali$beta &lt;- NULL\nstr(ali)\n\nList of 3\n $ alpha: int [1:4] 1 2 3 4\n $ delta: logi [1:4] TRUE FALSE FALSE TRUE\n $      : num 69\n\n\n\nunlist() : try to turn the list into a vector:\n\n\nunlist(ali)\n\nalpha1 alpha2 alpha3 alpha4 delta1 delta2 delta3 delta4        \n     1      2      3      4      1      0      0      1     69 \n\n\nUsing dimnames() function we can see the names from a matrix as list format and also we can asign names:\n\nb\n\n       P1 P2 P3 P4 P5 Use\nSpring  1  2  3  4  5   3\nSummer  6  7  8  9 10   2\nFall   11 12 13 14 15   0\nWinter 16 17 18 19 20   5\n\ncat(\"\\n\")\ndimnames(b)\n\n[[1]]\n[1] \"Spring\" \"Summer\" \"Fall\"   \"Winter\"\n\n[[2]]\n[1] \"P1\"  \"P2\"  \"P3\"  \"P4\"  \"P5\"  \"Use\"\n\ndimnames(b)[[1]][3] &lt;- \"Autumn\"\ncat(\"\\n\")\nb\n\n       P1 P2 P3 P4 P5 Use\nSpring  1  2  3  4  5   3\nSummer  6  7  8  9 10   2\nAutumn 11 12 13 14 15   0\nWinter 16 17 18 19 20   5\n\n\nAn important list returned from a function is the list from options() which contain elements describing things like number of digits to be displayed, scientific notation, the editor… With names(options()) we can see the names of the current options.\n\nnames(options())[3:15]\n\n [1] \"callr.condition_handler_cli_message\" \"catch.script.errors\"                \n [3] \"CBoundsCheck\"                        \"check.bounds\"                       \n [5] \"citation.bibtex.max\"                 \"continue\"                           \n [7] \"contrasts\"                           \"defaultPackages\"                    \n [9] \"demo.ask\"                            \"deparse.cutoff\"                     \n[11] \"device\"                              \"device.ask.default\"                 \n[13] \"digits\"",
    "crumbs": [
      "Annotations",
      "Base R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R: The very basics</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/00_basics_ndata.html#data-frames",
    "href": "to_book/notebooks/00_basics_ndata.html#data-frames",
    "title": "3  R: The very basics",
    "section": "6.3 Data Frames",
    "text": "6.3 Data Frames\nA data.frame is a list of vector with the same length which can be arrayed in a matrix-like rectangle. Each vector in the list will be the columns. To construct a data frame we use data.frame() function.\n\ndf &lt;- data.frame (\n    Who = letters[1:5],\n    Cost = c(3, 2, 11, 4, 0),\n    Paid = c(F, T, F, T, T),\n    stringsAsFactors=FALSE\n)\nrow.names(df) &lt;- as.character (row.names(df))\ndf\n\n  Who Cost  Paid\n1   a    3 FALSE\n2   b    2  TRUE\n3   c   11 FALSE\n4   d    4  TRUE\n5   e    0  TRUE\n\n\nA data frame must have a column and row names, if they are not assigned R will create them. Also R ensure that column names are valid and not duplicated. For columns we can use colnames() or dimnames() to assign it, or names() with a list. For row names we have row.names() or rownames() functions.\nBy default the data.frame() function turns character vectors into factors but rarely we want it in data cleaning. We can chage this with stringAsFactors=FALSE argument.\n\nhead() : return the first six rows of a given dataframe. Second argument is n=6 to specify a number of rows. A negative number returns the last n rows.\ntail() : return the last six rows of a given dataframe. Second argument is n=6 to specify a number of rows.\nstr() : compact representation of the data frame with data type per column.\ndim() : returns the dimension, number of rows and columns.\nsummary() : returns a brief description of each column.\n\n\n6.3.1 Missing Values\nBecause a data frame can have differnt classes (data types) in its colums, the missing values can be of different classes too. A numeric vector will have numeric NA which is different to logical NA from a logical vector.\n\nis.na() : returns a logical matrix showing which elements are missing.\nanyNA() : returns TRUE or FALSE to the question There are missing values?\nna.omit() : omit the observations (rows) of the data frame in which one or more elements is missing. Also keeps a track, we can see the deleted observations with attr(df, \"na.action\") ; attr(df, \"class\").\n\nExtract and Assignment\nWith data frames we can use both matrix and list styles of subsetting operations. In data frames when we select a row returns a data frame because can store different types of data. Selecting a column will produce a vector.\nUsing a doble bracket or a dollar sign will produce a vector. Using the name produce a data frame.\n\ndf[4,]\n\n  Who Cost Paid\n4   d    4 TRUE\n\nf &lt;- df$Paid\ndf[f,]\n\n  Who Cost Paid\n2   b    2 TRUE\n4   d    4 TRUE\n5   e    0 TRUE\n\ndf[(df[\"Paid\"] == F),]\n\n  Who Cost  Paid\n1   a    3 FALSE\n3   c   11 FALSE\n\ndf[\"Cost\"]\n\n  Cost\n1    3\n2    2\n3   11\n4    4\n5    0\n\n\nTo extract a vector from a dataframe’s column:\n\ndf[[2]]\n\n[1]  3  2 11  4  0\n\ncat(\"\\n\")\ndf[[\"Cost\"]]\n\n[1]  3  2 11  4  0\n\ncat(\"\\n\")\ndf[[\"Co\", exact=FALSE]]\n\n[1]  3  2 11  4  0\n\n\nPassing drop=FALSE argument we avoid to extract a vector:\n\ndf[, c(\"Who\", \"Paid\")]\n\n  Who  Paid\n1   a FALSE\n2   b  TRUE\n3   c FALSE\n4   d  TRUE\n5   e  TRUE\n\ndf[, \"Who\"]\n\n[1] \"a\" \"b\" \"c\" \"d\" \"e\"\n\ndf[, \"Who\", drop=FALSE]\n\n  Who\n1   a\n2   b\n3   c\n4   d\n5   e\n\n\nTo delete a column we can asign NULL to that column: df$Paid &lt;- NULL\nIn a data frame or list if we pass a subscript for unexistance row wil produce one row with NA values. Sometimes happens when we delete a row and a program or ourselfs try to access to that row by name.",
    "crumbs": [
      "Annotations",
      "Base R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R: The very basics</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/00_basics_ndata.html#operating-on-lists-and-data-frames",
    "href": "to_book/notebooks/00_basics_ndata.html#operating-on-lists-and-data-frames",
    "title": "3  R: The very basics",
    "section": "6.4 Operating on Lists and Data Frames",
    "text": "6.4 Operating on Lists and Data Frames\nIf we try to use apply() in a data frame, the function will convert it to a matrix, and the whole matrix is of the same data type. Also, we cannot use apply() to a list because does not have dimensions.\n\nlapply() : returns a list\nsapply() : runs lapply() and tries to make the output into a vector or a matrix. But if the return have different lengths, it will need to return a list. If it try to return items with diferent types, will convert these to a common type, then its better in these case use lapply().\n\n\nsapply (df, class)\n\n        Who        Cost        Paid \n\"character\"   \"numeric\"   \"logical\" \n\n\n\n6.4.1 Spliy, Apply, Combine\nFirst the data is split, then a function is applied to each piece, and the results recombined. The function tapply() do exactly that, but also we can use split() and sapply() or lapply().\n\ndf2 &lt;- data.frame (\n    Age = c(35, 24, 42, 63, 56, 66),\n    Spouse = c(31, 36, 40, 59, 60, 60),\n    Gender = c(\"F\", \"F\", \"M\", \"F\", \"M\", \"M\")\n)\nsplit (df2$Age, df2$Gender)\n\n$F\n[1] 35 24 63\n\n$M\n[1] 42 56 66\n\ncat(\"\\nUsing sapply() to obtain the mean by gender :\\n\")\n\n\nUsing sapply() to obtain the mean by gender :\n\nsapply (split (df2$Age, df2$Gender), mean)\n\n       F        M \n40.66667 54.66667 \n\n\nAbove, split() divides Gender with Age values, then sapply() use mean() function to each part and returns a vector with the results (is the recombine). Let’s see tapply() :\n\nround( tapply(df2$Age, df2$Gender, mean), 3 )\n\n     F      M \n40.667 54.667 \n\n\nWe can use split() on a data frame unlike tapply(), doing that the data frame will be divided and then we can use sapply() or lapply() to each part.\n\nsplit (df2, df2$Gender)\n\n$F\n  Age Spouse Gender\n1  35     31      F\n2  24     36      F\n4  63     59      F\n\n$M\n  Age Spouse Gender\n3  42     40      M\n5  56     60      M\n6  66     60      M\n\ncat(\"\\nSummary differencing Gender: \\n\")\n\n\nSummary differencing Gender: \n\nlapply ( split (df2, df2$Gender), summary )\n\n$F\n      Age            Spouse        Gender         \n Min.   :24.00   Min.   :31.0   Length:3          \n 1st Qu.:29.50   1st Qu.:33.5   Class :character  \n Median :35.00   Median :36.0   Mode  :character  \n Mean   :40.67   Mean   :42.0                     \n 3rd Qu.:49.00   3rd Qu.:47.5                     \n Max.   :63.00   Max.   :59.0                     \n\n$M\n      Age            Spouse         Gender         \n Min.   :42.00   Min.   :40.00   Length:3          \n 1st Qu.:49.00   1st Qu.:50.00   Class :character  \n Median :56.00   Median :60.00   Mode  :character  \n Mean   :54.67   Mean   :53.33                     \n 3rd Qu.:61.00   3rd Qu.:60.00                     \n Max.   :66.00   Max.   :60.00                     \n\n\n\ncat(\"\\nLet's see what happens with sapply: \\n\")\n\n\nLet's see what happens with sapply: \n\nsapply ( split (df2, df2$Gender), summary )\n\n      F                    M                   \n [1,] \"Min.   :24.00  \"    \"Min.   :42.00  \"   \n [2,] \"1st Qu.:29.50  \"    \"1st Qu.:49.00  \"   \n [3,] \"Median :35.00  \"    \"Median :56.00  \"   \n [4,] \"Mean   :40.67  \"    \"Mean   :54.67  \"   \n [5,] \"3rd Qu.:49.00  \"    \"3rd Qu.:61.00  \"   \n [6,] \"Max.   :63.00  \"    \"Max.   :66.00  \"   \n [7,] \"Min.   :31.0  \"     \"Min.   :40.00  \"   \n [8,] \"1st Qu.:33.5  \"     \"1st Qu.:50.00  \"   \n [9,] \"Median :36.0  \"     \"Median :60.00  \"   \n[10,] \"Mean   :42.0  \"     \"Mean   :53.33  \"   \n[11,] \"3rd Qu.:47.5  \"     \"3rd Qu.:60.00  \"   \n[12,] \"Max.   :59.0  \"     \"Max.   :60.00  \"   \n[13,] \"Length:3          \" \"Length:3          \"\n[14,] \"Class :character  \" \"Class :character  \"\n[15,] \"Mode  :character  \" \"Mode  :character  \"\n[16,] NA                   NA                  \n[17,] NA                   NA                  \n[18,] NA                   NA                  \n\n\nThe previous sapply() will try to construct a vector or matrix converting everything to a common type.\nA function to produce the same result is by() but without letting you save the list. It will performs the summary() operation on each column, broken down by gender:\n\nby ( df2, df2$Gender, summary )\n\ndf2$Gender: F\n      Age            Spouse        Gender         \n Min.   :24.00   Min.   :31.0   Length:3          \n 1st Qu.:29.50   1st Qu.:33.5   Class :character  \n Median :35.00   Median :36.0   Mode  :character  \n Mean   :40.67   Mean   :42.0                     \n 3rd Qu.:49.00   3rd Qu.:47.5                     \n Max.   :63.00   Max.   :59.0                     \n------------------------------------------------------------ \ndf2$Gender: M\n      Age            Spouse         Gender         \n Min.   :42.00   Min.   :40.00   Length:3          \n 1st Qu.:49.00   1st Qu.:50.00   Class :character  \n Median :56.00   Median :60.00   Mode  :character  \n Mean   :54.67   Mean   :53.33                     \n 3rd Qu.:61.00   3rd Qu.:60.00                     \n Max.   :66.00   Max.   :60.00                     \n\n\nSometimes the task split, apply and combine can be performed altogether, but but other times might require separate functions. Packages like dplyr for data frames or plyr for lists and arrays. Both are intended to be fast and efficient and to permit parallel computation.\n\n\n6.4.2 All-Numeric Data Frames\nA data frame with all of its values are logical or numeric when is converted to a matrix the numeric type is preserved.\nTo convert vectors to another class exists as.numeric() and the other as. functions, also there are as.matrix() and as.data.frame() to convert data frames to matrices and vice versa. It is useful for all-numeric data frames and older functions that require numeric matrices.\n\n\n6.4.3 Convenience Functions\nIt is recommended to users to use long names for data objects and columns for increases readability. But sometimes leads to use long line expression like:\nCustPayment2016$JanDebt + CustPayment2016$FebPurch - CustPayment2016$FebPmt\nThese can be handled with the functions:\n\nwith() : to perform operations on a data frame. First argument is the data frame, then the expression to be performed. Cannot be assigned to.\nwith ( CustPayment2016, JanDebt `FebPurch - FebPmt )\nwithin() : works in the same wey but unlike with(), this function can be assigned.\nCustPayment2016 &lt;- within ( CustPayment2016, FebDebt &lt;- JanDebt + FebPurch - FebPmt )\n\nFor beginners and to use then interactively and not for programming there are functions to make the subsetting and transformation process easier ( but generally its avoided):\n\nsubset() : To extract rows given a condition:\n\nsubset ( df, Paid == TRUE)\n\n  Who Cost Paid\n2   b    2 TRUE\n4   d    4 TRUE\n5   e    0 TRUE\n\n# Is the alternative to:\ndf[ df$Paid == TRUE, ]\n\n  Who Cost Paid\n2   b    2 TRUE\n4   d    4 TRUE\n5   e    0 TRUE\n\n\ntransform() : To specify transformations to existing columns and returns the updated version.\n\nTo change the way we write functions we can use a ‘pipe’ as %&gt;% provided by magrittr package. The pipe allows a function’s output to serve as input to another function. Not every function is suitable for piping, the pipes are particularly useful for nested functions.\n\ncat(\"Nested: \\n\")\n\nNested: \n\ncos (log (sqrt (8 - 3) ) )\n\n[1] 0.6933138\n\nlibrary(\"magrittr\")\ncat(\"\\nUsing pipes: \\n\")\n\n\nUsing pipes: \n\n(8 - 3) %&gt;% sqrt %&gt;% log %&gt;% cos \n\n[1] 0.6933138\n\n\n\n\n6.4.4 Re-Ordering, De-Duplicating, Sampling\nWe can create a vector with order() which contain indices sorted by the variables inside.\ndf\nnew.order &lt;- order( df$ID, df$Date )\n# convenient alternative:\nnew.order &lt;- with ( df, order (ID, Date) )\n\n# 'new.order' have a vector with the rows sort by increasing ID\n# To have the data frame with the new order:\ndf[new.order, ]\nFor duplicated values, we can use unique() passing the data frame to get every non-duplicated row. Just the floating-point error might be a problem detecting identical rows.\n\nsample() : first argument is the number of total rows, the second argument is the size of the sample we want. By default the result is a random set of integers without replacement. The row number of sampled data frame will be the same that originals.\n# Sample of 200 rows:\n# First, create a vector selecting the rows\ns200 &lt;- sample ( nrow(df), 200 ) \n\n# Using the sample vector to do the sampling:\ndf[s200, ]",
    "crumbs": [
      "Annotations",
      "Base R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R: The very basics</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/00_basics_ndata.html#date-and-time-objects",
    "href": "to_book/notebooks/00_basics_ndata.html#date-and-time-objects",
    "title": "3  R: The very basics",
    "section": "6.5 Date and Time Objects",
    "text": "6.5 Date and Time Objects\n\n6.5.1 Formatting Dates\nThe class Date internally store an integer representing the number of days since a particular origin date.\nDates can be represented in many shapes, american’s form, rest-of-the-world form, etc. the as.Date() function converts text into Date class.\n\n# First '0' will be converted into origin date.\ndd &lt;- as.Date ( c(0, 18252:18255), origin = \"1970-01-01\" )\n\ndd\n\n[1] \"1970-01-01\" \"2019-12-22\" \"2019-12-23\" \"2019-12-24\" \"2019-12-25\"\n\n\nAlso can convert text based representations in Date class such as “7/4/2019”. In this case we have to pass format= argument, and the format string had to contain the same pattern that the input text:\n\n%b ; %B : name of the month\n%a ; %A : name of the day of the week\n%d : day of the month\n%m : month in number\n%y ; %Y : for the yy or YYYY year.\n%F : Equivalent to %Y-%m-%d.\n%x : Date. Locale-specific on output, \"%y/%m/%d\" on input.\n\n\nas.Date ( c(\"Feb 29, 2016\", \"Feb 29, 2017\", \"September 30, 2017\"), \n          format = \"%b %d, %Y\" )\n\n[1] \"2016-02-29\" NA           \"2017-09-30\"\n\n# The second date will result NA because is not valid\n\nThe names of the days of the week and the months of the year are in computer’s locale, to read other languages we can change locales R.\n\nsp.dt &lt;- c(\"3 octubre 2016\", \"26 Marzo 2017\")\n\n( dts &lt;- as.Date (sp.dt, format = \"%d %B %Y\") )\n\n[1] NA NA\n\n# Changing locales to spanish:\nSys.setlocale (\"LC_TIME\", \"Spanish\")\n\n[1] \"Spanish_Spain.1252\"\n\n( dts &lt;- as.Date (sp.dt, format = \"%d %B %Y\") )\n\n[1] \"2016-10-03\" \"2017-03-26\"\n\n# Changing back:\nSys.setlocale (\"LC_TIME\", \"UK\")\n\n[1] \"English_United Kingdom.1252\"\n\n\n\n\n6.5.2 Operations on Date Objects\nThe functions months() and weekdays() returns the month name and weekday from passing Date object. It can be added the abbreviate=TRUE argument to abbreviate the output.\nTo extract the numeric month, day or year it can be used the format() function and then using as.numeric() to convert the character output to numeric.\n\ndt1 &lt;- as.Date (\"2024/05/17\")\ndt2 &lt;- as.Date (\"2024-07-03\")\n\n# Extracting weekdays:\nweekdays( c(dt1, dt2) )\n\n[1] \"Friday\"    \"Wednesday\"\n\ncat(\"\\n\")\n# Extracting month name:\nmonths( c(dt1, dt2), abbreviate = T )\n\n[1] \"May\" \"Jul\"\n\ncat(\"\\n\")\n# Identifying Quarters:\nquarters( c(dt1, dt2) )\n\n[1] \"Q2\" \"Q3\"\n\ncat(\"\\n\")\nformat ( c(dt1, dt2), \"%y\")\n\n[1] \"24\" \"24\"\n\nformat ( c(dt1, dt2), \"%d\")\n\n[1] \"17\" \"03\"\n\nformat ( c(dt1, dt2), \"%A, %B %d, %Y\")\n\n[1] \"Friday, May 17, 2024\"     \"Wednesday, July 03, 2024\"\n\n\nThe difference between dates is a period of time stored as difftime object. Functions such as mean() and range() works well but hist() or summary() fails producing the expected results.\nUsually we will convert difftime objects to numeric with as.numeric() function, for that will be a good habit to specify units = \"days\" argument (with the unit we want.\n\ncat(\"Date1 adding 30 days: \\n\")\n\nDate1 adding 30 days: \n\ndt1 + 30\n\n[1] \"2024-06-16\"\n\ncat(\"\\nDifference between date1 and date2: \\n\")\n\n\nDifference between date1 and date2: \n\n(dd &lt;- dt2 - dt1)\n\nTime difference of 47 days\n\ncat(\"\\nDifference to numeric: \\n\")\n\n\nDifference to numeric: \n\nas.numeric (dd)\n\n[1] 47\n\nunits(dd)\n\n[1] \"days\"\n\ncat(\"\\nTo numeric but in weeks: \\n\")\n\n\nTo numeric but in weeks: \n\nas.numeric (dd, units=\"weeks\")\n\n[1] 6.714286\n\n\n\n\n6.5.3 POSIXt Objects\nPOSIXlt object is implemented as a list, meanwhile POSIXct object is like a number useful is will be stored in a column.\n\n(ready &lt;- as.POSIXlt(\"2017-01-17 14:51:23\"))\n\n[1] \"2017-01-17 14:51:23 CET\"\n\ncat(\"\\n\")\nunlist (ready)\n\n   sec    min   hour   mday    mon   year   wday   yday  isdst   zone gmtoff \n  \"23\"   \"51\"   \"14\"   \"17\"    \"0\"  \"117\"    \"2\"   \"16\"    \"0\"  \"CET\"     NA \n\ncat(\"\\n\")\n( mdayn &lt;- as.numeric(unlist (ready)[\"mday\"]) )\n\n[1] 17\n\n\nIn this last example we can see January is month 0 then December is month 11. Weekday is 0 to 6 starting on Sunday.\nHere it can be used too the weekdays(), months() and quarters() functions, as well as format() function. This will be less efficient than the list-type extraction, and it is recommended use POSIXct objects where possible because changing time zones with POSIXlt can encounter unexpected behavior.\nWhen we convert a Date object to POSIXt the time will be 00:00 (midnight), and when it converted from POSIXt to Date, the time will be truncated.\nas.POSIXct() and as.POSIXlt() works like as.Date() but the date can be followed by 24-hour clock time (or 12h with AM/PM).\n\n( ct1 &lt;- as.POSIXct (\"Mar 30, 2017 12:26:08 am\", \n                     format = \"%b %d, %Y %I:%M:%S %p\") )\n\n[1] \"2017-03-30 00:26:08 CEST\"\n\ncat(\"\\n\")\n(ct2 &lt;- as.POSIXct (\"2017-03-29 22:26:08\", tz = \"UTC\"))\n\n[1] \"2017-03-29 22:26:08 UTC\"\n\ncat(\"\\nLooking for diferences: \\n\")\n\n\nLooking for diferences: \n\nas.numeric (ct1 - ct2, units = \"secs\")\n\n[1] 0\n\n\nAll the objects in a vector of length &gt;1 including weekdays() and months() will be displayed with the local time zone. For a single object these functions refer to the time zone of the object:\n\nc(ct1, ct2)\n\n[1] \"2017-03-30 00:26:08 CEST\" \"2017-03-30 00:26:08 CEST\"\n\ncat(\"\\n\")\nweekdays(c(ct1, ct2))\n\n[1] \"Thursday\" \"Thursday\"\n\ncat(\"\\n\")\nweekdays(ct2)\n\n[1] \"Wednesday\"\n\n\nThe time zone can be converted changing tzone attribute:\nattr ( ct1, tzone = \"UTC\" )\nThe help of Sys.timezone() containing the names of the time zones. When a POSIXct object is converted to Date object is rendered in UTC time zone by default:\n\nas.Date(ct2)\n\n[1] \"2017-03-29\"\n\nas.Date(ct1)\n\n[1] \"2017-03-29\"\n\n\nExample of POSIXct formatting flexibility:\n\n(crdt &lt;- date() )\n\n[1] \"Tue Nov  5 12:35:44 2024\"\n\ncat(\"\\n\")\n(now &lt;- as.POSIXct (crdt, format = \"%A %B %d %H:%M:%S %Y\" ) )\n\n[1] \"2024-11-05 12:35:44 CET\"\n\ncat(\"\\n\")\nas.Date (now)\n\n[1] \"2024-11-05\"\n\n\nMath Functions\nDate and POSIXt objects are numeric, then we can use some functions such as range(), max(), min(), mean(), median(), which will produce vectors of date objects.\ndiff() : computes differences between adjacent elements in a vector.\n\ndiff ( range( c(ct1, ct2) ) )\n\nTime difference of 0 secs\n\n\nThe function table() works too but does not work on POSIXlt (list) objects.\nWe can also create sequences with seq() function, it can be specified by=\"day\" argument:\n\nseq ( as.Date(\"2020-03-11\"), by = 3, length = 5 )\n\n[1] \"2020-03-11\" \"2020-03-14\" \"2020-03-17\" \"2020-03-20\" \"2020-03-23\"\n\ncat(\"\\n\")\npdt &lt;- as.POSIXct( (\"2020-03-11 13:05:00\") )\nseq ( pdt, by = 2, length = 3)\n\n[1] \"2020-03-11 13:05:00 CET\" \"2020-03-11 13:05:02 CET\"\n[3] \"2020-03-11 13:05:04 CET\"\n\ncat(\"\\nSequence by days:\\n\")\n\n\nSequence by days:\n\nseq (pdt, by = \"day\", length = 3)\n\n[1] \"2020-03-11 13:05:00 CET\" \"2020-03-12 13:05:00 CET\"\n[3] \"2020-03-13 13:05:00 CET\"\n\ncat(\"\\nBy day without altering the hour: \\n\")\n\n\nBy day without altering the hour: \n\nseq (pdt, by = \"DSTday\", length = 3)\n\n[1] \"2020-03-11 13:05:00 CET\" \"2020-03-12 13:05:00 CET\"\n[3] \"2020-03-13 13:05:00 CET\"\n\n\nWith POSIXt object using by=\"day\" the clock moves 24 hours, but by=\"DSTday\" moves 1 day preserving the clock.\nWe have to be more careful with POSIXt and difftime objects:\n\nd1 &lt;- as.POSIXct (\"2017-05-01 12:00:00\")\nd2 &lt;- as.POSIXct (\"2017-05-01 12:00:06\")\nd3 &lt;- as.POSIXct (\"2017-05-07 12:00:00\")\ncat(\"Are equal (d2 - d1) and (d3 - d1) ? :\\n\")\n\nAre equal (d2 - d1) and (d3 - d1) ? :\n\n(d2 - d1) == (d3 - d1)\n\n[1] FALSE\n\ncat(\"\\nSame operation but using as.numeric() : \\n\")\n\n\nSame operation but using as.numeric() : \n\nas.numeric(d2 - d1) == as.numeric(d3 - d1)\n\n[1] TRUE\n\ncat(\"\\nSame as.numeric but with units=days:\\n\")\n\n\nSame as.numeric but with units=days:\n\nas.numeric(d2 - d1, units=\"days\") == as.numeric(d3 - d1, units=\"days\")\n\n[1] FALSE\n\n\n\n\n6.5.4 Missing Values\nIn a vector dates of different classes should not be combined, a good practice is to use a function to force all the elements to have the same class.\n\nc(d1, NA)\n\n[1] \"2017-05-01 12:00:00 CEST\" NA                        \n\ncat(\"\\n\")\nc(NA, d1)\n\n[1]         NA 1493632800\n\ncat(\"\\n\")\nc( as.Date(NA), d1)\n\n[1] NA           \"2017-05-01\"\n\ncat(\"\\n\")\nc( as.POSIXct(NA), d1)\n\n[1] NA                         \"2017-05-01 12:00:00 CEST\"\n\n\nThe first c(d1, NA) knows what to do with the NA value because d1 is first, but the second one do not, then convert the vector into numeric. That not happens in the third and fourth command when we specify which class is the NA value.\n\n\n6.5.5 Apply() on Dates\nOften a data set will have a series of dates in each row, like with a data frame is better to use lapply() and sapply() functions:\n\ndt.df &lt;- data.frame(\n    Start = c( seq (as.Date(\"2020-05-03\"), by=\"months\", length = 6) )\n)\n\n# Creating a new column\ndt.df$End &lt;- c( seq (as.Date(\"2021-06-02\"), by=\"months\", length = 6) )\n\ndt.df\n\n       Start        End\n1 2020-05-03 2021-06-02\n2 2020-06-03 2021-07-02\n3 2020-07-03 2021-08-02\n4 2020-08-03 2021-09-02\n5 2020-09-03 2021-10-02\n6 2020-10-03 2021-11-02\n\n\n\nsapply ( 1:nrow(dt.df), \n         function (i) as.numeric (dt.df[i,2] - dt.df[i,1], \n                                  units = \"days\") ) \n\n[1] 395 394 395 395 394 395",
    "crumbs": [
      "Annotations",
      "Base R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R: The very basics</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/00_basics_ndata.html#actions-with-data-frames",
    "href": "to_book/notebooks/00_basics_ndata.html#actions-with-data-frames",
    "title": "3  R: The very basics",
    "section": "6.6 Actions with Data Frames",
    "text": "6.6 Actions with Data Frames\n\n6.6.1 Combining by Rows or Columns\nThe function data.frame() can be used with two data frames inside and will be joined. Also cbind() function is useful. Both can incorporate vectors and matrices as well. Remember that characters will be converted into Factors unless stringAsFactors=FALSE.\nIt is a good idea to check duplicated column names before combining, with intersection() function for example. If there are duplicated names R will use make.names() to generate unique alternatives (adding .1 .2 .3 …).\n\nnames(df)\n\n[1] \"Who\"  \"Cost\" \"Paid\"\n\nnames(df2)\n\n[1] \"Age\"    \"Spouse\" \"Gender\"\n\n# Forcing 'intersect' to get a column names coincidence\ndf2$Paid &lt;- c(NA,NA,NA,NA,NA,NA)\nintersect( names(df), names(df2) )\n\n[1] \"Paid\"\n\n\nWith rbind() we can combine data frames vertically (by rows). The columns in both data frames have to be the same, number and name, but not the order. Also the columns it’s recommended to have the same class or R will convert to the common class. Pass stringAsFactor=FALSE with rbind() to ensure the character columns.\n# Checking column names on two data frames:\nnam1 &lt;- names (df1)\nnam2 &lt;- names (df2)\n\nall (sort (nam1) == sort (nam2) )\nAbove we sort the names of each data frame with a comparation expecting TRUE as result.\n# Checking column classes\ncs1 &lt;- sapply (df1, class)\ncs2 &lt;- sapply (df2, class)\n\nisTRUE (all.equal (cs1, cs2[names (cs1) ] ) )\nAnd now we compare both classes.\n\nall.equal() : compares two objects and returns TRUE if the match. Also returns a report if there are differences.\nisTRUE() : returns TRUE if its argument is a single TRUE (expected for all.equal() ) or FALSE if there are something else.\n\nWhen the rows have basic numeric names R will adjust the resulting rows. But with modified row names R will try to keep them and taking care of the matches.\nIf we have a large number of data frames, for example in a list as the result of lapply() function, we can use do.call() to combine them. The function takes the name of a function to be run and a list of arguments and run the function with those arguments:\n# Result of lapply() function:\nlist.of.df\n\ndo.call ( \"rbind\", list.of.df )\nIn the command above we are assuming that data frames meet the rbind() criteria. Other basic example can be: do.call(\"log\", list(x = 32, base = 2) where the list are the argument of log().\n\n\n6.6.2 Merging Data Frames\nTo merge() usually the data frames to merge have a “key” field. Then merge() matches up the key and produces a data frame with one row per key with the columns of both data frames.\n\nmerge() arguments:\n\n(all.x=FALSE, all.y=FALSE) : default options. One row for each key that appears in bot x and y data frames (except when there are duplicated keys). Is an 'inner join'.\n(all.x=TRUE, all.y=FALSE) : One row for each key in x and columns of the corresponding keys that do not appear in y are filled with NA values. Is an 'left join'.\n(all.x=FALSE, all.y=TRUE) : Is the complementary one, an ‘right join’.\n(all.x=TRUE, all.y=TRUE) : This id the ‘outer join’, when the result has one row for every key in either x or y.\n\n\nWhen the keys are duplicated the function merge() does not care, it will do it. It is best to remove rows with duplicated keys or to create a new column with a unique key, before merging.\nIf the key match approximately (people names) the functions adist() and agrep() help find keys that match approximately.\n\n\n6.6.3 Comparing Data Frames\n\nidentical() : test for every strict equivalence. Returns TRUE when the two items are equal. Should not be applied to POSIXlt or data frames with this object.\nall.equal() : compares two objects but with more room for difference. Returns TRUE when two items are equal. By default is a match between names and attributes of two data frames. Correct way to compare: isTRUE(all.equal(df1, df2))\n\ntolerance= how different two numbers need to be to be declarated different.\n\n\n\n\n6.6.4 View and Editing Data Frames\n\nView() : shows a dear-only representation of a data frame.\nedit() : allows change to be made. Can be saved to reflect the changes\ndata.entry() : the changes are saved automatically.\n\nIs strongly recommended to use commented scripts and functions because in that way all the steps in the process will be reproducible.",
    "crumbs": [
      "Annotations",
      "Base R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R: The very basics</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/00_basics_ndata.html#handling-big-data",
    "href": "to_book/notebooks/00_basics_ndata.html#handling-big-data",
    "title": "3  R: The very basics",
    "section": "6.7 Handling Big Data",
    "text": "6.7 Handling Big Data\nR store the data in main memory on the machine being used.\nBash include tools to provide the ability to break the data into manageable pieces:\n\nsplit : breaks up a data set by rows.\ncut : extracts specific columns.\nshuf : which permits the lines in a file, which helps when taking random samples.\n\nThe data.table package advertises very fast subsetting and tabulation.\nThere are add-in packages to maintain “pointers” to data on disk, it is slower than memory but the storage can be expected to be huge:\n\nbigmemory\nff\ntm",
    "crumbs": [
      "Annotations",
      "Base R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R: The very basics</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/00_basics_ndata.html#data-handling-tools",
    "href": "to_book/notebooks/00_basics_ndata.html#data-handling-tools",
    "title": "3  R: The very basics",
    "section": "6.8 Data Handling Tools",
    "text": "6.8 Data Handling Tools\nTo account for columns with a vector of two or more classes we can extract the classes into a variable:\n\ndf2\n\n  Age Spouse Gender Paid\n1  35     31      F   NA\n2  24     36      F   NA\n3  42     40      M   NA\n4  63     59      F   NA\n5  56     60      M   NA\n6  66     60      M   NA\n\n\n\n( col.cls &lt;- sapply(df2, function (x) class (x)[1] ) )\n\n        Age      Spouse      Gender        Paid \n  \"numeric\"   \"numeric\" \"character\"   \"logical\" \n\n\n\ntable() to tabulate the columns classes.\n\ntable(col.cls)\n\ncol.cls\ncharacter   logical   numeric \n        1         1         2 \n\n\nTo count missing values by column, or count negative or a number equal to 99 like other ‘missing’ values:\n\nsapply( df2, function(x) sum(is.na(x) ) )\n\n   Age Spouse Gender   Paid \n     0      0      0      6 \n\n\n\nIf there are a large number of columns we can pass sapply() results to table()\n\n\nsapply( df2, function(x) sum(is.na(x) ) ) |&gt; table()\n\n\n0 6 \n3 1 \n\n\n\nsapply( df2, function(x) sum (x &lt; 0, na.rm = TRUE) )\n\n   Age Spouse Gender   Paid \n     0      0      0      0 \n\n\nTo compute the ranges of numeric columns in a search of outliers or anomalies.\n\nsapply (df2[, col.cls %in% c(\"numeric\", \"integer\") ], range, na.rm=TRUE ) \n\n     Age Spouse\n[1,]  24     31\n[2,]  66     60\n\n\nTo count unique values by column:\n\nsapply(df2, function(x) length(unique(na.omit(x) ) ) )\n\n   Age Spouse Gender   Paid \n     6      5      2      0 \n\n\n\nThe apply() function converts the data frame to a matrix first, should be used if all the columns of a data frame are of the same type. The sapply() function tries to return a vector or a matrix if it can, so if the return elements are of different classes they will often be converted. It is better to use lapply() unless we know that one of the other functions will succeed.",
    "crumbs": [
      "Annotations",
      "Base R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R: The very basics</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/00_basics_ndata.html#character-data",
    "href": "to_book/notebooks/00_basics_ndata.html#character-data",
    "title": "3  R: The very basics",
    "section": "7.1 Character Data",
    "text": "7.1 Character Data\n\n7.1.1 length() and nchar()\nOn a vector using length() we extract how many elements are in it, and using nchar() we extract how many letters there are in each element. The argument keepNA=FALSE allow nchar() to count NA values returning 2 of length.\n\nwriters &lt;- c(\"asimov\", \"tolstoi\", \"wolf\", \"sanderson\", \"joyce\")\nlength(writers)\n\n[1] 5\n\nnchar(writers)\n\n[1] 6 7 4 9 5\n\n\n\n\n7.1.2 Escaped characters\nIn character strings some characters are protected, we have to use backslash ( \\ ) to use them.\nWe have to escape the double quotation marks and the backslash:\nShe wrote, “To enter a ‘new-line,’ type”\\n” .”\n\n( quo &lt;- \"She wrote, \\\"To enter a 'new-line,' type \\\"\\\\n\\\" .\\\"\" )\n\n[1] \"She wrote, \\\"To enter a 'new-line,' type \\\"\\\\n\\\" .\\\"\"\n\nnchar (quo)\n\n[1] 47\n\ncat(quo, \"\\n\")\n\nShe wrote, \"To enter a 'new-line,' type \"\\n\" .\" \n\n\nEmpty String\nA vector length 0 corresponds to character(0). An empty string is a vector in which an element have spaces or not, indicated by \"\". It will be length 1 and nchar 0. Spreadsheets will sometimes produce this empty strings or strings of spaces:\n\nblanks &lt;- c (\" \", \" \", \"\", \"   \", \"\", \"2016\", \"\", \" 2016\", \"2016\", \"   \")\nlength(blanks)\n\n[1] 10\n\ntable(blanks)\n\nblanks\n                   2016  2016 \n    3     2     2     1     2 \n\nnames(table(blanks))\n\n[1] \"\"      \" \"     \"   \"   \" 2016\" \"2016\" \n\nnzchar(blanks)\n\n [1]  TRUE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE\n\n# Removing white spaces located in the start or end of a element:\nnoblanks &lt;- trimws(blanks)\ntable(noblanks)\n\nnoblanks\n     2016 \n   7    3 \n\nnames(table(noblanks))\n\n[1] \"\"     \"2016\"\n\n\nThe function nzchar() returns TRUE for strings that have non-zero length and FALSE for empty strings. And with trimws() we can remove blanks at the beginning and end of each element.\n\n\n7.1.3 Substrings\nFrequently in data cleaning we have to extract a piece of string, like a year or Zip code. For this task the substring() function takes a piece of text:\n\nsubstring() : given a vector extract the selected text.\n\nfirst= position of the first character to extract\nlast= position of the last character up to 1 Million. Can be omitted to get the end of the string.\n\n\nAlso, substring() can be on the left side of an assignment, then we can add some text to each element of a vector.\n\ndt1 &lt;- \"2017-02-03\"\n# Extracting the year\nsubstring(dt1, 1, 4)\n\n[1] \"2017\"\n\n# Extracting day and month (last five characters)\nsubstring (dt1, nchar(dt1) - 4)\n\n[1] \"02-03\"\n\n\nTo break a string into its individual characters we can do it with strsplit() and also with substring():\n\nsubstring(dt1, 1:nchar(dt1), 1:nchar(dt1) )\n\n [1] \"2\" \"0\" \"1\" \"7\" \"-\" \"0\" \"2\" \"-\" \"0\" \"3\"\n\nstrsplit(dt1, split = NULL)\n\n[[1]]\n [1] \"2\" \"0\" \"1\" \"7\" \"-\" \"0\" \"2\" \"-\" \"0\" \"3\"\n\n\nTo change the last letters of a character element with substring() :\n\nholid &lt;- month.name\nsubstring (holid[6:8], nchar(holid[6:8]) - 0 ) &lt;- \"9\"\nholid[5:9]\n\n[1] \"May\"       \"Jun9\"      \"Jul9\"      \"Augus9\"    \"September\"\n\n\n\n\n7.1.4 Case and Substitutions\nWe will need sometimes to manipulate the case of characters because R is case-sensitive.\ntolower(), toupper(), casefold() functions perfrom these convertions. Using casefold() have to be passed the argument upper= being TRUE or FALSE which change to upper or lower case respectively.\nThe function chartr() is a general solution for substitutions. It takes two arguments that are vectors of characters and changes each character in the first argument into the corresponding character in the second argument.\nTo capitalize the every first letter of a character string, from help(\"casefold\"):\n\n.simpleCap &lt;- function(x) {\n    s &lt;- strsplit(x, \" \")[[1]]\n    paste(toupper(substring(s, 1, 1)), substring(s, 2),\n          sep = \"\", collapse = \" \")\n}\n.simpleCap(\"the quick red fox jumps over the lazy brown dog\")\n\n[1] \"The Quick Red Fox Jumps Over The Lazy Brown Dog\"",
    "crumbs": [
      "Annotations",
      "Base R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R: The very basics</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/00_basics_ndata.html#converting-numbers-into-text",
    "href": "to_book/notebooks/00_basics_ndata.html#converting-numbers-into-text",
    "title": "3  R: The very basics",
    "section": "7.2 Converting numbers into text",
    "text": "7.2 Converting numbers into text\n\n7.2.1 Formatting Numbers\n\nformat() : is a way to format a set of numbers in a common way. E.g. lining up decimal points and commas.\n\ndigits= number of digits\nnsmall= number of digits (minimum) in the ‘small’ part (the right of the decimal point).\nbig.mark= determine the comma in the ‘big’ part (the thousand mark).\ndrop0trailing , removes trailing zeros in the small part.\nzero.print= , if TRUE, causes zeros to be printed with spaces.\n\n\n\nformat ( c(12.23, 1234.45678, 0, 10.000) )\n\n[1] \"  12.230\" \"1234.457\" \"   0.000\" \"  10.000\"\n\nformat ( c(12.23, 1234.45678, 0, 10.000), big.mark=\",\" )\n\n[1] \"   12.230\" \"1,234.457\" \"    0.000\" \"   10.000\"\n\nformat ( c(12.23, 1234.45678, 0, 10.000),\n         digits = 6, nsmall = 2, zero.print = FALSE, width = 2)\n\n[1] \"  12.23\" \"1234.46\" \"       \" \"  10.00\"\n\n\nDecimal part by default will be aligned. The last example shows a currency type format, which the digits and nsmall arguments have to be chosen carefully to produce two decimals.\nWith sprintf() we can include text and add leading zeros. The function have a format string containing text and conversion strings, which describe how numbers and other variables should appear in that output. A conversion strings start with a percent sign and contain modifiers and then a conversion character.\nThe conversion character %i or %d are for integer values, %f is for double-precision numerics, and %s is for character strings. This field can be formatted with two numbers separated by a period, the first one give the minimum width (total number of characters) and the second one is the number of digits to the right of the decimal points.\n\n# 8 characters, 2 decimals.\nsprintf ( \"%9.2f\", 1230.456789 )\n\n[1] \"  1230.46\"\n\n# 0 leading the character to fill with 0 until 8 characters.\nsprintf ( \"%09.2f\", 1230.456789 )\n\n[1] \"001230.46\"\n\n# Like the previous one but with spaces\nsprintf ( \"% 9.2f\", 1230.456789 )\n\n[1] \"  1230.46\"\n\n# Always a simbol leading the number\nsprintf ( \"%+9.2f\", 1230.456789 )\n\n[1] \" +1230.46\"\n\n# Left Justified\nsprintf ( \"%-9.2f\", 1230.456789 )\n\n[1] \"1230.46  \"\n\n# exponential\nsprintf ( \"%9.3g\", 1230.456789 )\n\n[1] \" 1.23e+03\"\n\n\nAlso we can use sprintf() with more than one vectors:\n\ncosts &lt;- c(3, 22, 456.32, 89340.4235, 1230045605.959)\nsprintf ( \"I spent $%.0f in %s\", costs, month.name[2:6] )\n\n[1] \"I spent $3 in February\"      \"I spent $22 in March\"       \n[3] \"I spent $456 in April\"       \"I spent $89340 in May\"      \n[5] \"I spent $1230045606 in June\"\n\n\nIn the conversion, the field width or precision can be passed as an argument if we specify an asterisk in the format:\n\nbigs &lt;- max (nchar (sprintf (\"%.2f\", costs) ) )\n\nsprintf (\"spent $%*.2f in %s\",\n         bigs, costs, month.name[2:6] )\n\n[1] \"spent $         3.00 in February\" \"spent $        22.00 in March\"   \n[3] \"spent $       456.32 in April\"    \"spent $     89340.42 in May\"     \n[5] \"spent $1230045605.96 in June\"    \n\n\nThe function sprintf() can be very useful to generate labels:\n\nsprintf (\"%03d\", 1:15 )\n\n [1] \"001\" \"002\" \"003\" \"004\" \"005\" \"006\" \"007\" \"008\" \"009\" \"010\" \"011\" \"012\"\n[13] \"013\" \"014\" \"015\"\n\n\n\n\n7.2.2 Scientific Notation\nWhen you represent a number by an optional sign, a number between 1 and 10 and a multiplier of a power of 10.\nIn R we can use options() to change scipen option. If scipen=999 the scientific notion will be disabled, and scipen=-999 will force scientific notion. When you close and open R scipen is re-set to default values. It is a better option to use format(x, scientific=FALSE).\n\n100000\n\n[1] 1e+05\n\nc( 1, 100000 )\n\n[1] 1e+00 1e+05\n\nc( 1, 100000, 123456 )\n\n[1]      1 100000 123456\n\nas.integer (1000000 + 1 )\n\n[1] 1000001\n\nformat ( 1000000, scientific=FALSE)\n\n[1] \"1000000\"\n\n\n\n\n7.2.3 Discretizing a Numeric Variable\nDiscretizing is to construct a categorical version of a numeric vector with a few levels for exploration or modeling purposes, it is also called ‘binning’.\n\ncut() : the arguments are the vector to be discretized and the breakpoints; optionally we can pass labels to be applied to the new levels. The result is a factor vector.\n\ninclude.lowest= if TRUE will include the left endpoint to the binning. By default will not be included.\nright= if FALSE makes intervals include their left end and exclude the right.\nbreaks= passing an integer will produce that number of bins with equal width.\n\n\n\nvec &lt;- c(1, 5, 7, 2, 8, 9, 3, 4, 10)\nas.character (cut (vec, c(1, 4, 7, 10), include.lowest=TRUE, right=FALSE) )\n\n[1] \"[1,4)\"  \"[4,7)\"  \"[7,10]\" \"[1,4)\"  \"[7,10]\" \"[7,10]\" \"[1,4)\"  \"[4,7)\" \n[9] \"[7,10]\"\n\n\n\n# Generating random numbers\nset.seed(168)\nrand.vec &lt;- rnorm(1000)\n\n# cut() with breaks=5 of equal width\ntable (cut (rand.vec, breaks=5) )\n\n\n (-3.43,-2.15] (-2.15,-0.867] (-0.867,0.411]   (0.411,1.69]    (1.69,2.97] \n            14            184            454            301             47 \n\n# Using quantile() to create bins with equal number of observations\ntable (cut (rand.vec, quantile(rand.vec) ) )\n\n\n(-3.42,-0.682] (-0.682,-0.01]  (-0.01,0.679]   (0.679,2.97] \n           249            250            250            250 \n\n# quantile() with probs argument to create 5 bins instead of 2\ntable (cut (rand.vec, \n            quantile(rand.vec, probs=seq(0, 1, 0.2) ), \n            include.lowest = TRUE ) )\n\n\n [-3.42,-0.862] (-0.862,-0.257]  (-0.257,0.284]   (0.284,0.843]    (0.843,2.97] \n            200             200             200             200             200",
    "crumbs": [
      "Annotations",
      "Base R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R: The very basics</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/00_basics_ndata.html#character-strings",
    "href": "to_book/notebooks/00_basics_ndata.html#character-strings",
    "title": "3  R: The very basics",
    "section": "7.3 Character Strings",
    "text": "7.3 Character Strings\n\npaste() , paste0() : sticks together two character vectors, and if its necessary convert them into a character vector first. By default it will insert a space between them.\n\nsep= to choose the separation, e.g. sep=\".\" or sep=\"\" .\ncollapse= combines all the strings of the vector into one long string. It will use the separator specified by the value of this argument. e.g. collapse=\"\" or =\"\\t\" .\n\n\n\npaste(\"a\", \"b\", \"c\")\n\n[1] \"a b c\"\n\npaste0( 1 == 2, 1 + 2)\n\n[1] \"FALSE3\"\n\npaste0(\"Arg\", sep=\".\", LETTERS[3:7])\n\n[1] \"Arg.C\" \"Arg.D\" \"Arg.E\" \"Arg.F\" \"Arg.G\"\n\npaste(letters[1:5], LETTERS[5:1], collapse = \", \")\n\n[1] \"a E, b D, c C, d B, e A\"\n\n\n\n7.3.1 Column Names\nWhen a data frame is constructed from data without header names, R construct names such as V1 and V2. Usually we will want to replace these names to others more meaningful.\nFor example, we want column names for 36 months of balance data from 2021 to 2023, also 36 months of payment data for the same years.\n\nouter() : given two vectors, performs another function on each pair of elements producing a matrix.\nexpand.grid() : given vectors of values produces a data frame containing all combinations of all the values.\n\n\nhead (\n    outer (month.abb, 2021:2023, paste, sep = \".\" ), 5\n)\n\n     [,1]       [,2]       [,3]      \n[1,] \"Jan.2021\" \"Jan.2022\" \"Jan.2023\"\n[2,] \"Feb.2021\" \"Feb.2022\" \"Feb.2023\"\n[3,] \"Mar.2021\" \"Mar.2022\" \"Mar.2023\"\n[4,] \"Apr.2021\" \"Apr.2022\" \"Apr.2023\"\n[5,] \"May.2021\" \"May.2022\" \"May.2023\"\n\n# Creating the month sequence for each year\nmonseq &lt;- outer (month.abb, 2021:2023, paste, sep = \".\" )\n# Creating the column names\nnewnam &lt;- c(\"ID\", \n            paste0 (\"Bal.\", monseq), \n            paste0 (\"Pay.\", monseq) )\n\nsample(newnam, 5)\n\n[1] \"Pay.Aug.2021\" \"Bal.Feb.2023\" \"Pay.Jun.2021\" \"Bal.Apr.2021\" \"Bal.Jun.2023\"\n\n\n\n# Alternative maybe more eficient but more tedious\np1 &lt;- rep (c(\"Bal\", \"Pay\"), 12 * 3)\n# Each month 2 times for Bal and Pay, and then 3 times for the years\np2 &lt;- rep ( rep(month.abb, each = 2), 3 )\n# Each year two times, for Bal and Pay\np3 &lt;- rep (2021:2023, each = 24)\n\nnewnam &lt;- c(\"ID\", paste(p1, p2, p3, sep=\".\") )\nsample(newnam, 5)\n\n[1] \"Pay.Jun.2022\" \"Bal.Mar.2023\" \"Bal.Jun.2022\" \"Bal.Sep.2021\" \"Bal.Aug.2023\"\n\n\n\nnames.df &lt;- expand.grid (c(\"Bal\", \"Pay\"), \n                         month.abb, \n                         c(2021:2023) )\n\nnewnam &lt;- paste ( names.df[[\"Var1\"]], \n                  names.df[[\"Var2\"]], \n                  names.df[[\"Var3\"]],\n                  sep = \".\" )\n\nsample(newnam, 5)\n\n[1] \"Bal.Feb.2023\" \"Bal.Jun.2023\" \"Bal.Nov.2023\" \"Pay.Oct.2023\" \"Bal.Aug.2021\"\n\n\n\n\n7.3.2 Tabulating Dates\nTo summarize vectors of dates:\n\nset.seed(168)\nrnd.dts &lt;- as.Date (sample (0:730, size = 600), origin = \"2021-01-01\")\n\ntable (quarters (rnd.dts) )\n\n\n Q1  Q2  Q3  Q4 \n153 139 154 154 \n\n# To combine year.month is also possible with substring() instead of format\ntable ( paste0 (format (rnd.dts, \"%Y\"), \".\", quarters(rnd.dts) ) )\n\n\n2021.Q1 2021.Q2 2021.Q3 2021.Q4 2022.Q1 2022.Q2 2022.Q3 2022.Q4 2023.Q1 \n     79      68      75      78      73      71      79      76       1 \n\n\nIf we want to use the month instead of the quarter, we can extract the moth name with month() function and use paste, or it is possible to use format() directly with the year and the month name.\n\nym.order &lt;- paste0 (2021:2023, \".\", month.name )\n\nym.tbl &lt;- table (format(rnd.dts, \"%Y.%B\") )\n\nym.tbl[ym.order][1:5]\n\n\n 2021.January 2022.February          &lt;NA&gt;    2021.April      2022.May \n           26            20                          20            24 \n\n\n\n\n7.3.3 Unique Keys\nOften we need to construct a column with an unique identifier for each row. These previous methods are useful for that task. Maybe the year and month are not enough and we have to add a customer ID or something else to achieve this column with unique keys.\n\n\n7.3.4 Files and Path names\nTo create a vector with every file in the working directory and each one with it absolute path we can use paste(getwd(), list.files(), sep=\"/\") . If we use list.files(full.names=TRUE) will return only the relative path.",
    "crumbs": [
      "Annotations",
      "Base R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R: The very basics</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/00_basics_ndata.html#regular-expressions",
    "href": "to_book/notebooks/00_basics_ndata.html#regular-expressions",
    "title": "3  R: The very basics",
    "section": "7.4 Regular Expressions",
    "text": "7.4 Regular Expressions\nWe use regular expressions to find strings that match a pattern. This patterns are case-sensitive but can be ignored. R can use POSIX regular expression and Perl-style re.\nFor regular expresson in R there are three primary tools which are grep(), regexpr() and sub(), all tree with its variants.\n\ngrep() : given a pattern and a vector of strings, returns a numeric vector with the indices of the string that match the pattern.\n\nvalue= if TRUE returns the matching string themselves.\nignore.case= if TRUE will ignore whether letters are in upper or lower-case.\ninvert= if TRUE reverses the search, returning the elements that not match. It’s not available with grepl()\nfixed= if TRUE suspends the rules about patterns and simply searches for an exact text string.\nperl= if TRUE indicate to grep() to use Perl-type regular expressions.\nuseBytes= if TRUE the matching should be done byte by byte.\n\ngrepl() : returns a logical vector indicating the elements that match.\n\nSpecial Characters in POSIX regular expressions:\n\nIt is common to see .+ in regular expressions, that means a sequence of one or more characters. Also we can see .* for zero or more characters.\nIs is a mistake to add spaces to your pattern thinking ‘it will be more readable’, because the regular expression will then take the spaces literally and require that they appear.\nLooking in a character vector with dates:\n\ndt &lt;- c(\n    \"Balance due 16 Jun or earlier in 2017\",\n    \"26 Aug or any day in 3021\",\n    \"'76 Trombones' marched in a 1962 film\",\n    \"4 Apr 2018\", \"9Aug2006\",\n    \"99 Voters May Register in 20188\"\n)\n\n\n.* if we have leading text\n[0-3]?[0-9] matches a one digit number, the first digit is optional indicated by ‘?’\n.* again for additional text\n(\", or.mon, \") will be the variable with the month names separated by pipes \"|\". The parenthesis make this a single pattern. The abbreviations will match a full name.\n.* more additional text\n[1-2][0-9[{3} four digits that have to start with 1 or 2\n\n\nor.mon &lt;- paste (month.abb, collapse = \"|\")\nor.mon\n\n[1] \"Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec\"\n\nre &lt;- paste0 ( \".*[0-3]?[0-9].*(\", or.mon, \").*[1-2][0-9]{3}\")\ngrep (re, dt, value = TRUE )\n\n[1] \"Balance due 16 Jun or earlier in 2017\"\n[2] \"4 Apr 2018\"                           \n[3] \"9Aug2006\"                             \n[4] \"99 Voters May Register in 20188\"      \n\n\nThe line with ‘20188’ match because the four digits with [1-2][0-9]{3} match. Also the ‘99’ matches because [0-3] is optional and the first 9 matches in the [0-9] pattern and the second match with .* .\n\n7.4.0.1 Escape Sequences\nTo write some special characters like the dollar sign $, we have to pass a backslash to escape $ from the engine which read regular expressions. To escape a character usually we should use \\$ but in R we need to escape also the backslash because R have special characters like \\n to create a new-line. Then we will write double backslash and the simbol \\\\$. In some case, to look for the backslash itself we have to write four backslashes, two for the escape and other two for the search: \\\\\\\\. Using the argument fixed=TRUE can simplify the search of this patterns.\n\npain &lt;- c(\"c:\\\\temp\", \"/bin/u\", \"$5\", \"\\n\", \"2 backs: \\\\\\\\\")\ngrep (\"$\", pain)\n\n[1] 1 2 3 4 5\n\ngrep (\"\\\\$\", pain, value=TRUE)\n\n[1] \"$5\"\n\ngrep (\"\\\\\\\\\", pain, value=TRUE)\n\n[1] \"c:\\\\temp\"      \"2 backs: \\\\\\\\\"\n\ngrep (\"\\\\\", pain, value=TRUE, fixed=TRUE)\n\n[1] \"c:\\\\temp\"      \"2 backs: \\\\\\\\\"\n\n# Looking for any slashes:\ngrep (\"\\\\|/\", pain, value=TRUE, fixed=FALSE)\n\ncharacter(0)\n\n\n\n\n7.4.0.2 Ranges\nWorking with ranges like [0-9] meaning any number between 0 and 9 we also can negate a character, for example [^0123] means any character other than 0, 1, 2, 3, 4.\nThe predefined character classes set include [:lower:], [:upper:] for lower- and upper-case; [:alpha:] for any letters; [:alnum:] for alphanumeric; [:digit:] for digits; [:punct:] for punctuation. There are more in help(\"regex\") pages. Then, [[:digit:]] match one digit and [^[:digit:]] match any character that is not a digit.\nTo finish, a word boundary is to search a word or string with a specific composition and can be identified by \\b or \\&lt; ... \\&gt;. For example using \"\\\\&lt;\\\\d{4}\\\\&gt;\" we are specifying a word that must include exactly four digits.\n\n\n7.4.1 regexpr()\nThe function regexpr() is more precise than grep(), it will return the location of the first match within the string (number of the first character of the match). This information can be useful to extract the number itself and not only identify the string.\ndt\n# looking for a word with an integer:\n(regout &lt;- regexpr( \"\\\\&lt;\\\\d+\\\\&gt;\", dt ) )\nattr(,\"match.legth\")\nThe first ‘13’ returned by regexpr() vector indicate that the 13th character is a number referencing the 16. The number -1 indicate that string no contains an integer as a word.\nThe attributes returned also give us information. \"match.length\" is the length of the match. Then, we could extract the \"match.length\" vector and use it with substring() to extract the number in the strings. Other method is with regmatches() given the character vector and the output of regexpr():\nregmatches (dt, regout)\nTo extract all integers in the character vector we will use gregexpr() but the return is a list because some strings can contain more than one integer word. With regmatches() we can extract the numbers anyway.\ngout &lt;- gregexpr (\"\\\\&lt;\\\\d+\\\\&gt;\", dt)\nregmatches (dt, gout)\n\n# Creating a matrix with the return of regmatches\nmatrix (as.numeric (unlist (regmatches (dt, gout) ) ), ncol=2, byrow=T )\nBu default the matching look for the match that are as long as possible. If in the pattern we add a ‘?’ it will stop that. for example with \\\\d.*?\\\\d will produce “4 Apr 3” although there are more numbers (“4 Apr 3021”).\n\n\n7.4.2 Replacement with Regular Expressions\n\nsub() : replaces the first matching pattern.\ngsub() : replaces all the matching patterns.\n\nThe first argument for both a the pattern to search, the second is the characters to replace for, and the third argument is the character vector where to look for. Other option is to use backreferences, if the patters is composed by two patterns between parentheses we can use \\1 \\2 in the second argument to refer the matches:\n\nbros &lt;- c(\"Isaac Asimov\", \"Leon Tolstoi\", \"Virginia Wolf\", \"Brandon Sanderson\")\n\n# Changing the name order with backreferences:\nsub (\"([[:alpha:]]+) ([[:alpha:]]+)\", \"\\\\2, \\\\1\", bros)\n\n[1] \"Asimov, Isaac\"      \"Tolstoi, Leon\"      \"Wolf, Virginia\"    \n[4] \"Sanderson, Brandon\"\n\n\n\n\n7.4.3 Splitting with Regular Expressions\n\nstrsplit() : given a vector and a pattern splits the text producing a list with one entry for each string. Also has the fixed=TRUE argument to not use regular expressions.\n\n\nkeys &lt;- c(\"CA-2017-04-02-66J-44\", \n          \"MI-2017-07-17-41H-72\", \n          \"CA-2017-08-24-Missing-378\")\n\n(key.list &lt;- strsplit (keys, \"-\") )\n\n[[1]]\n[1] \"CA\"   \"2017\" \"04\"   \"02\"   \"66J\"  \"44\"  \n\n[[2]]\n[1] \"MI\"   \"2017\" \"07\"   \"17\"   \"41H\"  \"72\"  \n\n[[3]]\n[1] \"CA\"      \"2017\"    \"08\"      \"24\"      \"Missing\" \"378\"    \n\nmatrix ( unlist (key.list), ncol = 6, byrow = TRUE )\n\n     [,1] [,2]   [,3] [,4] [,5]      [,6] \n[1,] \"CA\" \"2017\" \"04\" \"02\" \"66J\"     \"44\" \n[2,] \"MI\" \"2017\" \"07\" \"17\" \"41H\"     \"72\" \n[3,] \"CA\" \"2017\" \"08\" \"24\" \"Missing\" \"378\"\n\n\n\n\n7.4.4 Common Data Cleaning Task Using Regular Expressions\n\n7.4.4.1 Removing Leading and Trailing Spaces\n\n\"^ *\" : any string with leading spaces\n\" *$\" : any string with trailing spaces\n\n\ngsub ( \"^ *| *$\", \"\", c(\"  Text Spaces \", \"Trailing    \", \n                        \"None\", \"     Leading\" ) )\n\n[1] \"Text Spaces\" \"Trailing\"    \"None\"        \"Leading\"    \n\n\n\n\n7.4.4.2 Format Currency to Numeric\nTaking into account $12,345.67 or 12,345.67€ we have to remove the symbol and the comma before converting into numeric.\n\n\"\\\\$\" : dollar symbol\n\"\\\\€\" : euro symbol\n\"^[^0-9.]\" : non-numeric leading character\n\"[^[:digit:].]$\" : non-numeric trailing character\n\n\nas.numeric ( gsub (\"(^[^0-9.]|,)|(,|[^[:digit:].]$)\",\n                   \"\", c(\"$12,345.67\", \"98,765.43€\") ) )\n\n[1] 12345.67 98765.43\n\n\n\n\n7.4.4.3 Removing HTML Tags\nWe are looking for Bold and other tags like that.\n\n\"&lt;.*?&gt;\" : Everything between &lt; &gt;\n\n\nhtml &lt;- c('\n    &lt;!DOCTYPE html&gt;\n&lt;html lang=\"es\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;title&gt;Ejemplo de HTML&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;Bienvenido a mi página web&lt;/h1&gt;\n    &lt;p&gt;Este es un &lt;strong&gt;ejemplo&lt;/strong&gt; de un párrafo con &lt;em&gt;texto en cursiva&lt;/em&gt; y &lt;strong&gt;texto en negrita&lt;/strong&gt;.&lt;/p&gt;\n    &lt;ul&gt;\n        &lt;li&gt;Elemento de lista 1&lt;/li&gt;\n        &lt;li&gt;Elemento de lista 2&lt;/li&gt;\n        &lt;li&gt;Elemento de lista 3&lt;/li&gt;\n    &lt;/ul&gt;\n    &lt;a href=\"https://www.ejemplo.com\"&gt;Visita nuestro sitio web&lt;/a&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n')\n\nno.html &lt;- gsub ( \"&lt;.*?&gt;\", \"\", html)\ngsub (\"\\\\n\", \"\", no.html)\n\n[1] \"                Ejemplo de HTML    Bienvenido a mi página web    Este es un ejemplo de un párrafo con texto en cursiva y texto en negrita.            Elemento de lista 1        Elemento de lista 2        Elemento de lista 3        Visita nuestro sitio web\"\n\n\n\ncat(no.html)\n\n\n    \n\n    \n    \n    Ejemplo de HTML\n\n    Bienvenido a mi página web\n    Este es un ejemplo de un párrafo con texto en cursiva y texto en negrita.\n    \n        Elemento de lista 1\n        Elemento de lista 2\n        Elemento de lista 3\n    \n    Visita nuestro sitio web\n\n\n\n\n7.4.4.4 Linux Paths to Windows or R Paths\n\n# Linux to Windows\ngsub (\"/\", \"\\\\\\\\\", \"/usr/local/bin\")\n\n[1] \"\\\\usr\\\\local\\\\bin\"\n\n# Windows to Linux\ngsub(\"\\\\\\\\\", \"/\", \"\\\\usr\\\\local\\\\bin\")\n\n[1] \"/usr/local/bin\"\n\ngsub(\"\\\\\", \"/\", \"\\\\usr\\\\local\\\\bin\", fixed = TRUE)\n\n[1] \"/usr/local/bin\"\n\n\n\n\n\n7.4.5 Recomendations\nBecause Regular Expressions are complicated be sure to document them well. For debugging there are online aids to diagnosing problems, be sure to specify regular expression type POSIX with GNI extensions or PCRE.",
    "crumbs": [
      "Annotations",
      "Base R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R: The very basics</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/00_basics_ndata.html#utf-8-and-other-non-ascii-characters",
    "href": "to_book/notebooks/00_basics_ndata.html#utf-8-and-other-non-ascii-characters",
    "title": "3  R: The very basics",
    "section": "7.5 UTF-8 and Other Non-ASCII Characters",
    "text": "7.5 UTF-8 and Other Non-ASCII Characters\nLeading with symbols or letters with accent in Latin alphabet, each have their ASCII hexadecimal value, then we can use that after \"\\x\" to use it as a pattern in grep(), gsub() or other similar function. For example, to look for Ê we can introduce \\xca as ca represent the hexadecimal value for Ê. Also, Windows and Mac use different ASCII table with different some different values.\nUnicode symbols are intend to describe all the symbols in in all the world, and are shown in R by preceding them with \\U . The word China in simplified Chinese are represented \"\\U4E2D\" and \"\\U56FD\" . The most popular is UTF-8 encoding which a character in Unicode is represented by one or more bytes. The display of UTF-8 characters can be inconsistent between machines.\nR assigns an encoding to every element in a character vector and different elements in a vector may have different encodings.\n\nEncoding() : returns the encoding of the strings in a vector\niconv() : to convert the encodings.\n\nIn R ASCII strings are unencoded; latin1 characters are encoded as latin1; non-latin unicode are encoded as UTF-8.\n\nyogi.utf &lt;- \"It's d\\Ue9j\\Ue0 vu all over again.\"\nEncoding (yogi.utf)\n\n[1] \"UTF-8\"\n\nc( regexpr (\"\\\\xe0\", yogi.utf), regexpr (\"\\ue0\", yogi.utf), regexpr (\"à\", yogi.utf) )\n\n[1] 9 9 9\n\n\n\ndata.frame (a = \"\\U4e2d\\U56fd\", stringsAsFactors = FALSE)\n\n     a\n1 中国",
    "crumbs": [
      "Annotations",
      "Base R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R: The very basics</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/00_basics_ndata.html#factors",
    "href": "to_book/notebooks/00_basics_ndata.html#factors",
    "title": "3  R: The very basics",
    "section": "7.6 Factors",
    "text": "7.6 Factors\nA Factor vector looks like text but behaves like an integer. It can be created with as.factor() function which often is a final step; also are created when you add character vector into a data frame with data.frame() or cbind() functions.\nIt is recommended to turn the character vectors into factor only when all the data cleaning is finished, before modeling.\nA Factor vector the class() is factor, the mode() is numeric and the typeof() is integer. R will set the levels alphabetically.\n\nWhen you change a value of a factor vector, if that new value not corresponds to a level, will be NA.\nWhen you remove the elements with a specific value from the factor vector, that value is still one of the levels.\n\n\ncols.fac &lt;- as.factor( c(\"red\", \"green\", \"blue\", \"red\", \"red\", \"red\", \"blue\") )\ncols.fac\n\n[1] red   green blue  red   red   red   blue \nLevels: blue green red\n\ntable(cols.fac[cols.fac != \"red\"])\n\n\n blue green   red \n    2     1     0 \n\n\n\nlevels() : to check the levels in a factor vector and also to assign levels. This not change the underlying integer value corresponding that level.\n\n\nlevels(cols.fac)\n\n[1] \"blue\"  \"green\" \"red\"  \n\nlevels(cols.fac)[2] &lt;- \"yellow\"\nlevels(cols.fac)\n\n[1] \"blue\"   \"yellow\" \"red\"   \n\n\nThe levels can be reordered calling factor() function, this is useful for months and other levels that we want to have a custom order.\n\nfactor (cols.fac, levels = c(\"red\", \"blue\", \"yellow\") )\n\n[1] red    yellow blue   red    red    red    blue  \nLevels: red blue yellow\n\n\nDo not try to change the order with levels() function. In this case will change the levels of the elements itself, will not change ‘only’ the order.\nTo convert a factor vector into character, can be used as.character() function, but a more efficient way is to use levels(fac)[fac]. Also, to convert the factor vector with levels which look like integers into number, do not use as.numeric(), first has to be converted into character. Now, if we have a gender column with F and M as levels of the factor vector, we want to convert this vector into numeric, 0 for F and 1 for M. The function as.numeric() will convert F to 1 and M to 2, to solve this we can use: as.numeric(factor(gender))-1 .\nFinally, to combine two factor vectors, it is recommended to convert them first into characters, and then combine it. Later, if it’s necessary, convert them again into factors. A exception can arise combining with rbind(), but other than this case, combining factor vector will usually end badly.\n\nind &lt;- factor ( c(33, 4, 66, 3, 6, 77) )\nsrc &lt;- 101:199\n# as.numeric will convert the vector elements into its assigned integers, not the levels.\nas.numeric(ind)\n\n[1] 4 2 5 1 3 6\n\n\n\n# Correct way, looking for that indices into 'src' vector:\nsrc[as.numeric (as.character (ind) )]\n\n[1] 133 104 166 103 106 177\n\n# This will extract correctly the numbers\n\n\ngender &lt;- as.factor(c(\"F\", \"F\", \"M\", \"F\", \"M\", \"F\", \"M\") )\ngender\n\n[1] F F M F M F M\nLevels: F M\n\nas.numeric(gender) - 1\n\n[1] 0 0 1 0 1 0 1\n\n\n\n7.6.1 Missing Values\nMissing values in factors are represented by &lt;NA&gt;. Actually, it is possible to have a level  with NA values, but it is better no avoid that, as well is clever to avoid “NA” string value. The levels() function does not mention to NA values because they do not have level.\nWe can use the function addNA() to add a level which is a NA value, but will be better to replace NA values:\n\nmis.fac &lt;- as.factor(c(\"F\", NA, \"M\", \"F\", NA, \"F\", \"M\") )\nmis.fac\n\n[1] F    &lt;NA&gt; M    F    &lt;NA&gt; F    M   \nLevels: F M\n\nmis.fac &lt;- as.character(mis.fac)\nmis.fac[is.na (mis.fac)] &lt;- \"missing\"\n( mis.fac &lt;- as.factor(mis.fac) )\n\n[1] F       missing M       F       missing F       M      \nLevels: F M missing",
    "crumbs": [
      "Annotations",
      "Base R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R: The very basics</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/00_basics_ndata.html#r-objects-and-commands",
    "href": "to_book/notebooks/00_basics_ndata.html#r-objects-and-commands",
    "title": "3  R: The very basics",
    "section": "7.7 R Objects and Commands",
    "text": "7.7 R Objects and Commands\n\nget() : accepts a character string and returns the object with that name.\nexists() : can test to see whether an object exists\nget0() : allow a value to be specified in place of the error.\nls() : returns the names of the objects.\nassing() : given a name and a value creates a new R object. The argument pos=1 at the command line has no effect, but inside a function it creates a variable in the Workspace.\n\n\nfor (i in 1:24)\n    assign (paste0 (LETTERS[i], c(month.abb,month.abb)[i] ), i, pos = 1 )\n\nlet.mon &lt;- ls (pattern = \"^[A-Z]{2}?[a-z]{2}?\")\nlet.mon[3:6]\n\n[1] \"CMar\" \"DApr\" \"EMay\" \"FJun\"\n\nsapply (let.mon, function(i) object.size(get(i)))\n\nAJan BFeb CMar DApr EMay FJun GJul HAug ISep JOct KNov LDec MJan NFeb OMar PApr \n  56   56   56   56   56   56   56   56   56   56   56   56   56   56   56   56 \nQMay RJun SJul TAug USep VOct WNov XDec \n  56   56   56   56   56   56   56   56 \n\nremove (list = grep (\"^[A-Z]{2}?[a-z]{2}?\", ls(), value = T ) )\n\n\nparse() : to create an R ‘expression’ object with text argument.\neval() : executes the expression\n\neval(parse(text = \"log.11 &lt;- log(11)\")) : will create a variable called ‘log.11’. The combination of parse() and eval() lets us construct R commands and execute them (allowing us to execute sequences of commands once we have created them with paste() and other tools).",
    "crumbs": [
      "Annotations",
      "Base R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R: The very basics</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/01_functions_scripts.html",
    "href": "to_book/notebooks/01_functions_scripts.html",
    "title": "4  Functions, scripts and performance",
    "section": "",
    "text": "5 Writing Functions and Scripts\nTo accomplish repetitive task in data cleaning we are use functions and scripts.",
    "crumbs": [
      "Annotations",
      "Base R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Functions, scripts and performance</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/01_functions_scripts.html#functions",
    "href": "to_book/notebooks/01_functions_scripts.html#functions",
    "title": "4  Functions, scripts and performance",
    "section": "5.1 Functions",
    "text": "5.1 Functions\nA function starts with the reserved word function. Continue with a a list of arguments inside parentheses. Then the body of the function enclosed in braces.\n\nsqplus &lt;- function (x, y = 2) {\n    # y default value is 2\n    # Args: x, numeric;\n    #       y, numeric;\n    if (x &lt; 0) {\n        warning (\"Negative number cannot be operated\")\n        return (NA)\n    }\n    return (sqrt (x) + y)\n}\n# Trying the new function 'sqplus'\nsqplus (x=9, y=3)\n\n[1] 6\n\n\nThe ellipsis (...) is a special type of argument which captures all of the arguments that are not matched. If the ellipsis is the first argument, the rest of arguments have to match by name, without abbreviation. The missing() function returns TRUE if an argument is missing. When it is used inside a function should be located near the beginning. For example, if ‘arg’ gets assigned in code, then missing(arg) will be FALSE.\nA local variable is one that exists inside the created function, stored in a special area of memory by R. The global variables are in the workspace. It should be avoided to use the created variable of the workspace into functions, because it can change, otherwise the built-in variables (pi or letters) it’s OK.\nThe return value is the result of the computation done by the function. A function returns whatever is inside the first call to return(); when return() is missing, the function returns whatever the last line it executes produces. The return can be hidden with invisible() command, but if you assign the output, the return value will be stored.\nTo create functions R provides with fix() function which open an editor to write that new function. If the passing function name with fix() already exists it will open the existing function for editing. When it saves, R checks to see if the new version of the function has no errors. If there is an error you can use edit() function (immediately after encountering an error) without argument to operate on the most recently edited function.\nUsing the dump() command creates a disk file with the exact text of the function. The first argument for dump() is a vector of the names of the items to be dumped, and the second argument is the file name ( dump(\"func1\", \"func1.txt\") ). Once is on disk, it can be read with source() which also executes the code in your R session ( source(\"func1.txt\") ). Also, if the function already exists, it will overwrite the function.\nWhen we want to save complicated objects, the function saveRDS() will save on disk a binary file with that objects data and attributes ( saveRDF(robj, \"robjfile\") ). The, with readRDS() function returns the object just as it was saved, it won’t replace the object, simply returns it.\nAnd finally, save() function operates on sets of objects passing the object’s names in quotation marks. The complementary function is load() and re-creates the items specified at the time the file was created, with their original names (R will over-write existing objects with that names).",
    "crumbs": [
      "Annotations",
      "Base R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Functions, scripts and performance</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/01_functions_scripts.html#scripts",
    "href": "to_book/notebooks/01_functions_scripts.html#scripts",
    "title": "4  Functions, scripts and performance",
    "section": "5.2 Scripts",
    "text": "5.2 Scripts\nA script (created under ‘File/New file/R Script’) contain R commands, comments, musings, invalid code for fixing… Usually, the script will be visible in a separate windows as interactive usage. The commands can be run by line with ‘Control+Enter’ where the mouse is, or selecting some lines.\nThe scripts store a lot of the commands to use and often are run bit by bit interactively. If a line have an error, R will continue running all the commands. Also we can run a script all at one with the source() function and only the commands prior to the error are executed.\nA shell script is useful in a production environment where a specific task needs to be performed frequently. It is intended to be run a.l at once, not as part of an interactive session. Also can be run with source(). A script run with R, meanwhile a shell script run from a command line without having to open R.\nThe program ‘Rscript’ run shell scripts which the first line specify #!Rscript . Once the run is finished, returns to the command line. Normally are useful for produce output files, graphics, informative messages. With the command Rscript --help will show options when running shell scripts.\nIn a script a function is an entire unit but a script run line by line:\n# Function that will fail\nif (i &gt; 100)\n    x &lt;- x + 200\nelse\n    x &lt;- x - 200\nThe functions have to be ‘protected’ with brace. Here the second line does not end the expression because there is still an open brace:\nif (i &gt; 100) {\n    x &lt;- x + 200\n} else {\n    x &lt;- x - 200\n}\nWhat to do? Well, functions use local variables and when it’s saved R examine the functions for errors; also it has to run all at once. Scripts can be run line by line and passed as text files; however they only create global variables, and then can over-write exiting objects.",
    "crumbs": [
      "Annotations",
      "Base R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Functions, scripts and performance</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/01_functions_scripts.html#error-handling",
    "href": "to_book/notebooks/01_functions_scripts.html#error-handling",
    "title": "4  Functions, scripts and performance",
    "section": "5.3 Error Handling",
    "text": "5.3 Error Handling\n\n5.3.1 Debugging with cat()\nInsert cat() statements into the code at strategic locations to print results. When you see an error inserting before a cat() function to check the previous results is a good idea, and also labeling the statements to know where they are placed.\nIf you have a loop variable you can use if(i %% 100 == 0) cat(\"We're on rep n \", i, \"\\n\") which will print a line every 100 iterations.\n\n\n5.3.2 Debugging with traceback(), browser() and debug()\nWhen we don’t know where the error is (can be nested or whatever) a call to traceback() show the sequence of function calls that led to the error. For advance users, the recover() function lists the set of calls and starts a browser session in the one that the user selects.\nWhen a function or script encounters a call to browser(), it pauses and produce a prompt:\nBrowse[1]&gt; \nThe [1] indicates that was called at the command line; [2] for a function called by a function; higher values for more nested function calls. There we can type the name of an object to display it, enter other function calls, create local variables… Usually it is used to display or modify the values of variables in the function. Also browser() has special commands:\n\nc : continue, resume running.\ns : step, go to the next statement (also inside other functions).\nn : next, go to the next statement.\nf : finish, to finish the loop or function.\nQ : quit the browser.\n\nInside a function with ls() we ca see only the local variables of that function, and to see the global variables is ls(pos = 1).\nIn a script we can use verbose= as an argument and as part of the script to print different messages. Also can be added an argument called browse= that specifies where calls to browser() might be made (but ensure by default is FALSE). If verbose= is TRUE or with a specific number it will print some messages while the script runs:\nprocess_file &lt;- function(fname, verbose = 0) {\n\n  if (verbose &gt;= 1) {\n    cat(\"Now operating on file\", fname, \"\\n\")\n  }\n  \n  # Rest of the code\n  # ...\n  \n  if (verbose &gt;= 2) {\n    cat(\"Detailed diagnostics: Finished processing file\", fname, \"\\n\")\n  }\n  \n}\n\n# Function with different verbose levels\nprocess_file(\"data.csv\", verbose = 1)\nprocess_file(\"data.csv\", verbose = 2)\nFinally, debug() function labels a function to be ‘browsed’ whenever it runs. This persists until undebug() remove the label; and with debugonce() label a function for just once run. Debugging produce the browser prompt, save us to include the explicit call to browse() inside a function.\n\n\n5.3.3 Issuing Error and Warning Messages\nWherever the function stop() is it will stop the function and print a message. A common function of stop() is to test whether the arguments are of the expected type. The stopifnot() function acts like stop(), but if any expressions are not all TRUE stops and produce an error message indicating the first expression which was not TRUE.\nif(!is.matrix(x)) stop(\"X must be matrix\")\n\n# Check which could be false:\nif(any(is.na (b)) || any(b &lt; 0)) { stop(\"Illegal argument b\") }\n# The double || OR would stop evaluating if the first any() is TRUE.\n# Then, if it's TRUE, we could add:\nstopifnot(b &gt; 0)\nWhen it’s not necessary to stops the function, warning() function prints out text supplied by the function (often after a call to paste() assembling some diagnostic info) and the function attempts to continue.\nIf more then 10 warning messages are generated, don’t be displayed, we can call to warnings() to access the messages.\nThe try() function lets to try a expression and if fails return an object class ‘try-error’. It is useful when we are relying on programs and files outside R’s control.\na &lt;- function (arg1) {\n    if (missing (arg1)) stop (\"Missing argument in a!\")\n    return (arg1^2)\n}\n\nb &lt;- function (input = 9, offset) {\n    a.result &lt;- a (offset)\n    return (input + a.result)\n}\n\nb()\n\ntraceback()\n\nNo traceback available \n\n\nReading from bottom to top of the traceback() output, we can see the error in a call to b(), which called a() at line #2 of the b() function, and the error took place at the second line of a().\nThis is the same function but in this version using try() function:\n\nb &lt;- function (input = 9, offset) {\n    a.check &lt;- try (a.result &lt;- a (offset) )\n    if (class (a.check)[1] == \"try-error\") {\n        warning (\"Call to a() failed; setting a.result = 3\")\n        a.result &lt;- 3\n    }\n    return (input + a.result)\n}\n\nb()\n\nError in a(offset) : could not find function \"a\"\n\n\nWarning in b(): Call to a() failed; setting a.result = 3\n\n\n[1] 12\n\n\nLots of R objects returns a vector of classes when we use class() function, that’s why we specified class(a.check)[1].",
    "crumbs": [
      "Annotations",
      "Base R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Functions, scripts and performance</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/01_functions_scripts.html#interacting-with-the-os",
    "href": "to_book/notebooks/01_functions_scripts.html#interacting-with-the-os",
    "title": "4  Functions, scripts and performance",
    "section": "5.4 Interacting with the OS",
    "text": "5.4 Interacting with the OS\nBy default R presumes that is dealing with files in the working directory. The getwd() command prints the working directory and with setwd() we can change to a new location.\n\nlist.files() : list the files in the working directory (without arguments).\n\nfull.names= if TRUE list the files in a vector of directories, returns a path name.\nrecursive= if TRUE find files in the working directory and subdirectories.\npattern= to pass a regular expression, such as “xlsx*$” for excel files.\nignore.case= if TRUE will ignore upper- and lower-case.\n\nfile.info() : gives information about a file. Can be passed an absolute path for a file outside the working directory.\nfile.copy(), file.exists(), file.remove()\ndir.exists(), dir.create()\n\nTo list the Environment Variables we can use the function Sys.getenv(), as an argument we can specified a variable to extract. R_HOME for example gives the directory where R is installed. Also, to create or update variables is the command Sys.setenv() where we can pass a variable name and a value like Sys.setenv(REPS = 12). This is a way to pass information from “outside” into R.",
    "crumbs": [
      "Annotations",
      "Base R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Functions, scripts and performance</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/01_functions_scripts.html#speeding-process",
    "href": "to_book/notebooks/01_functions_scripts.html#speeding-process",
    "title": "4  Functions, scripts and performance",
    "section": "5.5 Speeding process",
    "text": "5.5 Speeding process\nProfiling is the process of measuring how much time and memory a function uses. The Rprof() function can help to identify both the steps that use time and memory. The function writes a log file, then with summaryRprof() we can get a report.\nUsing a vectorized function on a vector will almost always be more efficient than looping over the individual entries. If it’s possible to replace a for() or while() loop with an apply() the result will be faster, more efficient. In large data sets where there are more many rows than columns, the point is to try to avoid loop over rows because looping over columns is usually much less costly.\n\n# Non-Vectorized:\nf1 &lt;- function (x, y = 2) {\n    if (x &lt; 0) {\n        warning (\"Neg(s)\")\n        return (NA)\n    }\n    return (sqrt(x) + y)\n}\nf1(5)\n\n[1] 4.236068\n\n\n\n# Vectorized\nf2 &lt;- function (x, y = 2) {\n    out &lt;- as.numeric (rep(NA, length(x)))\n    if (any(x &lt; 0)) warning (\"Neg(s)\")\n    out[x &gt;= 0] &lt;- sqrt(x[x &gt; 0]) + y\n    return (out)\n}\nf2(5)\n\n[1] 4.236068\n\n\nIn the vectorized version we create an out vector filled with NA values and then only will fill the values which are x &gt;= 0 . Also, we use as.numeric() because by default NA values are logical which shouldn’t be an issue but just in case.\nVectorization makes code harder and slower to write, maybe the time of develop a simple code, run, explain and maintain it, it is more ‘person’ efficient than write it vectorized.\nCompiling is other way to speed up the code. It is the translation of R code into ‘byte’ code. The package compile allow to compile our functions, one at a time with cmpfun() (or compile() for expressions); or using compilePKGS() to compile package by package.\n\nwhat &lt;- function (n = 100) {\n    for (i in 1:n) {}\n}\n# timing the function 'what' n=6,000,000,000\nsystem.time (what (6e9) )\n\n   user  system elapsed \n  42.28    0.02   43.36 \n\n# compiling and testing again: \nlibrary(compiler)\nwhat.cmp &lt;- cmpfun(what)\nsystem.time (what.cmp (6e9))\n\n   user  system elapsed \n  42.28    0.01   43.36 \n\n\nThe previous result with system.time() will depends on how fast the computer is. Another approach is enableJIT() (or ‘just-in-time’ compilation) which given a number to specify how the compilation should work. For example, enableJIT(3) performs as much compilation as possible. Once we make this call, functions are compiled before their first use and will remain compiled. To ‘uncompile’ a function we can edit it with fix(), also it is possible to convert it into text like what.cmp &lt;- eval(parse(text=deparse(what.cmp, control=\"useSource\"))).\nParallel Processing can speed the things up using multiple cores of our computer. The package parallel allows control over the use of multiple cores. To know how many cores it has the function detectCores() will tell us. Requires three steps to use parallel: a cluster of cores with makeCluster() once per session; any necessary items from the global workspace need to be passed to the cluster with clusterExport(); and the cluster is passed to one of the functions that knows how to use it.\n\nparSapply() : acts like sapply() but with parallel processing.\n\n\nlibrary(parallel)\ndetectCores()\n\n[1] 16\n\n\n\nclust &lt;- makeCluster(10)\nclusterExport (clust, c(\"what\", \"what.cmp\"))\nsystem.time (\n    parSapply (clust, 1:10, function (i) what (6e9/10))\n)\n\n   user  system elapsed \n   0.00    0.00    5.78 \n\nsystem.time (\n    parSapply(clust, 1:10, function (i) what.cmp (6e9/10))\n)\n\n   user  system elapsed \n   0.00    0.00    5.82 \n\n\nFunction inside parSapply() are not compiled when enableJIT() is enabled, have to been compiled explicitly (or run the function first and then export it).\nIt is a good practice to stop the cluster with stopCluster() when parallel processing is complete.\nFor the last bit of information, R can interface with code in machine level, often originally written in C or Fortran. We run code like this all the time without knowing it.",
    "crumbs": [
      "Annotations",
      "Base R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Functions, scripts and performance</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/01_functions_scripts.html#to-remember",
    "href": "to_book/notebooks/01_functions_scripts.html#to-remember",
    "title": "4  Functions, scripts and performance",
    "section": "5.6 To remember",
    "text": "5.6 To remember\nBe consistent with the code you write. Use comments to explain the code. The blank lines, spaces, indentation are useful to make de code readable.\n\nMany bugs arise from unexpected input. Check NA values with anyNA() or using na.rm=TRUE argument in some functions.\nR sometimes convert one row or column to a vector, and then when we want to use that column (matrix), often the function will be expecting a matrix not a vector.\nTake care to missing values propagating through computations.\nEnsure that files, paths and folders exists and can be accessible, file.exists() and file.info() to see if a file is writable.\nSome functions expects a single result, but using if(class(obj) == \"lm\") can produce a warning because often class() could return a vector of classes. Then class(obj)[1] will work.",
    "crumbs": [
      "Annotations",
      "Base R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Functions, scripts and performance</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/02_data_into_r.html",
    "href": "to_book/notebooks/02_data_into_r.html",
    "title": "5  Data into R",
    "section": "",
    "text": "6 Getting Data into and out of R\nWhen we gather data for analysis, we have to track it’s history meaning where was acquired, from what source and on what date; this is known as data’s provenance. A place to keep this information can be in the scripts.",
    "crumbs": [
      "Annotations",
      "Base R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data into R</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/02_data_into_r.html#tabular-data",
    "href": "to_book/notebooks/02_data_into_r.html#tabular-data",
    "title": "5  Data into R",
    "section": "6.1 Tabular Data",
    "text": "6.1 Tabular Data\nOne of the most common data we use come in the form of rectangular or tabular data, which is observations in rows and measurements (variables) in columns. Lent’s read tabular data files (ASCII text) into R data frames and to try some approaches when is not working.\n\n6.1.1 Files with Delimiters\nCommonly a observation is represented by a single row which fields are separated by a delimiter such a comma, tab, semicolon, pipe, or exclamation point; and the end of each observation is marked with a end-of-line character which can vary across platforms.\n\nread.table(), read.csv(), read.delim() : returns a data frame. Each function differs on it’s default settings. Arguments for read.table():\n\nheader= if TRUE will use the first row of the file as column headers.\nsep= the separator character. The default value is \" \".\nquote= character to be used to surround quotes. By default is set for \" and ' .\ncomment.char= character which introduce a comment line\nstringAsFactors= if FALSE will left the character columns as character and TRUE for Factor class.\ncolClasses= expect a vector with the class of each column.\nna.strings= expect a vector with the indicators of missing value in the input data.\n\nread.csv2() : for European style comma. Use semi-colon as delimiter.\nread.delim2() : for European style comma.\n\nUsually, the quote argument is set to \"\\\"\" to recognize the double quotation marks because ' can be used as apostrophe or turned off with \"\". Also the comment argument is turned off with \"\" because it is not usual found # in data files and sometimes in fields there are ‘#1’ or similar characters.\n\n\n6.1.2 Column Classes\nWe almost always pass stringAsFactor=FALSE when we are reading data, maybe except when we know the data is numeric and pre-cleaned and using colClasses.\nThe colClasses argument allows to specify the column type for each for the columns. To get an idea what are in the columns we can pass nrow= argument to read a dozen of rows with read.table() and inspect the resulting data frame. It can be specified numeric, character, logical, Date, POSIXct, and also can convert classes as well (see help(read.table)). colClasses recycle it’s elements, to start you can pass “character” and watch what is inside.\n\n\n6.1.3 Common Problems\n\nEmbedded Delimiters\nProblems can arise with simple quote mark, # characters and also a comma as part of some text. This kind of difficulties come often from spreadsheets. If the text with comma is between double quotation marks we can use quote=\"\\\"\".\n\n\nUnknown Missing Value Indicator\nBy default R expects missing values as NA. A spreadsheet from Excel can have #NULL!, #N/A, #VALUE!, this three could be included in na.strings= argument.\n\n\nEmpy or Nearly Empy Filds\nEmpty fields will be NA values in numeric fields. But in character fields are difficult to perceive.\nOne of the firsts tasks is to extract the column classes and compare that with what we see:\ntable ( sapply(df, function(x) class(x)) )\nIf we know that a column should be numeric (NumID) but it is not, we can tabulate the elements that R is unable to convert:\ntable ( df$NumID[is.na(as.numeric(df$NumID))] )\nExamining the set of missing value indicators in the data can be helpful to include that values to na.stings argument in another call to read.table().\n\n\nBlank Lines\nWhen there are blank rows by default read.table() will skip it. If wee need that the lines from two different files correspond we can pass skip.blank.lines=FALSE argument which will set NA in numeric columns and \"\" in character ones.\n\n\nRow Names\nBy default read.table() will create a row names starting from 1 and up, unless the header has one fewer filed than the rows, in which case R uses the first column as row names. Row names must be unique, if in the data set the first column of row names are not unique we can pass row.names=NULL to create an integer type row names. To specify a column to be the row names (or a vector) we can use row.names= argument.\n\n\n\n6.1.4 When read.table() go wrong\nIf we check the file ‘data/addresses.csv’ we can see is a comma delimited file with headers, then we can specified some arguments:\nread.table (\"../data/addresses.csv\", header = TRUE, sep = \",\", quote = \"\",\n            comment.char = \"\", stringsAsFactors = FALSE)\nWe can check the first line to be sure that are headers:\n\nread.table (\"../data/addresses.csv\", header = F, sep = \",\", quote = \"\",\n            comment.char = \"\", stringsAsFactors = FALSE, nrows = 1)\n\n  V1       V2      V3   V4    V5\n1 ID LastName Address City State\n\n\n\ncount.fields() : returns the fields per row.\n\n\ncount.fields (\"../data/addresses.csv\", sep = \",\", quote = \"\", comment.char = \"\")\n\n[1] 5 5 6 5 6 5\n\n\nOK, something is wrong in the third and fifth row, let’s see what it is:\n\nread.table (\"../data/addresses.csv\", header = F, sep = \",\", quote = \"\",\n            comment.char = \"\", stringsAsFactors = FALSE, \n            nrows = 1, skip = 2)\n\n  V1     V2           V3       V4       V5 V6\n1 11 Macina 401 1st Ave.  Apt 13G New York NY\n\n\nNow we could conclude that in this 3rd row there are embedded commas in V3 corresponding to Address column.\nWe can force when creating the data frame using the largest number of columns possible by passing fill=TRUE argument. The rows with the last column filled tell us that there are problems in the input data:\n\n(\nadd &lt;- read.table (\"../data/addresses.csv\", header = T, sep = \",\", quote = \"\",\n                   comment.char = \"\", stringsAsFactors = FALSE, fill = T)\n)\n\n           ID          LastName       Address     City State\n001 O'Higgins      48 Grant Rd.    Des Moines       IA      \n011    Macina      401 1st Ave.       Apt 13G New York    NY\n242    Roeder    71 Quebec Ave.   E. Thetford       VT      \n146  Stephens   1234 Smythe St.            #5  Detroit    MI\n241  Ishikawa 986 OceanView Dr. Pacific Grove       CA      \n\n\nThe IDs have become row names because there are a less field in headers. In the case the ID was duplicated we have to pass row.names=NULL argument.\nTo continue, we are going to identify the bad formatted rows, then we’re going to paste() the columns 2 and 3 from that rows which had embedded comma, and to finish changing the positions of columns 4-5 to 3-4.\n\n# Vector with logical values:\nfixc &lt;- add$State != \"\"\n# Paste column 2 and 3 from that rows\nadd[fixc, 2] &lt;- paste (add[fixc, 2], add[fixc, 3])\n# Changing the order\nadd[fixc, 3:4] &lt;- add[fixc, 4:5]\nadd\n\n           ID              LastName       Address City State\n001 O'Higgins          48 Grant Rd.    Des Moines   IA      \n011    Macina 401 1st Ave.  Apt 13G      New York   NY    NY\n242    Roeder        71 Quebec Ave.   E. Thetford   VT      \n146  Stephens   1234 Smythe St.  #5       Detroit   MI    MI\n241  Ishikawa     986 OceanView Dr. Pacific Grove   CA      \n\n\nOur last step will be to delete the last column after saving the column names. And remove the variables that we won’t need any more.\n\n# Saving column names and deleting State column\nmycols &lt;- colnames (add)\nadd$State &lt;- NULL\n# ID column will take the values of the previus rownames, then assing colnames\nadd &lt;- data.frame (ID = rownames (add), add)\ncolnames (add) &lt;- mycols\n# Replace old rownames:\nrownames (add) &lt;- NULL\n# Removing objects:\nrm (fixc, mycols)\nadd\n\n   ID  LastName               Address          City State\n1 001 O'Higgins          48 Grant Rd.    Des Moines    IA\n2 011    Macina 401 1st Ave.  Apt 13G      New York    NY\n3 242    Roeder        71 Quebec Ave.   E. Thetford    VT\n4 146  Stephens   1234 Smythe St.  #5       Detroit    MI\n5 241  Ishikawa     986 OceanView Dr. Pacific Grove    CA\n\n\n\nUsing Scan\nGiven the previous data set, let’s explore the content of that file with scan(), a general-purpose data input tool.\n\nsep=\"\\\" : to read entire lines\nwhat=character() or what=\"\" : by default scan() expect numbers, so we specify that it will encounter characters.\n\n\n(\nadd.scan &lt;- scan (\"../data/addresses.csv\", what = character(), sep = \"\\n\",\n                  quote = \"\", comment.char = \"\")\n)\n\n[1] \"ID,LastName,Address,City,State\"                 \n[2] \"001,O'Higgins,48 Grant Rd.,Des Moines,IA\"       \n[3] \"011,Macina,401 1st Ave., Apt 13G,New York,NY\"   \n[4] \"242,Roeder,71 Quebec Ave.,E. Thetford,VT\"       \n[5] \"146,Stephens,1234 Smythe St., #5,Detroit,MI\"    \n[6] \"241,Ishikawa,986 OceanView Dr.,Pacific Grove,CA\"\n\n\nWe can fix it replacing the third comma:\n\n# locating all the commas:\ncommas &lt;- gregexpr (\",\", add.scan)\n# Extracting long rows:\ncomma.5 &lt;- lengths (commas) == 5\n# Locating the third comma:\ncomma.gone &lt;- sapply (commas[comma.5], function(x) x[3])\n# Replacing comma for semi-colon:\nsubstring (add.scan[comma.5], comma.gone, comma.gone) &lt;- \";\"\nadd.scan\n\n[1] \"ID,LastName,Address,City,State\"                 \n[2] \"001,O'Higgins,48 Grant Rd.,Des Moines,IA\"       \n[3] \"011,Macina,401 1st Ave.; Apt 13G,New York,NY\"   \n[4] \"242,Roeder,71 Quebec Ave.,E. Thetford,VT\"       \n[5] \"146,Stephens,1234 Smythe St.; #5,Detroit,MI\"    \n[6] \"241,Ishikawa,986 OceanView Dr.,Pacific Grove,CA\"\n\n\nThe next step will be read.table() from text, also we can save our progress with write.table() in case we will use it again o for other users.\n\nread.table (text = add.scan, header = TRUE, sep = \",\", quote = \"\",\n            comment = \"\", stringsAsFactors = FALSE,\n            colClasses = c(ID = \"character\") )\n\n   ID  LastName               Address          City State\n1 001 O'Higgins          48 Grant Rd.    Des Moines    IA\n2 011    Macina 401 1st Ave.; Apt 13G      New York    NY\n3 242    Roeder        71 Quebec Ave.   E. Thetford    VT\n4 146  Stephens   1234 Smythe St.; #5       Detroit    MI\n5 241  Ishikawa     986 OceanView Dr. Pacific Grove    CA\n\n\n\n\n\n6.1.5 Writing Delimited Files\nTo perform this task we use write.table(), write.csv(), write.csv2() and it’s analogs. Also with write.table() can be passed a matrix. Saving a file with write.table() generally it be passed sep= argument specifying the delimiter, quote=FALSE to not save the quotes in character fields but sometimes it is necessary for numbers with leading zeros or embedded commas. Rarely we want to save the row names, then with row.names=FALSE we omit them.\n\nFixed width Files\nA fixed-width file have a specific number of character for each field, for example, ID have 4 characters, name 15, account 12… For this files we use read.fwf() that has many of the same arguments as read.table(), and the most important is widths= which expect an integer vector with the lengths of the fields.\n\n\n\n6.1.6 End-of-Line Characters:\nWindows has \\r and \\n. OS X and Linux only \\n. Depending the platform and application we should be aware, but R is flexible with read.table() and scan() functions which permits some flexibility.",
    "crumbs": [
      "Annotations",
      "Base R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data into R</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/02_data_into_r.html#non-tabular-data",
    "href": "to_book/notebooks/02_data_into_r.html#non-tabular-data",
    "title": "5  Data into R",
    "section": "6.2 Non-Tabular Data",
    "text": "6.2 Non-Tabular Data\nSometimes files are too big to fit into the main memory, then we will need some techniques to work with that files. If we need a subset of records, we can filter the data set without reading it all into memory. On the other hand, there are files not suitable for read.table() like JSON and XML, or binary data.\n\nfile() and relatives : return a connection object that stores all the information that R needs.\n\nopen= whether the file is to be read, written or append to (“r”, “w”, “a”). It can be passed with ‘+’ to read and write, and adding t or b for text or binary modes. open=\"a+b\" opens a binary file for reading and appending.\n\nclose() : to close a connection. Is a good practice to close the connection when we are finish with it.\n\nThe function readLines() opens the file, reads as many lines we want and closes the file. The argument n= mean number of lines and n=-1 for all lines. We can use readLines() over a connection to consecutively read lines from that connected file because will remain open. The analogous function is writeLines() and writeChar() which adds a null character after it’s end-of-line character but using eos=NULL argument the string is written without terminator.\n\nreadLines (\"../data/addresses.csv\", n = 1)\n\n[1] \"ID,LastName,Address,City,State\"\n\ncon.add &lt;- file (\"../data/addresses.csv\", open = \"r\")\nreadLines (con.add, n = 2)\n\n[1] \"ID,LastName,Address,City,State\"          \n[2] \"001,O'Higgins,48 Grant Rd.,Des Moines,IA\"\n\nreadLines (con.add, n = 2)\n\n[1] \"011,Macina,401 1st Ave., Apt 13G,New York,NY\"\n[2] \"242,Roeder,71 Quebec Ave.,E. Thetford,VT\"    \n\nclose (con.add)\n\nWhen a file is open R maintains a ‘pointer’ that describes the location, one for read and one for write. With seek() function we can get the current location of the file, and passing where= argument we can choose a position which is useful to jump to a prespecified position. But help tell us that seek() on windows is discouraged. Finally, flush() function can be used after a file output to ensure the write on disk operation.\n\n6.2.1 Different encodings\nFor most of the previous functions (read.table or scan) we can pass fileEncoding= argument to specify the encoding in the file we are going to work with, and encoding= argument specifies the encoding of the R object that contains the data.\nTo write on file we have to be certain what encoding have and pass to file() the encoding= argument and read using readLines() and again enconding=\"UTF-8\". In the same way to write we use file to open a connection selecting the encoding and passing useBytes=TRUE argument which prevent R to convert encoded strings back to the native encoding before writing.\n\n\n6.2.2 Null Character and Binary Data\nIn hexadecimal are represented as 00 or 0x00 by R, that’s not R’s NULL value. By default with scan() and read.table() will stop reading a NULL character and it will continue with the next field. The argument skipNul=TRUE allows to skip over NULL which is a safe choice for delimited data. For intended NULL in text we have to read the file as binary.\n\nreadBin() : to read binary data. Requires the number of bytes to read because it won’t recognize the end-of-line characters. It will return a vector with class raw.\n\nOnce read the file, we can write it back with writeBin() or convert it into data.\n\nrawToChar() : when we know that the raw data represents text, unless there are embedded nulls.\n\nWith a raw vector we can look for NULLs and replace it with space or other character that we want:\nvec[vec == 0x00] &lt;- as.raw(0x20)\nIf all the previous steps has gone well, after write the data back or converted to text we can use read.fwf() or readLines().",
    "crumbs": [
      "Annotations",
      "Base R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data into R</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/02_data_into_r.html#reading-data-from-relational-databases",
    "href": "to_book/notebooks/02_data_into_r.html#reading-data-from-relational-databases",
    "title": "5  Data into R",
    "section": "6.3 Reading data from Relational Databases",
    "text": "6.3 Reading data from Relational Databases\nIt is not necessary to load all the data into R, if the data tables are small, it doesn’t matter but with large amount of data it is better using the database as much as possible. We will need the package RODBC to be installed into R.\nTo use odbcConnect() we need to have installed ODBC drivers from Microsoft, and then open de application and configure a new DSN which will be the first argument. This DSN contain the database, language and additional configurations. It will create a handle to be used with other commands. The alternative is odbcDiverConnect() which provide us more flexibility; the first argument is a connection string with ODBC configuration.\nconnection &lt;- odbcConnect(\"SQL2022\", uid=\"rstats\", pwd=\"P@ssw0rd\")\nadv.tab &lt;- sqlTables(connection, tableType = \"TABLE\")\nadv.tab[4:8,]\nsqlColumns(connection, \"Employee\")[3:10,4:ncol(sqlColumns(connection, \"Employee\"))]\nOnce we are done with the connection (handle) it is recommended to close it with close(connection) or odbcClose(connection)\n\n6.3.1 SQL Commands\nWe are going to interact with the database using ODBC functions such as sqlQuery() passing a character string.\nemp &lt;- sqlQuery (connection, \"\n                 SELECT * FROM HumanResources.Employee\n                 \")\ns10 &lt;- sample(nrow(emp), 10)\nemp[s10,]\nemp.mal &lt;- sqlQuery (connection, \"\n                    SELECT BusinessEntityID, LoginID, VacationHours, SickLeaveHours\n                    FROM HumanResources.Employee\n                    WHERE Gender = 'M'\n                    \", stringsAsFactors = FALSE)\ns10 &lt;- sample(nrow(emp.mal), 10)\nemp.mal[s10,]\nThe sqlQuery() function supports simple arithmetic operations such as count, max or min, combine columns arithmetically, logarithms, aggregate data into groups…\n\nJoining Tables\nJoin in SQL is like merge() function in R. It is to match up two tables according to the value of a column in each one.\nemp.sales &lt;- sqlQuery (connection, \"\n                    SELECT *\n                    FROM Sales.SalesPerson\n                    LEFT JOIN HumanResources.Employee \n                    ON Sales.SalesPerson.BusinessEntityID = HumanResources.Employee.BusinessEntityID\n                    \", stringsAsFactors = FALSE)\n# s10 &lt;- sample(nrow(emp.sales), 10)\nemp.sales\n\n\nResults in Pieces\nWhen we call to sqlQuery() performs two tasks, first it sends the query and then fetch the results. If we are going to work with very large tables we can do this tasks separately.\n\nsqlFetch() : get the first batch with a given number of rows. Subsequent calls should be made to sqlFetchMore() with max= argument.\n\nAlso, calling sqlQuery() passing max= for complicated queries, we can follow it with sqlGetResults() and rows_at_time= argument with an integer between 1 and 1024 (number of rows to fetch at a time).\nsqlFetch(connection, \"Sales.Customer\", max = 10)\nsqlFetchMore(connection, max = 10)\n\n\nSQLite\nSQLite is a ‘serverless’ database which data is stored in one large file well suited to smaller applications. The RSQLite package connects R to SQLite databases. Some useful function are:\n\ndbConnect() : passing as first argument SQLite() and the second dbname= argument is the name of the file containing the data. Returns a handle for other functions.\ndbListTables() : list the tables. The first argument can be the handle.\ndbListFields() : return the fields (columns) in a table.\ndbGetQuery() : analogous to sqlQuery(), executes a query and returns the data.\ndbSendQuery() and dbFetch() : create a query and receive the data. dbSendQuery() does not returns data, it prepares the database to return data with dbFetch().",
    "crumbs": [
      "Annotations",
      "Base R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data into R</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/02_data_into_r.html#large-numbers-of-input-files",
    "href": "to_book/notebooks/02_data_into_r.html#large-numbers-of-input-files",
    "title": "5  Data into R",
    "section": "6.4 Large Numbers of Input Files",
    "text": "6.4 Large Numbers of Input Files\nAs Data Scientists will have to perform tasks in directories and files which sometimes there are thousands of them. For example, unzipping files and using its content.\n\nlist.files() : list files matching a pattern (regular expression).\nunzip() : to unzip zipped files.\nOther file. family functions such as file.copy(), file.remove(), file.rename()…\n\nThe datasets used for this example are from: https://github.com/wesm/pydata-book/tree/3rd-edition/datasets/babynames\nTo continue we are going to unzip the babynames files from 1950s to 2000s and load the comma separated data. First we need to know what are their names and the names of the files inside each file.\n\n# listing files corresponding with babynames data\nlist.files(path = \"../\" , \n           pattern = \"^babynames.*\\\\.zip$\",\n           recursive = TRUE, full.names = F)\n\n[1] \"data/babynames/babynames-1950.zip\" \"data/babynames/babynames-1960.zip\"\n[3] \"data/babynames/babynames-1970.zip\" \"data/babynames/babynames-1980.zip\"\n[5] \"data/babynames/babynames-1990.zip\" \"data/babynames/babynames-2000.zip\"\n\n\n\n# Exploring one .zip file\nunzip(\"../data/babynames/babynames-1950.zip\", list = TRUE)\n\n          Name Length                Date\n1  yob1951.txt 135349 2024-08-27 17:19:00\n2  yob1952.txt 137658 2024-08-27 17:19:00\n3  yob1953.txt 140015 2024-08-27 17:19:00\n4  yob1954.txt 141564 2024-08-27 17:19:00\n5  yob1955.txt 143552 2024-08-27 17:19:00\n6  yob1956.txt 146612 2024-08-27 17:19:00\n7  yob1957.txt 149179 2024-08-27 17:19:00\n8  yob1958.txt 148618 2024-08-27 17:19:00\n9  yob1959.txt 151965 2024-08-27 17:19:00\n10 yob1960.txt 154001 2024-08-27 17:19:00\n\n\n\n# Listing .zip files\nzipfiles &lt;- list.files(path = \"../\" , \n                       pattern = \"^babynames.*\\\\.zip$\",\n                       recursive = TRUE, full.names = T)\n\n# Unzip every file\nfor (f in 1:length(zipfiles)) {\n    unzip (zipfiles[f])\n}\n\n# Listing .txt files \ntxtfiles &lt;- list.files(path = \"../\",\n                       pattern = \"^yob.*\\\\.txt$\",\n                       recursive = TRUE, full.names = TRUE)\n# Extracting names and years\ntxtnames &lt;- sub (\"../notebooks/\", \"\", txtfiles)\nyr &lt;- substring (txtnames, 4, 7)\n\n# creating a data frame with every file using year as a column\nresult &lt;- NULL\nfor (i in 1:length (txtfiles) ) {\n    dat &lt;- data.frame (Year = yr[i], read.csv (txtfiles[i], \n                                               header = FALSE, \n                                               stringsAsFactor=FALSE))\n    result &lt;- rbind (result, dat)\n}\n\ns10 &lt;- sample (nrow(result), 10)\nresult[s10,]\n\n        Year        V1 V2 V3\n334244  1976  Melville  M  7\n282326  1973    Farris  M 13\n692888  1993     Neill  M 12\n212022  1969   Darnell  F 65\n29755   1953     Orlin  M 14\n999257  2004 Kristlynn  F  5\n779947  1997 Maryellen  F 35\n652395  1992  Lekeisha  F 17\n221993  1969     Cregg  M 11\n1169719 2009  Rosslynn  F  5\n\n\n\ncolnames(result) &lt;- c(\"Year\", \"Name\", \"Sex\", \"Count\")\n# cleaning exported files\nrem &lt;- file.remove(txtfiles)\nremove(rem)\nresult[s10,]\n\n        Year      Name Sex Count\n334244  1976  Melville   M     7\n282326  1973    Farris   M    13\n692888  1993     Neill   M    12\n212022  1969   Darnell   F    65\n29755   1953     Orlin   M    14\n999257  2004 Kristlynn   F     5\n779947  1997 Maryellen   F    35\n652395  1992  Lekeisha   F    17\n221993  1969     Cregg   M    11\n1169719 2009  Rosslynn   F     5",
    "crumbs": [
      "Annotations",
      "Base R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data into R</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/02_data_into_r.html#other-formats",
    "href": "to_book/notebooks/02_data_into_r.html#other-formats",
    "title": "5  Data into R",
    "section": "6.5 Other Formats",
    "text": "6.5 Other Formats\nThe clipboard can be used for moving text between programs. R sees the clipboard as a file named “clipboard” which can be used with read.table(\"clipboard\") or write.table(df, \"clipboard\") and then paste it into a spreadsheet. For MAC and Linux there are different approaches.\nFrom spreadsheets like Microsoft Excel with format .xls or .xlsx we can use read.xls() or read.xlsx() functions from gdata package. The spreadsheets have to be rectangular as a data frame must be.\nAlso we can acquire a web page with getURI() function of the RCurl package which will return a character vector of length 1. For HTML tables readHTMLTable() function from XML package will be our usual tool.\nWorking with XML using functions provided by XML package we can use xmlTreeParse() for read in a file and returns a tree-like object of class XMLDocument. This object acts like an R list where we can use its names to extract fields. The xmlValue() function converts the list element into text. Other approach is to use xmlParse() function and then xpathSApply() to search with RE or basic search resulting a list of object of class xmlNode.\nFor JSON can be used rjson, RJSONIO and jsonlite packages which read and write JSON objects. If we have a file containing a whole set of JSON messages, one per line we can use scan(..., sep=\"\\n\", what=\"\") and apply a conversion fromJSON() to each message.\nTo finish, to extract data using a REST API can be used RCurl and httr packages. For other statistical packages such as SAS, SPSS or Minitab the recommended package is foreign.",
    "crumbs": [
      "Annotations",
      "Base R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data into R</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/02_data_into_r.html#r-data",
    "href": "to_book/notebooks/02_data_into_r.html#r-data",
    "title": "5  Data into R",
    "section": "6.6 R Data",
    "text": "6.6 R Data\n\nsave() : the output is a file with the objects stored in.\nload() : restore all the objects stored on disk by save() (can overwrite objects)\nsaveRSD() : take an object and process a disk file with its data and attributes.\nreadRDS() : returns the object just like was saved.\nsave.image() : to save objects into the workspace, and by default creates a .RData file.\nattach() : to add an R data file to the search path (where R look for objects)",
    "crumbs": [
      "Annotations",
      "Base R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data into R</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/03_data_handling.html",
    "href": "to_book/notebooks/03_data_handling.html",
    "title": "6  Data Handling",
    "section": "",
    "text": "7 Acquiring and Reading Data\nWe can receive data from different sources and formats such as text, XML, JSON, spreadsheet… And our provider can send us the data trying to be helpful by summarized data, deleting or filling records.\nTo be sure, get as much data as possible, at as detailed a level as possible.\nOnce we have the data, we have to read it into R as a data frame. Here starts the cleaning process, identifying missing values, column classes and names. Also to create a key field as unique identifier.",
    "crumbs": [
      "Annotations",
      "Base R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Handling</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/03_data_handling.html#combining-by-row",
    "href": "to_book/notebooks/03_data_handling.html#combining-by-row",
    "title": "6  Data Handling",
    "section": "9.1 Combining by Row",
    "text": "9.1 Combining by Row\nSometimes we need to combine datasets with the same type of observations from diferent sources. They have to have:\n\nSame column names. Exactly.\nSame column classes.\nMatch categorical values.\nExamine the key column to ensure that will be unique after combine. If not, construct a new one.\nCreate a new column specifying the source (original dataset).\n\nAs an example, lets compare three datasets. First, the number of columns. And second, the names ordered.\n# Number of columns, we expect only one number:\nlength ( unique( c(ncol(MAD), ncol(BCN), ncol(BIL)) ) )\n\n# Compare pairs of datasets on column names, we expect TRUE:\nall ( sort(names(MAD)) ) == all ( sort(names(BCN)) )\nall ( sort(names(MAD)) ) == all ( sort(names(BIL)) )\nNext, we compare the column classes with all.equal() which is necessary for lists.\n# Compare pair of classes expecting TRUE\nall.equal ( sapply(MAD, class),\n            sapply(BCN, class)[names(MAD)] )\nall.equal ( sapply(MAD, class),\n            sapply(BIL, class)[names(MAD)] )\n\n# If class returns a vector length 2 (like POSIX date objects)\n# we have to use a custom function:\nall.equal ( sapply(MAD, function(x) class(x)[1])\n            sapply(BCN, function(x) class(x)[1])[names(MAD)] )\nTo verify the categorical (factor) variables we are going to convert the factor class into characters for compare their values.\n# Getting column names for columns with character or factor class\ncats &lt;- names(MAD)[ sapply (MAD, class) == \"character\" ||\n                    sapply (MAD, class) == \"factor\" ]\n\n# Extracting the unique values for these columns on each dataset\nlevs.mad &lt;- lapply ( MAD[,cats],\n                     function(x) unique (sort (as.character (x))) )\nlevs.bcn &lt;- lapply ( BCN[,cats],\n                     function(x) unique (sort (as.character (x))) )                  \nlevs.bil &lt;- lapply ( BIL[,cats],\n                     function(x) unique (sort (as.character (x))) )\n\n# Compare the extracted values expecting TRUE\nall.equal ( levs.mad, levs.bcn ) && all.equal ( levs.mad, levs.bil )\nThe last thing to do it is create a column with the source before combining:\nMAD$Source &lt;- \"MAD\"\nBCN$Source &lt;- \"BCN\"\nBIL$Source &lt;- \"BIL\"\ncities &lt;- rbind (MAD, BCN, BIL, stringsAsFactors=FALSE)",
    "crumbs": [
      "Annotations",
      "Base R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Handling</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/03_data_handling.html#combining-by-column",
    "href": "to_book/notebooks/03_data_handling.html#combining-by-column",
    "title": "6  Data Handling",
    "section": "9.2 Combining by Column",
    "text": "9.2 Combining by Column\nFor data frames with the same number of rows using data.frame() function we can combine them and the function will distinct the column names to be unique.",
    "crumbs": [
      "Annotations",
      "Base R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Handling</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/03_data_handling.html#merging-by-key",
    "href": "to_book/notebooks/03_data_handling.html#merging-by-key",
    "title": "6  Data Handling",
    "section": "9.3 Merging by key",
    "text": "9.3 Merging by key\nWhen the observations of both data frames have different orders we will use merge() function.\n\nThe return of merge() is sorted according to key column.\nWith duplicated name columns it will change to a unique name (col.x and col.y). It is better before merging to rename that columns in both data frames.",
    "crumbs": [
      "Annotations",
      "Base R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Handling</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/99_summary.html",
    "href": "to_book/notebooks/99_summary.html",
    "title": "7  Base-R Summary",
    "section": "",
    "text": "8 R-base\nTypes: logical &lt; raw &lt; numeric &lt; complex &lt; character.",
    "crumbs": [
      "Annotations",
      "Base R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Base-R Summary</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/99_summary.html#lists",
    "href": "to_book/notebooks/99_summary.html#lists",
    "title": "7  Base-R Summary",
    "section": "8.1 Lists",
    "text": "8.1 Lists\n\nlist()\nsplit() : divides a vector into pieces according to the value of another vector. Returns a list. Missing values in the second vector passed will be dropped.\nlength():returns the number of elements inside a list, and with lengths() function the length of each element.\nstr(): returns a description of every element on the list:\nunlist() : try to turn the list into a vector:",
    "crumbs": [
      "Annotations",
      "Base R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Base-R Summary</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/99_summary.html#data-frames",
    "href": "to_book/notebooks/99_summary.html#data-frames",
    "title": "7  Base-R Summary",
    "section": "8.2 Data Frames",
    "text": "8.2 Data Frames\n\ndata.frame()\nstringAsFactors=FALSE\nhead() : return the first six rows of a given dataframe. Second argument is n=6 to specify a number of rows. A negative number returns the last n rows.\ntail() : return the last six rows of a given dataframe. Second argument is n=6 to specify a number of rows.\nstr() : compact representation of the data frame with data type per column.\ndim() : returns the dimension, number of rows and columns.\nsummary() : returns a brief description of each column.\nis.na() : returns a logical matrix showing which elements are missing.\nanyNA() : returns TRUE or FALSE to the question There are missing values?\nna.omit() : omit the observations (rows) of the data frame in which one or more elements is missing. Also keeps a track, we can see the deleted observations with attr(df, \"na.action\") ; attr(df, \"class\").\nlapply() : returns a list\nsapply() : runs lapply() and tries to make the output into a vector or a matrix. But if the return have different lengths, it will need to return a list. If it try to return items with diferent types, will convert these to a common type, then its better in these case use lapply().\n\n\n8.2.1 Split, Apply, Combine\nFirst the data is split, then a function is applied to each piece, and the results recombined. The function tapply() do exactly that, but also we can use split() and sapply() or lapply().\n\nsapply (split (df2$Age, df2$Gender), mean)\ntapply(df2$Age, df2$Gender, mean)\nwith() : to perform operations on a data frame. First argument is the data frame, then the expression to be performed. Cannot be assigned to.\nwith ( CustPayment2016, JanDebtFebPurch - FebPmt )`\nwithin() : works in the same wey but unlike with(), this function can be assigned.\nCustPayment2016 &lt;- within ( CustPayment2016, FebDebt &lt;- JanDebt + FebPurch - FebPmt )\n\n\n\n8.2.2 Re-Ordering, De-Duplicating, Sampling\n\norder(): to sort a vector or data frame:\nnew.order &lt;- with ( df, order (ID, Date) )\nsample() : first argument is the number of total rows, the second argument is the size of the sample we want. By default the result is a random set of integers without replacement. The row number of sampled data frame will be the same that originals.",
    "crumbs": [
      "Annotations",
      "Base R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Base-R Summary</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/99_summary.html#date-and-time-objects",
    "href": "to_book/notebooks/99_summary.html#date-and-time-objects",
    "title": "7  Base-R Summary",
    "section": "8.3 Date and Time Objects",
    "text": "8.3 Date and Time Objects\n\nas.Date(): converts text into Date class.\nformat= - %b ; %B : name of the month - %a ; %A : name of the day of the week - %d : day of the month - %m : month in number - %y ; %Y : for the yy or YYYY year. - %F : Equivalent to %Y-%m-%d. - %x : Date. Locale-specific on output, \"%y/%m/%d\" on input.\nmonths(), weekdays(), quarters() returns the month name, weekday and quarter from passing Date object.\n\nabbreviate=TRUE argument to abbreviate the output.\n\nformat(): To extract the numeric month, day or year\n\nThe difference between dates is a period of time stored as difftime object. Functions such as mean() and range() works well but hist() or summary() fails producing the expected results.\nUsually we will convert difftime objects to numeric with as.numeric() function, for that will be a good habit to specify units = \"days\" argument (with the unit we want.\n\n8.3.1 POSIX\nPOSIXlt object is implemented as a list, meanwhile POSIXct object is like a number useful is will be stored in a column.\n\nas.POSIXct() and as.POSIXlt()\n\nThe time zone can be converted changing tzone attribute:\nattr ( ct1, tzone = \"UTC\" ). The help of Sys.timezone() containing the names of the time zones.\n\ndiff() : computes differences between adjacent elements in a vector.\ntable(): does not work on POSIXlt (list) objects.\nseq(): it can be specified by=\"day\" argument\n\n\n\n8.3.2 Combining\n\nintersection(): useful for check duplicated column names\nmake.names(): to generate unique column names\nall.equal() : compares two objects and returns TRUE if the match. Also returns a report if there are differences.\nisTRUE() : returns TRUE if its argument is a single TRUE (expected for all.equal() ) or FALSE if there are something else.\ndo.call(): takes the name of a function to be run and a list of arguments and run the function with those arguments-\ndo.call ( \"rbind\", list.of.df )\nmerge() arguments:\n\n(all.x=FALSE, all.y=FALSE) : default options. One row for each key that appears in bot x and y data frames (except when there are duplicated keys). Is an 'inner join'.\n(all.x=TRUE, all.y=FALSE) : One row for each key in x and columns of the corresponding keys that do not appear in y are filled with NA values. Is an 'left join'.\n(all.x=FALSE, all.y=TRUE) : Is the complementary one, an ‘right join’.\n(all.x=TRUE, all.y=TRUE) : This id the ‘outer join’, when the result has one row for every key in either x or y.\n\n\nIf the key match approximately (people names) the functions adist() and agrep() help find keys that match approximately.\n\n\n8.3.3 Comparing Data Frames\n\nidentical() : test for every strict equivalence. Returns TRUE when the two items are equal. Should not be applied to POSIXlt or data frames with this object.\nall.equal() : compares two objects but with more room for difference. Returns TRUE when two items are equal. By default is a match between names and attributes of two data frames. Correct way to compare: isTRUE(all.equal(df1, df2))\n\ntolerance= how different two numbers need to be to be declarated different.\n\n\n\n\n8.3.4 View and Editing Data Frames\n\nView() : shows a dear-only representation of a data frame.\nedit() : allows change to be made. Can be saved to reflect the changes\ndata.entry() : the changes are saved automatically.",
    "crumbs": [
      "Annotations",
      "Base R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Base-R Summary</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/99_summary.html#characters",
    "href": "to_book/notebooks/99_summary.html#characters",
    "title": "7  Base-R Summary",
    "section": "8.4 Characters",
    "text": "8.4 Characters\n\nlength(): extract how many elements are in it\nnchar(): we extract how many letters there are in each element.\nkeepNA=FALSE allow nchar() to count NA values returning 2 of length.\nnzchar(): returns TRUE for strings that have non-zero length and FALSE for empty strings.\ntrimws(): remove blanks at the beginning and end of each element.\nsubstring() : given a vector extract the selected text.\n\nfirst= position of the first character to extract\nlast= position of the last character up to 1 Million. Can be omitted to get the end of the string.\n\ntolower(), toupper(): perform convertion to lower or upper case.\ncasefold() have to be passed the argument upper= being TRUE or FALSE which change to upper or lower case respectively.\nchartr(): substitutions. It takes two arguments that are vectors of characters and changes each character in the first argument into the corresponding character in the second argument.\n\n\n8.4.1 Formatting Numbers\n\nformat() : is a way to format a set of numbers in a common way. E.g. lining up decimal points and commas.\n\ndigits= number of digits\nnsmall= number of digits (minimum) in the ‘small’ part (the right of the decimal point).\nbig.mark= determine the comma in the ‘big’ part (the thousand mark).\ndrop0trailing , removes trailing zeros in the small part.\nzero.print= , if TRUE, causes zeros to be printed with spaces.\n\nsprintf(): have a format string containing text and conversion strings, which describe how numbers and other variables should appear in that output. A conversion strings start with a percent sign and contain modifiers and then a conversion character.\nThe conversion character %i or %d are for integer values, %f is for double-precision numerics, and %s is for character strings. This field can be formatted with two numbers separated by a period, the first one give the minimum width (total number of characters) and the second one is the number of digits to the right of the decimal points.\n\n\n# 8 characters, 2 decimals.\nsprintf ( \"%9.2f\", 1230.456789 )\n\n[1] \"  1230.46\"\n\n# 0 leading the character to fill with 0 until 8 characters.\nsprintf ( \"%09.2f\", 1230.456789 )\n\n[1] \"001230.46\"\n\n# Like the previous one but with spaces\nsprintf ( \"% 9.2f\", 1230.456789 )\n\n[1] \"  1230.46\"\n\n# Always a simbol leading the number\nsprintf ( \"%+9.2f\", 1230.456789 )\n\n[1] \" +1230.46\"\n\n# Left Justified\nsprintf ( \"%-9.2f\", 1230.456789 )\n\n[1] \"1230.46  \"\n\n# exponential\nsprintf ( \"%9.3g\", 1230.456789 )\n\n[1] \" 1.23e+03\"\n\n\nAlso we can use sprintf() with more than one vectors:\n\ncosts &lt;- c(3, 22, 456.32, 89340.4235, 1230045605.959)\nsprintf ( \"I spent $%.0f in %s\", costs, month.name[2:6] )\n\n[1] \"I spent $3 in February\"      \"I spent $22 in March\"       \n[3] \"I spent $456 in April\"       \"I spent $89340 in May\"      \n[5] \"I spent $1230045606 in June\"\n\n\n\n\n8.4.2 Discretizing a Numeric Variable\nDiscretizing is to construct a categorical version of a numeric vector with a few levels for exploration or modeling purposes, it is also called ‘binning’.\n\ncut() : the arguments are the vector to be discretized and the breakpoints; optionally we can pass labels to be applied to the new levels. The result is a factor vector.\n\ninclude.lowest= if TRUE will include the left endpoint to the binning. By default will not be included.\nright= if FALSE makes intervals include their left end and exclude the right.\nbreaks= passing an integer will produce that number of bins with equal width.\n\n\n\n\n8.4.3 Character Strings\n\npaste() , paste0() : sticks together two character vectors, and if its necessary convert them into a character vector first. By default it will insert a space between them.\n\nsep= to choose the separation, e.g. sep=\".\" or sep=\"\" .\ncollapse= combines all the strings of the vector into one long string. It will use the separator specified by the value of this argument. e.g. collapse=\"\" or =\"\\t\" .\n\n\n# To combine year.month is also possible with substring() instead of format\ntable ( paste0 (format (rnd.dts, \"%Y\"), \".\", quarters(rnd.dts) ) )\n\nouter() : given two vectors, performs another function on each pair of elements producing a matrix.\nexpand.grid() : given vectors of values produces a data frame containing all combinations of all the values.",
    "crumbs": [
      "Annotations",
      "Base R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Base-R Summary</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/99_summary.html#regular-expressions",
    "href": "to_book/notebooks/99_summary.html#regular-expressions",
    "title": "7  Base-R Summary",
    "section": "8.5 Regular Expressions",
    "text": "8.5 Regular Expressions\nhelp(\"regex\")\nFor regular expresson in R there are three primary tools which are grep(), regexpr() and sub(), all tree with its variants.\n\ngrep() : given a pattern and a vector of strings, returns a numeric vector with the indices of the string that match the pattern.\n\nvalue= if TRUE returns the matching string themselves.\nignore.case= if TRUE will ignore whether letters are in upper or lower-case.\ninvert= if TRUE reverses the search, returning the elements that not match. It’s not available with grepl()\nfixed= if TRUE suspends the rules about patterns and simply searches for an exact text string.\nperl= if TRUE indicate to grep() to use Perl-type regular expressions.\nuseBytes= if TRUE the matching should be done byte by byte.\n\ngrepl() : returns a logical vector indicating the elements that match.\n.* if we have leading text\n[0-3]?[0-9] matches a one digit number, the first digit is optional indicated by ‘?’\n.* again for additional text\n(\", or.mon, \") will be the variable with the month names separated by pipes \"|\". The parenthesis make this a single pattern. The abbreviations will match a full name.\n.* more additional text\n[1-2][0-9[{3} four digits that have to start with 1 or 2\n\nThe function regexpr() is more precise than grep(), it will return the location of the first match within the string (number of the first character of the match). This information can be useful to extract the number itself and not only identify the string.\n\nsub() : replaces the first matching pattern.\ngsub() : replaces all the matching patterns.\nstrsplit() : given a vector and a pattern splits the text producing a list with one entry for each string. Also has the fixed=TRUE argument to not use regular expressions.\n\n\n8.5.1 Common Data Cleaning Task Using Regular Expressions\nBecause Regular Expressions are complicated be sure to document them well. For debugging there are online aids to diagnosing problems, be sure to specify regular expression type POSIX with GNI extensions or PCRE.\n\n8.5.1.1 Removing Leading and Trailing Spaces\n\n\"^ *\" : any string with leading spaces\n\" *$\" : any string with trailing spaces\n\n\ngsub ( \"^ *| *$\", \"\", c(\"  Text Spaces \", \"Trailing    \", \n                        \"None\", \"     Leading\" ) )\n\n[1] \"Text Spaces\" \"Trailing\"    \"None\"        \"Leading\"    \n\n\n\n\n8.5.1.2 Format Currency to Numeric\nTaking into account $12,345.67 or 12,345.67€ we have to remove the symbol and the comma before converting into numeric.\n\n\"\\\\$\" : dollar symbol\n\"\\\\€\" : euro symbol\n\"^[^0-9.]\" : non-numeric leading character\n\"[^[:digit:].]$\" : non-numeric trailing character\n\n\nas.numeric ( gsub (\"(^[^0-9.]|,)|(,|[^[:digit:].]$)\",\n                   \"\", c(\"$12,345.67\", \"98,765.43€\") ) )\n\n[1] 12345.67 98765.43\n\n\n\n\n\n8.5.2 UTF-8 and Other Non-ASCII Characters\n\nEncoding() : returns the encoding of the strings in a vector\niconv() : to convert the encodings.",
    "crumbs": [
      "Annotations",
      "Base R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Base-R Summary</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/11_transformations_tidyverse.html",
    "href": "to_book/notebooks/11_transformations_tidyverse.html",
    "title": "8  Transformations with Tidyverse",
    "section": "",
    "text": "9 Slicing and filtering\nThe basic form of dplyr for slicing it to use a . to consider everything from the object that precedes it. The returned object will be a tibble.\ndf %&gt;% .[1:4, c(2:5)]\n\n# A tibble: 4 × 4\n  workclass        fnlwgt education education_num\n  &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt;\n1 State-gov         77516 Bachelors            13\n2 Self-emp-not-inc  83311 Bachelors            13\n3 Private          215646 HS-grad               9\n4 Private          234721 11th                  7\ndf %&gt;% slice_min(age, prop = 0.15)\n\n# A tibble: 5,570 × 15\n     age workclass fnlwgt education education_num marital_status occupation     \n   &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;          \n 1    17 ?         304873 10th                  6 Never-married  ?              \n 2    17 Private    65368 11th                  7 Never-married  Sales          \n 3    17 Private   245918 11th                  7 Never-married  Other-service  \n 4    17 Private   191260 9th                   5 Never-married  Other-service  \n 5    17 Private   270942 5th-6th               3 Never-married  Other-service  \n 6    17 Private    89821 11th                  7 Never-married  Other-service  \n 7    17 Private   175024 11th                  7 Never-married  Handlers-clean…\n 8    17 ?         202521 11th                  7 Never-married  ?              \n 9    17 ?         258872 11th                  7 Never-married  ?              \n10    17 Private   211870 9th                   5 Never-married  Other-service  \n# ℹ 5,560 more rows\n# ℹ 8 more variables: relationship &lt;chr&gt;, race &lt;chr&gt;, sex &lt;chr&gt;,\n#   capital_gain &lt;dbl&gt;, capital_loss &lt;dbl&gt;, hours_per_week &lt;dbl&gt;,\n#   native_country &lt;chr&gt;, target &lt;chr&gt;\ndf %&gt;% slice_sample(n=10, replace = FALSE) %&gt;% .[,1:4]\n\n# A tibble: 10 × 4\n     age workclass        fnlwgt education   \n   &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;       \n 1    55 Federal-gov      189985 Some-college\n 2    32 Self-emp-not-inc 319280 Assoc-acdm  \n 3    30 Local-gov        311913 HS-grad     \n 4    22 Local-gov        289982 Bachelors   \n 5    28 Private          183175 Some-college\n 6    60 ?                386261 Bachelors   \n 7    44 Private          755858 HS-grad     \n 8    23 Private           70919 Bachelors   \n 9    33 Private           35378 Bachelors   \n10    33 Private          201988 Masters",
    "crumbs": [
      "Annotations",
      "Data Wrangling and Model",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Transformations with Tidyverse</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/11_transformations_tidyverse.html#filtering",
    "href": "to_book/notebooks/11_transformations_tidyverse.html#filtering",
    "title": "8  Transformations with Tidyverse",
    "section": "9.1 Filtering",
    "text": "9.1 Filtering\n\nfilter(): returns rows given a condition.\nselect(): to choose variable to show in the given order.\n\n\ndf %&gt;%\n  filter( age &lt; 30 ) %&gt;%\n  select( marital_status, age, education ) %&gt;%\n  slice_sample( n = 10)\n\n# A tibble: 10 × 3\n   marital_status       age education   \n   &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;       \n 1 Never-married         25 Bachelors   \n 2 Never-married         20 HS-grad     \n 3 Never-married         20 Some-college\n 4 Never-married         23 HS-grad     \n 5 Never-married         17 10th        \n 6 Never-married         20 Some-college\n 7 Never-married         24 HS-grad     \n 8 Never-married         25 HS-grad     \n 9 Married-civ-spouse    29 HS-grad     \n10 Never-married         23 Some-college\n\n\n\ndistinct(): to extract unique values.\n\n\ndf %&gt;% distinct(sex) \n\n# A tibble: 2 × 1\n  sex   \n  &lt;chr&gt; \n1 Male  \n2 Female",
    "crumbs": [
      "Annotations",
      "Data Wrangling and Model",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Transformations with Tidyverse</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/11_transformations_tidyverse.html#the-mutate-function",
    "href": "to_book/notebooks/11_transformations_tidyverse.html#the-mutate-function",
    "title": "8  Transformations with Tidyverse",
    "section": "13.1 The mutate() function",
    "text": "13.1 The mutate() function\n\nmutate(): designed to create new variables. Also can be used to modified existing variables using the same column name. Given a data frame, then the new variable name equal to the values.\nrecode(): working as a mapping to transform variables\n\nThe new variable which you can create could be whatever are in our mind, a custom function, a simple calculation, a vector…\n\ndf.no.na %&gt;%\n    mutate ( total_gain = capital_gain - capital_loss,\n             tax = ifelse( total_gain &gt;= 15000,\n                           total_gain * 0.21,\n                           total_gain * 0.1) ) %&gt;%\n    slice_sample(n=10) %&gt;% select(total_gain, tax, occupation, age) %&gt;%\n    arrange ( desc(tax) )\n\n# A tibble: 10 × 4\n   total_gain   tax occupation          age\n        &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt;\n 1          0    0  Prof-specialty       20\n 2          0    0  Prof-specialty       47\n 3          0    0  Prof-specialty       34\n 4          0    0  Exec-managerial      35\n 5          0    0  Sales                35\n 6          0    0  Tech-support         32\n 7          0    0  Machine-op-inspct    33\n 8          0    0  Sales                39\n 9          0    0  Tech-support         22\n10      -1902 -190. Exec-managerial      54\n\n\nCombining mutate() and recode() to transform values, useful for categories:\n\ndf.no.na %&gt;%\n    mutate( over_under = recode ( target, '&lt;=50'='under', '&gt;50'='over') ) %&gt;%\n    select(target, over_under) %&gt;%\n    slice_sample(n=10)\n\n# A tibble: 10 × 2\n   target over_under\n   &lt;chr&gt;  &lt;chr&gt;     \n 1 &lt;=50K  &lt;=50K     \n 2 &gt;50K   &gt;50K      \n 3 &gt;50K   &gt;50K      \n 4 &lt;=50K  &lt;=50K     \n 5 &gt;50K   &gt;50K      \n 6 &lt;=50K  &lt;=50K     \n 7 &lt;=50K  &lt;=50K     \n 8 &lt;=50K  &lt;=50K     \n 9 &lt;=50K  &lt;=50K     \n10 &lt;=50K  &lt;=50K     \n\n\nFor binning a new variable using mutate() in combination with cut(). To cut() will pass the column, a vector with the intervals, and a vector with the name of that intervals.\n\ndf.no.na %&gt;%\n    mutate( age_avg = mean(age), \n            over_under_age_avg = cut( age,\n                                      c(0, mean(age), max(age)),\n                                      c('Lower than avg', 'Above the avg'))) %&gt;%\n    select (age, age_avg, over_under_age_avg) %&gt;%\n    slice_sample(n=10)\n\n# A tibble: 10 × 3\n     age age_avg over_under_age_avg\n   &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;             \n 1    54    38.6 Above the avg     \n 2    26    38.6 Lower than avg    \n 3    27    38.6 Lower than avg    \n 4    31    38.6 Lower than avg    \n 5    21    38.6 Lower than avg    \n 6    19    38.6 Lower than avg    \n 7    23    38.6 Lower than avg    \n 8    21    38.6 Lower than avg    \n 9    46    38.6 Above the avg     \n10    24    38.6 Lower than avg",
    "crumbs": [
      "Annotations",
      "Data Wrangling and Model",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Transformations with Tidyverse</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/11_transformations_tidyverse.html#ggplot2",
    "href": "to_book/notebooks/11_transformations_tidyverse.html#ggplot2",
    "title": "8  Transformations with Tidyverse",
    "section": "16.1 ggplot2",
    "text": "16.1 ggplot2\nAs a briefly introduction to ggplot2, it is maybe considered one of the best tools for data visualization.\nThe syntax is: first, the ggplot() functions receives the data. Going forward we must link the layers using ‘+’. Then we choose the geometry corresponding the graphic type. In geometry we pass aes() with the axis inside like aes(x, y, fill), and the color, size points and other arguments.\nLet’s try a scatterplot with mtcars dataset to view the relationship between ‘miles per gallon’ and ‘horsepower’:\n\nggplot(data = mtcars) +\n    geom_point( aes( x = hp, y = mpg),\n                color = 'darkblue', size=4, alpha=0.5 ) +\n    ggtitle(\"Relationship between HP vs MPG\")",
    "crumbs": [
      "Annotations",
      "Data Wrangling and Model",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Transformations with Tidyverse</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/12_ExpDatAn.html",
    "href": "to_book/notebooks/12_ExpDatAn.html",
    "title": "9  Exploratory Data Analysis",
    "section": "",
    "text": "Libraries\nlibrary(tidyverse, quietly = TRUE)\nlibrary(skimr, quietly = TRUE)\nlibrary(statsr, quietly = TRUE)\nlibrary(GGally, quietly = TRUE)\nlibrary(corrplot, quietly = TRUE)\nDataset: https://github.com/fivethirtyeight/data/tree/master/college-majors\nThe intent of this section is to go over a practical project, following the steps: loading the dataset, understanding the data, treating missing values, exploring and visualizing, and the analysis report.",
    "crumbs": [
      "Annotations",
      "Data Wrangling and Model",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/12_ExpDatAn.html#univariate-analysis",
    "href": "to_book/notebooks/12_ExpDatAn.html#univariate-analysis",
    "title": "9  Exploratory Data Analysis",
    "section": "13.1 Univariate Analysis",
    "text": "13.1 Univariate Analysis\nIt is to look one variable at a time. Let’s graph some histograms of the numeric variables with a for loop.\n\nfor (var in colnames(select_if(df.clean, is.numeric)) ) {\n    g = ggplot(df.clean) + \n        geom_histogram( aes( unlist(df.clean[, var]) ), bins=20,\n                        fill = \"darkblue\", color = \"lightblue\", alpha = 0.5) +\n        labs( title = paste(\"Histogram of\", var),\n              x = var)\n    plot(g)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd to be able to see outliers the boxplot will be our aproach.\n\nfor ( var in colnames( select_if(df.clean, is.numeric) ) ) {\n    g = ggplot(df.clean) +\n        geom_boxplot( aes( y = unlist(df.clean[, var]) ),\n                      fill = \"lightblue\", color = \"blue\") +\n        labs(title = paste(\"Boxplot of\", var),\n             y = var)\n    plot(g)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can add a QQ plot as a visual indication of normality.\n\nfor (var in colnames( select_if(df.clean, is.numeric)) ) {\n    g = ggplot(df.clean, aes( sample = unlist( df.clean[, var])) ) +\n        stat_qq() +\n        stat_qq_line() +\n        labs( title = paste(\"QQ-plot of\", var) )\n    plot(g)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAt this point we can see that the distributions are right skewed with many outliers. To finish, we can use statistic tests to be sure if a variable distribution is normal.\n\nnorm.stats &lt;- data.frame()\nfor (var in colnames( select_if( df.clean, is.numeric) ) ) {\n    ks &lt;- ks.test( unlist( df.clean[, var] ), y = pnorm) \n    sh &lt;- shapiro.test( unlist( df.clean[, var]) )\n    vals &lt;- data.frame(\n        ks_test = sprintf(\"%6.5f\" ,c(ks$p.value)),\n        shapiro_test = sprintf(\"%6.5f\", c(sh$p.value) ),\n        row.names = var\n    )\n    norm.stats &lt;- bind_rows(norm.stats, vals)\n}\n\nWarning in ks.test.default(unlist(df.clean[, var]), y = pnorm): ties should not\nbe present for the one-sample Kolmogorov-Smirnov test\nWarning in ks.test.default(unlist(df.clean[, var]), y = pnorm): ties should not\nbe present for the one-sample Kolmogorov-Smirnov test\nWarning in ks.test.default(unlist(df.clean[, var]), y = pnorm): ties should not\nbe present for the one-sample Kolmogorov-Smirnov test\nWarning in ks.test.default(unlist(df.clean[, var]), y = pnorm): ties should not\nbe present for the one-sample Kolmogorov-Smirnov test\nWarning in ks.test.default(unlist(df.clean[, var]), y = pnorm): ties should not\nbe present for the one-sample Kolmogorov-Smirnov test\nWarning in ks.test.default(unlist(df.clean[, var]), y = pnorm): ties should not\nbe present for the one-sample Kolmogorov-Smirnov test\nWarning in ks.test.default(unlist(df.clean[, var]), y = pnorm): ties should not\nbe present for the one-sample Kolmogorov-Smirnov test\nWarning in ks.test.default(unlist(df.clean[, var]), y = pnorm): ties should not\nbe present for the one-sample Kolmogorov-Smirnov test\nWarning in ks.test.default(unlist(df.clean[, var]), y = pnorm): ties should not\nbe present for the one-sample Kolmogorov-Smirnov test\nWarning in ks.test.default(unlist(df.clean[, var]), y = pnorm): ties should not\nbe present for the one-sample Kolmogorov-Smirnov test\n\nremove(ks)\nremove(sh)\nremove(vals)\nnorm.stats\n\n                     ks_test shapiro_test\nRank                 0.00000      0.00003\nTotal                0.00000      0.00000\nMen                  0.00000      0.00000\nWomen                0.00000      0.00000\nShareWomen           0.00000      0.00433\nSample_size          0.00000      0.00000\nEmployed             0.00000      0.00000\nFull_time            0.00000      0.00000\nPart_time            0.00000      0.00000\nFull_time_year_round 0.00000      0.00000\nUnemployed           0.00000      0.00000\nUnemployment_rate    0.00000      0.01973\nMedian               0.00000      0.00000\nP25th                0.00000      0.00000\nP75th                0.00000      0.00000\nCollege_jobs         0.00000      0.00000\nNon_college_jobs     0.00000      0.00000\nLow_wage_jobs        0.00000      0.00000",
    "crumbs": [
      "Annotations",
      "Data Wrangling and Model",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/12_ExpDatAn.html#multivariate-analysis",
    "href": "to_book/notebooks/12_ExpDatAn.html#multivariate-analysis",
    "title": "9  Exploratory Data Analysis",
    "section": "13.2 Multivariate Analysis",
    "text": "13.2 Multivariate Analysis\n\nIn a project, the idea is to explore how the explanatory variables (X) affect the response variable (y).\n\nHere we want to know how the variables affect to ‘unemployment_rate’. For this task could be useful the scatter-plot to see the pattern created when two variables are compared. And the correlation coefficient which tell us how much relation they have.\n\nggpairs(): from GGally returns a matrix with scatterplot and correlations.\n\n\ndf.clean %&gt;%\n    select(Men, Women, Part_time, Unemployment_rate, \n           Non_college_jobs, Low_wage_jobs) %&gt;%\n    ggpairs() \n\n\n\n\n\n\n\n\nThere are only select some variables, it have been discarded the character ones and cherry-pick variables with some correlation. The full correlation matrix could not be seen.\n\ncorrs &lt;- round( cor(df.clean[, -c(1, 2, 3, 7)]), 3)\ncorrplot (corrs, method = \"number\", type = \"lower\", \n          tl.cex = 0.8, number.cex = 0.6)\n\n\n\n\n\n\n\n\nLooking into ‘Unemployment_rate’ the correlations are so lightly, in this case a linear regression would not be the best approach.\nWhen two explanatory variables have high correlations means both variables can explain the same variance in the target variable, creating redundancy and making it more difficult to determine the effect of each individual explanatory feature. Then, it must be eliminated before modeling.",
    "crumbs": [
      "Annotations",
      "Data Wrangling and Model",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/12_ExpDatAn.html#exploring",
    "href": "to_book/notebooks/12_ExpDatAn.html#exploring",
    "title": "9  Exploratory Data Analysis",
    "section": "13.3 Exploring",
    "text": "13.3 Exploring\nWhen we want to explore more deeply the dataset we can start doing questions and looking for answers:\n\nWhat are the top 10 majors with the lower unemployment rate?\nAnd with the higher unemployment rate?\nWhat are the majors with more specialized jobs (requiring college)?\nWhat are the best median-value-paying jobs?\nDo the majors with more share of women enrolled have a higher or lower unemployed rate?\nDo the majors with more share of women have higher rate of part time workers?\nDo the majors with a greater share of women enrolled have a similar salary median?\n\n\nWhat are the top 10 majors with the lower unemployment rate?\n\n# Selecting top 10\ntop10.hig.emp &lt;- df.clean %&gt;%\n  select(Major, Unemployment_rate) %&gt;%\n  arrange( desc(Unemployment_rate) ) %&gt;%\n  head(10)\n\n# Selecting top 10\ntop10.low.emp &lt;- df.clean %&gt;%\n  select(Major, Unemployment_rate) %&gt;%\n  arrange( Unemployment_rate ) %&gt;%\n  head(10)\ntop10.low.emp\n\n# A tibble: 10 × 2\n   Major                                      Unemployment_rate\n   &lt;fct&gt;                                                  &lt;dbl&gt;\n 1 MATHEMATICS AND COMPUTER SCIENCE                     0      \n 2 MILITARY TECHNOLOGIES                                0      \n 3 BOTANY                                               0      \n 4 SOIL SCIENCE                                         0      \n 5 EDUCATIONAL ADMINISTRATION AND SUPERVISION           0      \n 6 ENGINEERING MECHANICS PHYSICS AND SCIENCE            0.00633\n 7 COURT REPORTING                                      0.0117 \n 8 MATHEMATICS TEACHER EDUCATION                        0.0162 \n 9 PETROLEUM ENGINEERING                                0.0184 \n10 GENERAL AGRICULTURE                                  0.0196 \n\n\n\n# plot top 10 higher Unemployment rate\nggplot(top10.hig.emp) +\n  geom_col( aes(x = Unemployment_rate,\n                y = reorder( Major, +Unemployment_rate) ),\n            color = \"darkblue\", fill = \"lightblue\" ) +\n  labs( y = \"Major\" ) +\n  geom_text( aes( x = Unemployment_rate,\n                  y = Major, label = round(Unemployment_rate, 3) ),\n             size = 2, hjust = 1.2) +\n  ggtitle(\"Unemployment rate by Major\")\n\n\n\n\n\n\n\n\n\n# plot top 10 lower Unemployment rate\nggplot(top10.low.emp) +\n  geom_col( aes(x = Unemployment_rate,\n                y = reorder( Major, -Unemployment_rate) ),\n            color = \"darkblue\", fill = \"lightblue\" ) +\n  labs( y = \"Major\" ) +\n  geom_text( aes( x = Unemployment_rate,\n                  y = Major, label = round(Unemployment_rate, 3) ),\n             size = 2, hjust = 1.2) +\n  ggtitle(\"Unemployment rate by Major\")\n\n\n\n\n\n\n\n\nThe first graph of the top 10 unemployment rate looks fine but the second one not at all. Let’s try the same but using the proportion of total employees for ordering the data within unemployment rate.\n\ntop10.hig.prop &lt;- df.clean %&gt;%\n  mutate (prop = Total / sum(Total) ) %&gt;%\n  select (Major, Unemployment_rate, prop) %&gt;%\n  arrange(desc(prop), desc(Unemployment_rate)) %&gt;%\n  head(10)\n\n# plot top 10 higher Unemployment rate by proportion\nggplot(top10.hig.prop) +\n  geom_col( aes(x = Unemployment_rate,\n                y = reorder( Major, +Unemployment_rate) ),\n            color = \"darkblue\", fill = \"lightblue\" ) +\n  labs( y = \"Major\" ) +\n  geom_text( aes( x = Unemployment_rate,\n                  y = Major, label = round(Unemployment_rate, 3) ),\n             size = 2, hjust = 1.2) +\n  ggtitle(\"Unemployment rate by Major\")\n\n\n\n\n\n\n\n\n\ntop10.low.prop &lt;- df.clean %&gt;%\n  mutate (prop = Total / sum(Total) ) %&gt;%\n  select (Major, Unemployment_rate, prop) %&gt;%\n  arrange(desc(prop), Unemployment_rate) %&gt;%\n  head(10)\n\n# plot top 10 lower Unemployment rate by proportion\nggplot(top10.low.prop) +\n  geom_col( aes(x = Unemployment_rate,\n                y = reorder( Major, -Unemployment_rate) ),\n            color = \"darkblue\", fill = \"lightblue\" ) +\n  labs( y = \"Major\" ) +\n  geom_text( aes( x = Unemployment_rate,\n                  y = Major, label = round(Unemployment_rate, 3) ),\n             size = 2, hjust = 1.2) +\n  ggtitle(\"Unemployment rate by Major\")\n\n\n\n\n\n\n\n\nIt would not be necessary the second visualization, It have the same values because it is ordered by proportion. It show the most popular majors ordered by employment rate, and nursing with the best rate.\n\n\nMore specialized jobs\nJust like the last time, we are going to need new columns such as proportions of college jobs:\n\ndf.clean &lt;- df.clean %&gt;%\n  mutate ( College_jobs_pct = College_jobs / (College_jobs + Non_college_jobs),\n           Employees_pct = Total / sum(Total) )\n\n\ndf.clean %&gt;%\n  select(Major_category, Major, College_jobs_pct) %&gt;%\n  group_by(Major_category) %&gt;%\n  summarize(mean_college_jobs_pct = mean(College_jobs_pct)) %&gt;%\n  arrange(desc(mean_college_jobs_pct))\n\n# A tibble: 16 × 2\n   Major_category                      mean_college_jobs_pct\n   &lt;fct&gt;                                               &lt;dbl&gt;\n 1 Education                                           0.714\n 2 Engineering                                         0.680\n 3 Computers & Mathematics                             0.604\n 4 Biology & Life Science                              0.593\n 5 Interdisciplinary                                   0.570\n 6 Physical Sciences                                   0.532\n 7 Health                                              0.516\n 8 Psychology & Social Work                            0.508\n 9 Agriculture & Natural Resources                     0.406\n10 Humanities & Liberal Arts                           0.386\n11 Social Science                                      0.375\n12 Communications & Journalism                         0.348\n13 Arts                                                0.328\n14 Law & Public Policy                                 0.324\n15 Business                                            0.297\n16 Industrial Arts & Consumer Services               NaN    \n\n\n\n\nBest median-value-paying jobs\nFirst we have to add a variable for median salaries grouped by major category, and then use that to generate a boxplot visualization for comparison.\n\ndf.clean &lt;- df.clean %&gt;%\n  group_by(Major_category) %&gt;%\n  mutate( Salary_mdn = median(Median) )\n\nggplot(data = df.clean) +\n  geom_boxplot( aes( x = reorder(Major_category, Salary_mdn),\n                     y = Median),\n                fill = \"lightblue\", color = \"darkblue\") +\n  labs( x = \"Major_category\" ) +\n  ggtitle (\"Median by Major Category\") +\n  theme( axis.text.x = element_text( angle = 45, hjust = 1.2 ) ) +\n  expand_limits( x = c(0, NA), y = c(0, NA) ) +\n  scale_y_continuous( labels = scales::comma )\n\n\n\n\n\n\n\n\nWe can observe that Engineering, Maths, Business, Physics are the top paying jobs.\nOther approaching is to do multiple comparison (post-hoc) of the median.\n\npairwise.wilcox.test(df.clean$Median, df.clean$Major_category, p.adjust.method = \"holm\") %&gt;% suppressWarnings()\n\n\n    Pairwise comparisons using Wilcoxon rank sum test with continuity correction \n\ndata:  df.clean$Median and df.clean$Major_category \n\n                                    Agriculture & Natural Resources Arts \nArts                                1.000                           -    \nBiology & Life Science              1.000                           1.000\nBusiness                            0.801                           0.352\nCommunications & Journalism         1.000                           1.000\nComputers & Mathematics             0.295                           0.542\nEducation                           1.000                           1.000\nEngineering                         0.002                           0.007\nHealth                              1.000                           1.000\nHumanities & Liberal Arts           1.000                           1.000\nIndustrial Arts & Consumer Services 1.000                           1.000\nInterdisciplinary                   1.000                           1.000\nLaw & Public Policy                 1.000                           1.000\nPhysical Sciences                   1.000                           0.759\nPsychology & Social Work            1.000                           1.000\nSocial Science                      1.000                           1.000\n                                    Biology & Life Science Business\nArts                                -                      -       \nBiology & Life Science              -                      -       \nBusiness                            0.884                  -       \nCommunications & Journalism         1.000                  0.988   \nComputers & Mathematics             0.451                  1.000   \nEducation                           0.428                  0.008   \nEngineering                         0.000066               0.042   \nHealth                              1.000                  1.000   \nHumanities & Liberal Arts           0.367                  0.007   \nIndustrial Arts & Consumer Services 1.000                  1.000   \nInterdisciplinary                   1.000                  1.000   \nLaw & Public Policy                 1.000                  1.000   \nPhysical Sciences                   1.000                  1.000   \nPsychology & Social Work            0.884                  0.063   \nSocial Science                      1.000                  1.000   \n                                    Communications & Journalism\nArts                                -                          \nBiology & Life Science              -                          \nBusiness                            -                          \nCommunications & Journalism         -                          \nComputers & Mathematics             0.655                      \nEducation                           1.000                      \nEngineering                         0.143                      \nHealth                              1.000                      \nHumanities & Liberal Arts           1.000                      \nIndustrial Arts & Consumer Services 1.000                      \nInterdisciplinary                   1.000                      \nLaw & Public Policy                 1.000                      \nPhysical Sciences                   1.000                      \nPsychology & Social Work            1.000                      \nSocial Science                      1.000                      \n                                    Computers & Mathematics Education\nArts                                -                       -        \nBiology & Life Science              -                       -        \nBusiness                            -                       -        \nCommunications & Journalism         -                       -        \nComputers & Mathematics             -                       -        \nEducation                           0.004                   -        \nEngineering                         0.025                   0.000007 \nHealth                              1.000                   1.000    \nHumanities & Liberal Arts           0.005                   1.000    \nIndustrial Arts & Consumer Services 1.000                   1.000    \nInterdisciplinary                   1.000                   1.000    \nLaw & Public Policy                 1.000                   0.277    \nPhysical Sciences                   1.000                   0.014    \nPsychology & Social Work            0.057                   1.000    \nSocial Science                      1.000                   1.000    \n                                    Engineering Health\nArts                                -           -     \nBiology & Life Science              -           -     \nBusiness                            -           -     \nCommunications & Journalism         -           -     \nComputers & Mathematics             -           -     \nEducation                           -           -     \nEngineering                         -           -     \nHealth                              0.000499    -     \nHumanities & Liberal Arts           0.000011    0.867 \nIndustrial Arts & Consumer Services 0.023       1.000 \nInterdisciplinary                   1.000       1.000 \nLaw & Public Policy                 0.899       1.000 \nPhysical Sciences                   0.054       1.000 \nPsychology & Social Work            0.001       1.000 \nSocial Science                      0.004       1.000 \n                                    Humanities & Liberal Arts\nArts                                -                        \nBiology & Life Science              -                        \nBusiness                            -                        \nCommunications & Journalism         -                        \nComputers & Mathematics             -                        \nEducation                           -                        \nEngineering                         -                        \nHealth                              -                        \nHumanities & Liberal Arts           -                        \nIndustrial Arts & Consumer Services 1.000                    \nInterdisciplinary                   1.000                    \nLaw & Public Policy                 0.323                    \nPhysical Sciences                   0.017                    \nPsychology & Social Work            1.000                    \nSocial Science                      0.755                    \n                                    Industrial Arts & Consumer Services\nArts                                -                                  \nBiology & Life Science              -                                  \nBusiness                            -                                  \nCommunications & Journalism         -                                  \nComputers & Mathematics             -                                  \nEducation                           -                                  \nEngineering                         -                                  \nHealth                              -                                  \nHumanities & Liberal Arts           -                                  \nIndustrial Arts & Consumer Services -                                  \nInterdisciplinary                   1.000                              \nLaw & Public Policy                 1.000                              \nPhysical Sciences                   1.000                              \nPsychology & Social Work            1.000                              \nSocial Science                      1.000                              \n                                    Interdisciplinary Law & Public Policy\nArts                                -                 -                  \nBiology & Life Science              -                 -                  \nBusiness                            -                 -                  \nCommunications & Journalism         -                 -                  \nComputers & Mathematics             -                 -                  \nEducation                           -                 -                  \nEngineering                         -                 -                  \nHealth                              -                 -                  \nHumanities & Liberal Arts           -                 -                  \nIndustrial Arts & Consumer Services -                 -                  \nInterdisciplinary                   -                 -                  \nLaw & Public Policy                 1.000             -                  \nPhysical Sciences                   1.000             1.000              \nPsychology & Social Work            1.000             1.000              \nSocial Science                      1.000             1.000              \n                                    Physical Sciences Psychology & Social Work\nArts                                -                 -                       \nBiology & Life Science              -                 -                       \nBusiness                            -                 -                       \nCommunications & Journalism         -                 -                       \nComputers & Mathematics             -                 -                       \nEducation                           -                 -                       \nEngineering                         -                 -                       \nHealth                              -                 -                       \nHumanities & Liberal Arts           -                 -                       \nIndustrial Arts & Consumer Services -                 -                       \nInterdisciplinary                   -                 -                       \nLaw & Public Policy                 -                 -                       \nPhysical Sciences                   -                 -                       \nPsychology & Social Work            0.276             -                       \nSocial Science                      1.000             0.775                   \n\nP value adjustment method: holm \n\n\nOn the last output we look for p &lt; 0.05 meaning there are differences between the medians of that groups. With the boxplot we can see which are greater visually but with these comparisons we can actually know which is greater than another statistically.\n\n\nRelatioship between Majors with more share of women and unemployed rate\nThis task should be do it with correlations, and then we can plot to see its graphic representations.\n\n#cor(df.clean$ShareWomen, df.clean$Unemployment_rate, method = \"pearson\")\n\ncor(df.clean$ShareWomen, df.clean$Unemployment_rate,\n    method = \"spearman\")\n\n[1] 0.0663\n\n\nThe spearman correlation is non-parametric and it is suitable for when the relationship of two variables are non-linear, or do not follow the norm distribution.\n\nggplot( data = df.clean ) +\n  geom_point( aes( x = ShareWomen, y = Unemployment_rate ),\n              color = \"darkblue\") +\n  ggtitle (\"Share of women vs. Unemployment rate\") +\n  labs ( subtitle = \"There is no linear relationship, the graphic is spread\", color = \"darkgray\", size = 8) +\n  theme( plot.subtitle = element_text( color = \"darkgray\", \n                                               size = 10) )\n\n\n\n\n\n\n\n\n\n\nDo the majors with more share of women have higher rate of part time workers?\nJust like the last question, we can see the relationship of this two variables with a correlation.\n\ncor(df.clean$ShareWomen, df.clean$Part_time, method = \"spearman\")\n\n[1] 0.3348\n\n\nIt can be seen a medium-slight positive correlation between both Share of woman enrolled and workers with part time contract. This is how looks like the correlation:\n\nggplot(data = df.clean) +\n    geom_point( aes( x = ShareWomen, y = Part_time),\n                color = \"darkblue\") +\n    labs( title = \"Share of Women vs. Part-Time jobs\",\n          subtitle = \"The relationship between the variables is slightly weak [0.33],\\n siggesting that when the share of women is higher also there are more part-jobs\") +\n    theme( plot.subtitle = element_text( color = \"darkgray\", size = 8) )\n\n\n\n\n\n\n\n\n\n\nMajors with a greater share of women and salary median\n\ncor(df.clean$ShareWomen, df.clean$Median)\n\n[1] -0.6187\n\ncor(df.clean$ShareWomen, df.clean$Low_wage_jobs)\n\n[1] 0.1878\n\n\nThe first correlation show when the greater is the share of women enrolled, the less are the Median salary; and in the second correlation, almost without strength if the share of women is high there are a slight relationship (almost none) with more low wage jobs.\n\nggplot(data = df.clean) +\n    geom_point( aes( x = ShareWomen, y = Low_wage_jobs, \n                     color = \"Low Wage Jobs\"),\n                alpha = 1, shape = \"o\", size = 2) +\n    geom_point( aes( x = ShareWomen, y = Median, \n                     color = \"Median Salary\"), alpha = 0.5) +\n    labs(title = \"Share of women vs. low wage, and vs. Median Salary\",\n         y = \"\", color = \"Share of Women vs.:\") +\n    scale_color_manual(values = c(\"Low Wage Jobs\" = \"darkblue\", \n                                  \"Median Salary\" = \"darkred\") ) +\n    theme(legend.title = element_text())\n\n\n\n\n\n\n\n\nThe next step could be to test if the median of lower share of women is greater or not than higher share of women. To verify this we can run hypothesis tests. First we are going to add a column with two categories, one for higher share and other for lower share of women. Then we have to be sure which test we can use, for that we have to check the normality and if its fine, the homoscedasticity.\n\ndf.clean &lt;- df.clean %&gt;% \n    mutate( ShareWomen_cat = ifelse(ShareWomen &gt; 0.5, \"higher\", \"lower\") )\n\ndf.higher.w &lt;- df.clean %&gt;%\n    filter( ShareWomen_cat == \"higher\") %&gt;%\n    select( Major, Major_category, ShareWomen, \n            ShareWomen_cat, Low_wage_jobs, Median)\n\ndf.lower.w &lt;- df.clean %&gt;%\n    filter( ShareWomen_cat == \"lower\") %&gt;%\n    select( Major, Major_category, ShareWomen, \n            ShareWomen_cat, Low_wage_jobs, Median)\n\n\nks.test( df.higher.w$Low_wage_jobs, \"pnorm\") %&gt;% suppressWarnings()\n\n\n    Exact one-sample Kolmogorov-Smirnov test\n\ndata:  df.higher.w$Low_wage_jobs\nD = 0.99, p-value = 0.000000000000001\nalternative hypothesis: two-sided\n\nks.test( df.lower.w$Low_wage_jobs, \"pnorm\" ) %&gt;% suppressWarnings()\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  df.lower.w$Low_wage_jobs\nD = 0.95, p-value &lt;0.0000000000000002\nalternative hypothesis: two-sided\n\nks.test( df.higher.w$Median, \"pnorm\" ) %&gt;% suppressWarnings()\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  df.higher.w$Median\nD = 1, p-value &lt;0.0000000000000002\nalternative hypothesis: two-sided\n\nks.test( df.lower.w$Median, \"pnorm\" ) %&gt;% suppressWarnings()\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  df.lower.w$Median\nD = 1, p-value &lt;0.0000000000000002\nalternative hypothesis: two-sided\n\n\nIn the las output we show that every test was lower than 0.05 thus there are no normality on these groups. Let’s check homoscedasticity:\n\nwith(df.clean, car::leveneTest(Median, ShareWomen_cat) )  %&gt;% suppressWarnings()\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value    Pr(&gt;F)    \ngroup   1    25.4 0.0000012 ***\n      170                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nwith(df.clean, car::leveneTest(Low_wage_jobs, ShareWomen_cat) )  %&gt;% suppressWarnings()\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value Pr(&gt;F)\ngroup   1    1.86   0.17\n      170               \n\n\nThe first result for Median on ShareWomen_cat levels have p &lt; 0.05 then we reject the Null Hypothesis; nevertheless, the second result for Low_wage_jobs on ShareWomen_cat levels the p is 0.17, then we maintain the Null Hypothesis of Homogeneity of Variances.\nNow we know that for compare Median we will need a robust test, but with Low_wage_jobs we might use t-test because the sample is greater than 50 and that will not affect so much the results.\n\nwilcox.test(df.higher.w$Median, df.lower.w$Median)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  df.higher.w$Median and df.lower.w$Median\nW = 1142, p-value = 0.00000000000001\nalternative hypothesis: true location shift is not equal to 0\n\ncat(\"Median of Higher Share Woman: \", median(df.higher.w$Median),\n    \"\\nMedian of Lower Share Woman: \", median(df.lower.w$Median), \"\\n\" )\n\nMedian of Higher Share Woman:  34000 \nMedian of Lower Share Woman:  45000 \n\nt.test(df.higher.w$Low_wage_jobs, df.lower.w$Low_wage_jobs)\n\n\n    Welch Two Sample t-test\n\ndata:  df.higher.w$Low_wage_jobs and df.lower.w$Low_wage_jobs\nt = 1.8, df = 170, p-value = 0.07\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -162.2 3917.3\nsample estimates:\nmean of x mean of y \n     4708      2831 \n\n\nFor Median there are differences between the median of the groups, but for Low Wage seem like there are not differences between the mean of the groups with this sample.\nLet’s try another approach. Now using rep_sample_n() we are going to take 100 samples of 1000 observations allowing repetitions, and then taking the mean.\n\ndf.higher.samp &lt;- df.higher.w %&gt;%\n    rep_sample_n( size = 1000, reps = 200, replace = TRUE ) %&gt;%\n    summarize( mu_Median = mean(Median), \n               mu_Low_wage = mean(Low_wage_jobs) )\n\ndf.lower.samp &lt;- df.lower.w %&gt;%\n    rep_sample_n( size = 1000, reps = 200, replace = TRUE ) %&gt;%\n    summarize( mu_Median = mean(Median), \n               mu_Low_wage = mean(Low_wage_jobs) )\n\n\nshapiro.test(df.lower.samp$mu_Median)\n\n\n    Shapiro-Wilk normality test\n\ndata:  df.lower.samp$mu_Median\nW = 0.99, p-value = 0.09\n\nshapiro.test(df.higher.samp$mu_Median)\n\n\n    Shapiro-Wilk normality test\n\ndata:  df.higher.samp$mu_Median\nW = 0.99, p-value = 0.2\n\nshapiro.test(df.lower.samp$mu_Low_wage)\n\n\n    Shapiro-Wilk normality test\n\ndata:  df.lower.samp$mu_Low_wage\nW = 0.99, p-value = 0.4\n\nshapiro.test(df.higher.samp$mu_Low_wage)\n\n\n    Shapiro-Wilk normality test\n\ndata:  df.higher.samp$mu_Low_wage\nW = 0.99, p-value = 0.4\n\n\n\nt.test(df.higher.samp$mu_Median, df.lower.samp$mu_Median)\n\n\n    Welch Two Sample t-test\n\ndata:  df.higher.samp$mu_Median and df.lower.samp$mu_Median\nt = -372, df = 276, p-value &lt;0.0000000000000002\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -12461 -12330\nsample estimates:\nmean of x mean of y \n    34605     47000 \n\nt.test(df.higher.samp$mu_Low_wage, df.lower.samp$mu_Low_wage)\n\n\n    Welch Two Sample t-test\n\ndata:  df.higher.samp$mu_Low_wage and df.lower.samp$mu_Low_wage\nt = 91, df = 358, p-value &lt;0.0000000000000002\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 1853 1935\nsample estimates:\nmean of x mean of y \n     4719      2826 \n\n\nBoth p-value are close to zero, thus we can reject the null hypothesis of t-test. Then we can say the groups mean are different with this sample.\n\nWe can infer with 95% confidence that the majors with 50% or more of those enrolled being women have, on average, more low-wage jobs and less median salary.\n\n\ndf.salary.w &lt;- df.clean %&gt;%\n  group_by(ShareWomen_cat) %&gt;%\n  summarize(Salary_mean = mean(Median) )\n\n\nggplot( data = df.clean ) +\n  geom_boxplot( aes( x = ShareWomen_cat, y = Median), \n                fill = \"lightblue\", color = \"darkblue\" ) +\n  labs( title = \"Average Salary When \n        ShareWomen is lower/higher than 50%\",\n        subtitle = \"The average salary for majors with more women enrolled \\n\n        is lower than the majors with less women, reinforcing the perception \\n \n        that women are getting lower salaries.\",\n        x = \"50% Share of Women\",\n        y = \"Mean Salary\" ) +\n  geom_text( data = df.salary.w, aes( x = ShareWomen_cat,\n                                      y = Salary_mean,\n                                      label = round(Salary_mean)),\n             size = 2.2, vjust = 1, color = \"black\" ) +\n  theme(plot.subtitle = element_text( color = \"darkgray\", size = 9 ) )\n\n\n\n\n\n\n\n\nTo close this Exploratory Data Analysis we just need to compose the Analysis report.",
    "crumbs": [
      "Annotations",
      "Data Wrangling and Model",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/12_ExpDatAn.html#report",
    "href": "to_book/notebooks/12_ExpDatAn.html#report",
    "title": "9  Exploratory Data Analysis",
    "section": "Report",
    "text": "Report\nEDA of a complete dataset, the missing values are less than 1% which was excluded to preserve the original data only without adding calculated numbers.\nThe descriptive statistics show high variances due to the presence of outliers. The women in college represents the 52%. The unemployment rate is around 6% presenting high variance.\nAre jobs more specialized which require a degree such as Education, Maths, Engineering. The engineering is the top-paying category followed by Computer and Maths, Physical Science, and Business. Nevertheless, there are no statistical differences between categories on salary median except comparing Engineering which is statistically higher.\nComparing gender it can be shown that majors with more women enrolled have more low wage jobs and also majors with more share of women seems to have lower salaries. Moreover, the unemployment rate are lower in majors with more share of women.",
    "crumbs": [
      "Annotations",
      "Data Wrangling and Model",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/12_ExpDatAn.html#next-steps",
    "href": "to_book/notebooks/12_ExpDatAn.html#next-steps",
    "title": "9  Exploratory Data Analysis",
    "section": "Next steps",
    "text": "Next steps\nIf further analysis are required which needs modeling, for linear regression attention has to be paid to multicollinear variables which can affect its reliability.\n\nThe last text was an example of Analysis report, it could be filled with more content if its necessary. But this is the idea.",
    "crumbs": [
      "Annotations",
      "Data Wrangling and Model",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/13_enchanced_visualizations.html",
    "href": "to_book/notebooks/13_enchanced_visualizations.html",
    "title": "10  Enchanced Visualizations",
    "section": "",
    "text": "11 Introduction to ggplot2\nlibrary(tidyverse)\nlibrary(datasets)\nlibrary(patchwork)\ndata(\"mtcars\")",
    "crumbs": [
      "Annotations",
      "Data Wrangling and Model",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Enchanced Visualizations</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/13_enchanced_visualizations.html#the-grammar-of-graphics",
    "href": "to_book/notebooks/13_enchanced_visualizations.html#the-grammar-of-graphics",
    "title": "10  Enchanced Visualizations",
    "section": "11.1 The grammar of graphics",
    "text": "11.1 The grammar of graphics\n\nData: the dataset to use\nGeometry: the type of graphic to be plotted.\nAesthetics: Are what are to be seen in a plot. x axis, y axis, color, fill, shape, opacity, width and line type.\nStatistics: graphics where there is a need to indicate the number of bins, regression type…\nCoordinates: Cartesian or Polar coordinates. The x and y Cartesian coordinates by default will be used.\nFacets: to plot multiple graphics for different groups.\nThemes: are the preset configurations such as background color, grid, fonts…",
    "crumbs": [
      "Annotations",
      "Data Wrangling and Model",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Enchanced Visualizations</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/13_enchanced_visualizations.html#basic-syntax-of-ggplot2",
    "href": "to_book/notebooks/13_enchanced_visualizations.html#basic-syntax-of-ggplot2",
    "title": "10  Enchanced Visualizations",
    "section": "11.2 Basic syntax of ggplot2",
    "text": "11.2 Basic syntax of ggplot2\nThe basic syntax follows the order:\n\nWhat is the dataset to be used?\nWhat kind of graphic will be plotted?\nWhat goes on the X axis and Y axis?\nWhat is the graphic title?\n\nggplot(data = df) +     # data set to use\n    geom_point(         # type of grapth to plot \n    mapping = aes( x = X, y = Y) ) +        # Aesthetics\n    ggtitle(\"Title\")    # Title",
    "crumbs": [
      "Annotations",
      "Data Wrangling and Model",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Enchanced Visualizations</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/13_enchanced_visualizations.html#plot-types",
    "href": "to_book/notebooks/13_enchanced_visualizations.html#plot-types",
    "title": "10  Enchanced Visualizations",
    "section": "11.3 Plot types",
    "text": "11.3 Plot types\n\n11.3.1 Histograms\n\ngeom_histogram()\n\n\nggplot(mtcars) +\n    geom_histogram(\n        mapping = aes( x = mpg ), \n        bins = 20,\n        color = \"lightblue\",\n        fill = \"darkblue\",\n        alpha = 0.6\n    ) +\n    ggtitle( \"Whistogram of Miles per Gallon\" )\n\n\n\n\n\n\n\n\n\n\n11.3.2 Boxplot\n\ngeom_boxplot()\n\nFor univariate plots it required only one axis. For multivariate the categories could be on X.\n\nggplot( data = mtcars ) +\n    geom_boxplot( aes( y = mpg ), fill = \"lightblue\", color = \"darkblue\" ) +\n    ggtitle( \"Boxplot of miles per gallon\")\n\n\n\n\n\n\n\n\n\nggplot( mtcars ) +\n    geom_boxplot( aes( x = factor(am),\n                       y = mpg ), fill = \"lightblue\" ) +\n    labs( x = \"Transmission\",\n          title = \"Comparison betweeen automatic and manual transmission on MPG\",\n          subtitle = \"The median of manual cars (1) is higher\" )\n\n\n\n\n\n\n\n\n\n\n11.3.3 Scatterplot\n\ngeom_point()\n\nUseful for see relationships between variables and correlations.\n\nggplot( data = mtcars ) +\n    geom_point( aes( x = wt, y = mpg ),\n                color = \"darkblue\",\n                size = 4,\n                shape = 15, # squares\n                alpha = 0.7 ) +\n    labs ( x = \"Weight of the cars\",\n           y = \"Milles per Gallon\",\n           title = \"How the weight affects MPG in cars?\",\n           subtitle = \"As the weight increases, the car will maje less MPG\")\n\n\n\n\n\n\n\n\n\n\n11.3.4 Barplots\n\ngeom_bar()\nstat_summary(): alternative to geom_bar(), given the aesthetics, function and geometry to use.\ngeom_col(): requires x and y axes. It will return the bars with the summed y-axis by category.\n\nUseful for categorical variables to show counts or values.\n\nggplot( mtcars ) +\n    geom_bar( aes( x = factor(am),\n                   fill = factor(am) ) ) +      \n                 # Fill in 'aes' using the same factor means color per category\n    labs( x = \"Automatic(0) | Manual(1)\" ) +\n    ggtitle( \"Count of observations by transmission type\" )\n\n\n\n\n\n\n\n\n\nggplot( mtcars ) +\n    geom_bar( aes( x = factor(am), \n                   y = mpg,\n                   fill = factor(am) ),\n              stat = \"summary\", fun = \"mean\" ) # To use the mean on 'y' axis\n\n\n\n\n\n\n\n\n\nggplot( mtcars ) +\n    stat_summary( aes( x = factor(am),\n                       y = mpg,\n                       fill = factor(am) ), \n                  fun = \"mean\",\n                  geom = \"bar\" )\n\n\n\n\n\n\n\n    # Alternative to the previous graphic\n\n\nggplot( mtcars ) +\n    geom_col( aes( x = factor(am),\n                   y = mpg,\n                   fill = factor(am) ) ) +\n    labs( x = \"Automatic(0) | Manual(1)\" )\n\n\n\n\n\n\n\n\n\nggplot( mtcars ) +\n    geom_bar( aes( x = factor(cyl), fill = factor(vs) ),\n              position = \"stack\" )\n\n\n\n\n\n\n\nggplot( mtcars ) +\n    geom_bar( aes( x = factor(cyl), fill = factor(vs) ),\n              position = \"dodge\" )\n\n\n\n\n\n\n\nggplot( mtcars ) +\n    geom_bar( aes( x = factor(cyl), fill = factor(vs) ),\n              position = \"fill\" )\n\n\n\n\n\n\n\n\n\n\n11.3.5 Line plots\n\ngeom_line()\n\nUseful to show progression over time.\n\ndf.line &lt;- data.frame(\n    month = seq(1, 12, 1),\n    sales = rlnorm(12),\n    sales2 = rgamma(12, 1)\n)\ndf.line[1:5,]\n\n  month      sales    sales2\n1     1 0.40212478 2.5404338\n2     2 0.68381982 3.0001922\n3     3 2.54140127 1.1424754\n4     4 0.47056744 0.1769046\n5     5 0.09183938 1.5964452\n\n\n\nggplot( df.line ) +\n    geom_line( aes( x = month,\n                    y = sales,\n                    group = 1),\n               linewidth = 1,\n               color = \"darkblue\" ) +\n    ggtitle( \"Car sales throughout the months\")\n\n\n\n\n\n\n\n\n\nggplot( df.line ) +\n    geom_line( aes( x = month,\n                    y = sales,\n                    group = 1,\n                    color = \"Sales year 1\"), # the label for this line\n               linewidth = 1,\n               linetype = 2) +\n    geom_line( aes( x = month,\n                    y = sales2,\n                    group = 1,\n                    color = \"Sales year 2\"),\n               linewidth = 1,\n               linetype = 1) +\n    ggtitle( \"Car sales throughout the monts\" ) +\n    labs( subtitle = \"Two year comparison\",\n          color = \"Sales Year\") # The title for the legend\n\n\n\n\n\n\n\n\n\n\n11.3.6 Smooth geometry\n\ngeom_smooth()\nmethod= by default polynomial function (loess). There are also linear regression (lm) generalized additive models (gam), robust linear models (rlm) and generalized linear models (glm).\n\nIt calculates a line that helps to see a trend in the points, using linear regression, polynomial regression or a general linear model.\n\nggplot( mtcars ) +\n  geom_point( aes( x = hp,\n                   y = mpg,\n                   color = factor(vs) ) ) +\n  geom_smooth( aes( x = hp,\n                    y = mpg ),\n               method = 'loess' )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWe can add aes() to ggplot() to be more efficient.\n\nggplot( mtcars, aes( x = hp, y = mpg ) ) +\n  geom_point( aes( color = factor(vs) ) ) +\n  geom_smooth( method = 'loess' )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n11.3.7 Themes\n\ntheme_bw(), theme_classic(), theme_light(), theme_dark(), theme_gray(), theme_linedraw(), theme_minimal(), and theme_void()\n\nCan be added to the visualization plot to change the preset visual configuration. Let’s see how looks the last visualization:\n\nggplot( mtcars, aes( x = hp, y = mpg ) ) +\n  geom_point( aes( color = factor(vs) ) ) +\n  geom_smooth( method = 'loess' ) +\n  theme_bw()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nggplot( mtcars, aes( x = hp, y = mpg ) ) +\n  geom_point( aes( color = factor(vs) ) ) +\n  geom_smooth( method = 'loess' ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nggplot( mtcars, aes( x = hp, y = mpg ) ) +\n  geom_point( aes( color = factor(vs) ) ) +\n  geom_smooth( method = 'loess' ) +\n  theme_linedraw()\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Annotations",
      "Data Wrangling and Model",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Enchanced Visualizations</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/13_enchanced_visualizations.html#useful-links",
    "href": "to_book/notebooks/13_enchanced_visualizations.html#useful-links",
    "title": "10  Enchanced Visualizations",
    "section": "11.4 Useful links",
    "text": "11.4 Useful links\nCheatsheet:\nhttps://github.com/rstudio/cheatsheets/blob/4ee6c2f03a32478560f927834af25c13c899a0a2/data-visualization.pdf\nAdding labels to a graph with geom_text():\nhttps://r-graphics.org/recipe-bar-graph-labels",
    "crumbs": [
      "Annotations",
      "Data Wrangling and Model",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Enchanced Visualizations</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/13_enchanced_visualizations.html#facet-grids",
    "href": "to_book/notebooks/13_enchanced_visualizations.html#facet-grids",
    "title": "10  Enchanced Visualizations",
    "section": "12.1 Facet grids",
    "text": "12.1 Facet grids\nFacet grids to generate multiple graphics on a matrix.\n\nfacet_grid(): to divide the visualization\nfacet_wrap(): forces the subplots into a rectangular matrix\nvars(): it is needed to pass the variables which will divide the facet_grid() or facet_wrap().\n\n\ndf.diam &lt;- diamonds\n\nggplot( df.diam ) +\n  geom_point( aes( x = carat, y = price, \n                   color = factor(cut), alpha = 0.5) )\n\n\n\n\n\n\n\n\n\nggplot( df.diam ) +\n  geom_point( aes( x = carat, y = price,\n                   color = factor(cut),\n                   alpha = 0.5 ) ) +\n  facet_grid(rows = vars(cut) ) +\n  theme( legend.position = \"none\" )\n\n\n\n\n\n\n\n\n\nggplot( df.diam ) +\n  geom_point( aes( x = carat, y = price,\n                   color = cut, alpha = 0.6 ) ) +\n  facet_grid( rows = vars(cut), cols = vars(clarity) ) +\n  theme_minimal() +\n  theme( legend.position = \"none\")\n\n\n\n\n\n\n\n\n\ng &lt;- ggplot( df.diam ) +\n  geom_point( aes( x = carat, y = price,\n                   color = factor(cut),\n                   alpha = 0.5 ) ) +\n  facet_grid(rows = vars(cut) ) +\n  theme_minimal() +\n  theme( legend.position = \"none\" )\ng + facet_wrap( vars(cut) )",
    "crumbs": [
      "Annotations",
      "Data Wrangling and Model",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Enchanced Visualizations</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/13_enchanced_visualizations.html#map-plots",
    "href": "to_book/notebooks/13_enchanced_visualizations.html#map-plots",
    "title": "10  Enchanced Visualizations",
    "section": "12.2 Map plots",
    "text": "12.2 Map plots\nThis section will be a slightly view to the capabilities of ggplot2 with spatial data analysis.\n\ngeom_map()\n\nThe first thing to do is to load the map to an object. Then we have to map the locations of state capitals.\n\n# Load the map\nus &lt;- map_data(\"state\")\n\n# Load the dataset:\nurl &lt;- \"https://raw.githubusercontent.com/PacktPublishing/Data-Wrangling-with-R/main/Part3/Chapter11/USA_states.csv\"\nstates &lt;- read_csv(url)\n\nRows: 50 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): state, state_cd, capital\ndbl (3): latitude, longitude, GDP\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nremove(url)\n\n\n# 'state' column to lowercase\nstates &lt;- states %&gt;% mutate( state = str_to_lower(state) )\n\n# Map locations\nus_map &lt;- ggplot( states ) +\n  geom_map( aes( longitude, latitude,\n                 map_id = state),\n            map = us, color = \"black\", fill = \"azure\" ) +\n  xlim(-130, -60) +\n  ylim(20,55) +\n  theme_void()\n\nWarning in geom_map(aes(longitude, latitude, map_id = state), map = us, :\nIgnoring unknown aesthetics: x and y\n\nus_map +\n  geom_point( aes( x = longitude,\n                   y = latitude,\n                   size = GDP,\n                   fill = GDP),\n              shape = 24 )\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThe xlim() and ylim() center the map to the states, there are other two points out of the map.",
    "crumbs": [
      "Annotations",
      "Data Wrangling and Model",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Enchanced Visualizations</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/13_enchanced_visualizations.html#time-series-plots",
    "href": "to_book/notebooks/13_enchanced_visualizations.html#time-series-plots",
    "title": "10  Enchanced Visualizations",
    "section": "12.3 Time series plots",
    "text": "12.3 Time series plots\nLine plots may be the best type of graph to visualize a time series which is a sequence of days, months or other kind of time frame.\nWe will add to the plot scale_x_date() to create x-tics with the dates. Can be limited to make a ‘close up’ in a month with limit= argument, for example. With the argument date_breaks() it can be specified when we want a tick, the date_labels= argument to specified the format of date to print.\n\ndf.time &lt;- data.frame(\n  date = seq(ymd('2022-01-01'), ymd('2022-06-30'), by = 'days'),\n  measure = as.integer(runif(181, min = 600, max = 1000) +\n    sort( rexp( 181, 0.001 ) ) )\n)\nhead(df.time)\n\n        date measure\n1 2022-01-01     714\n2 2022-01-02    1009\n3 2022-01-03     684\n4 2022-01-04     760\n5 2022-01-05    1009\n6 2022-01-06     824\n\n\n\nbasic.plot &lt;- ggplot( df.time ) +\n  geom_line( aes( x = date, y = measure ), size = 0.8 ) +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\nbasic.plot\n\n\n\n\n\n\n\n\n\nbasic.plot + \n  scale_x_date(date_breaks = \"4 weeks\", date_labels = \"%W %y\")\n\n\n\n\n\n\n\n\n\nbasic.plot + \n  scale_x_date(date_breaks = \"1 weeks\", date_labels = \"%d %m\",\n               limit=as.Date( c(\"2022-05-01\", \"2022-06-01\") ) )\n\nWarning: Removed 149 rows containing missing values or values outside the scale range\n(`geom_line()`).",
    "crumbs": [
      "Annotations",
      "Data Wrangling and Model",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Enchanced Visualizations</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/13_enchanced_visualizations.html#d-plots",
    "href": "to_book/notebooks/13_enchanced_visualizations.html#d-plots",
    "title": "10  Enchanced Visualizations",
    "section": "12.4 3D Plots",
    "text": "12.4 3D Plots\nThe classic 2d plot will be the best option, however a 3D graphic could create a good impression to the audience. Sometimes a 3D graph can be useful, with surface graphics which represents the surface of a place. Also when two variables are very close to each other and the separation may happen in other dimension.\n\nplot_ly(): from plotly library.\n\n\n# Surface 3D plot\nsurface &lt;- matrix( as.integer( sort( abs( runif(160, 90, 180) ) ) ),\n                   nrow = 80, ncol = 20 )\n\nplot_ly(z = ~ surface) %&gt;% add_surface()\n\n\n\n\n\n\n# 2D and 3D comparison\n\nvar1 = rnorm(20, mean = 25, sd = 5 )\nvar2 = var1 + rgamma(20,1)\n\ndf.2d3d &lt;- data.frame( var1 = var1,\n                       var2 = var2,\n                       var3 = 1:20,\n                       var4 = rep( c(\"A\", \"B\"), each = 10) )\n\n## plot 2d\n\nggplot(df.2d3d) +\n    geom_point( aes( x = var1, y = var2, color = var4 ) )\n\n\n\n\n\n\n\n\n\n# Plot 3d\n\nplot_ly( df.2d3d, x = ~var1, y = ~var2, z = ~var3,\n         color = ~var4, colors = c(\"turquoise\", \"coral\") ) %&gt;%\n    add_markers()   # To add the points\n\n\n\n\n\nJust that, these are the two type of 3D graphs recommended, otherwise the third dimension could add more complexity to the interpretation of the data specially with comparisons. Then, 3D graphs are not good when precision is required.",
    "crumbs": [
      "Annotations",
      "Data Wrangling and Model",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Enchanced Visualizations</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/13_enchanced_visualizations.html#adding-interactivity-to-graphics",
    "href": "to_book/notebooks/13_enchanced_visualizations.html#adding-interactivity-to-graphics",
    "title": "10  Enchanced Visualizations",
    "section": "12.5 Adding interactivity to graphics",
    "text": "12.5 Adding interactivity to graphics\nCombining ggplot2 graphs with plotly it will add some interaction with the visualization.\nUsing one of the plots used before, this is how looks like adding interactivity:\n\nggplotly(): passing the ggplot() code will add interaction.\n\n\nggplotly(\n    \n    ggplot( df.diam ) +\n  geom_point( aes( x = carat, y = price, \n                   color = factor(cut), alpha = 0.5) )\n  \n)\n\n\n\n\n\nNow, hovering the points we obtain the corresponding values.\nPD: this last visualization with ggplotly() consumes a lot of computer’s resources.",
    "crumbs": [
      "Annotations",
      "Data Wrangling and Model",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Enchanced Visualizations</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/13_enchanced_visualizations.html#useful-links-1",
    "href": "to_book/notebooks/13_enchanced_visualizations.html#useful-links-1",
    "title": "10  Enchanced Visualizations",
    "section": "12.6 Useful links",
    "text": "12.6 Useful links\n\nMaps with ggplot2: https://ggplot2-book.org/maps.html\nplotly R: https://plotly.com/r/\nTime series: https://r-graph-gallery.com/279-plotting-time-series-with-ggplot2.html",
    "crumbs": [
      "Annotations",
      "Data Wrangling and Model",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Enchanced Visualizations</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/13_enchanced_visualizations.html#plotting-graphics-in-microsoft-power-bi",
    "href": "to_book/notebooks/13_enchanced_visualizations.html#plotting-graphics-in-microsoft-power-bi",
    "title": "10  Enchanced Visualizations",
    "section": "13.1 Plotting graphics in Microsoft Power BI",
    "text": "13.1 Plotting graphics in Microsoft Power BI\nSometimes we will need a custom visualization for Power BI, then we can plot a ggplot2 graphic in Power BI which it will give us more flexibility.\n\nvar1 = rnorm(200, mean = 25, sd = 5)\nvar2 = var1 + rnorm(200, mean = 25, sd = 15)\n\ndf.pwbi &lt;- data.frame( var1 = var1,\n                       var2 = var2,\n                       var3 = 1:200,\n                       var4 = rep( c(\"A\", \"B\", \"C\", \"D\"), each = 50) )\n\nwrite_csv(df.pwbi, \"../data/r2pwrbi.csv\")\n\nOnce we have the dataset imported to Power BI, when we click on the ‘R’ button on the visualizations side panel will show a pop-up to Enable script visuals.\nIt will appear a box on the dashboard, there we drop variables which we want to work. To do an histogram we drop only one. Once we do that, on the bottom field the ‘R script editor’ will be enabled.\n\nPower BI will use ‘dataset’ variable name to hold the data.frame object with all the variables we drop on the plotting area.\nIt will remove duplicates entries.\nThe code it shown is commented for our information but is executed in background.\n\nLet’s try an histogram, for that I will use this code:\n\ndataset &lt;- df.pwbi\nlibrary(ggplot2)\n\nggplot(dataset) +\n    geom_histogram( aes(var1), bins = 10, \n                    color = \"azure\", fill = \"darkblue\") +\n    labs( title = \"Histogram of Variable 1\",\n          subtitle = \"Random variable visualization on Power BI\") +\n    theme_minimal()\n\n\n\n\n\n\n\n\nOnce is written, just click ‘Run script’ and if its compatible, it will be shown.",
    "crumbs": [
      "Annotations",
      "Data Wrangling and Model",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Enchanced Visualizations</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/13_enchanced_visualizations.html#creating-word-clouds",
    "href": "to_book/notebooks/13_enchanced_visualizations.html#creating-word-clouds",
    "title": "10  Enchanced Visualizations",
    "section": "14.1 Creating word clouds",
    "text": "14.1 Creating word clouds\n\nwordcloud2(data = word.freq, color = \"random-dark\", size = 1)",
    "crumbs": [
      "Annotations",
      "Data Wrangling and Model",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Enchanced Visualizations</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/13_enchanced_visualizations.html#useful-links-2",
    "href": "to_book/notebooks/13_enchanced_visualizations.html#useful-links-2",
    "title": "10  Enchanced Visualizations",
    "section": "14.2 Useful links",
    "text": "14.2 Useful links\n\nText mining: https://www.tidytextmining.com/tidytext.html\nPowerBI visuals with R: https://learn.microsoft.com/en-us/power-bi/create-reports/desktop-r-visuals",
    "crumbs": [
      "Annotations",
      "Data Wrangling and Model",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Enchanced Visualizations</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/14_modeling2deploy.html",
    "href": "to_book/notebooks/14_modeling2deploy.html",
    "title": "11  Modeling to Deploy",
    "section": "",
    "text": "12 Building a Model",
    "crumbs": [
      "Annotations",
      "Data Wrangling and Model",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Modeling to Deploy</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/14_modeling2deploy.html#machine-learning-concepts",
    "href": "to_book/notebooks/14_modeling2deploy.html#machine-learning-concepts",
    "title": "11  Modeling to Deploy",
    "section": "12.1 Machine Learning concepts",
    "text": "12.1 Machine Learning concepts\nModeling data involves finding patterns that can help us explain a response (most probable outcome). One of the most important things is that the input data is clean and representative of the reality we are trying to model.\n\nClassification models: Used for categorical output. We transform the input data into patterns that will be separated into groups. Thus, each observation will be classified into a group, according to its patterns.\n\nKNN, decision tree, random forest, logistic regression, and support vector machine.\n\nRegression models: Used for numerical output. It will find patterns in number and return a continuous output. As summary, it will captures the relationship between the input variables and calculate an estimate of the output.\n\nLinear regression, polynomial regression, and regression tree.\n\n\nAlgorithm types:\n\nSupervised: algorithm which receives data containing variables that can explain an outcome, the content and the answers to learn. Once it learn the patterns, with new data it will generalize the solution. Can be used classification and regression models.\nUnsupervised: algorithm which do not receive labeled variable, instead read the dataset looking for patterns that can help explain the data. Clustering models use this algorithm.\nReinforcement: algorithm which learn by trial and error. It will perform an actions and check how its going. Positive is rewarded, negative is penalized. It tries to reduce the penalties to the minimum possible. Useful for video games.",
    "crumbs": [
      "Annotations",
      "Data Wrangling and Model",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Modeling to Deploy</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/14_modeling2deploy.html#understanding-the-project",
    "href": "to_book/notebooks/14_modeling2deploy.html#understanding-the-project",
    "title": "11  Modeling to Deploy",
    "section": "12.2 Understanding the project",
    "text": "12.2 Understanding the project\n\nWhen starting a project, we need a purpose which is the goal we want to reach at the end.\n\n\n12.2.1 The dataset\n\nurl.data &lt;- \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\" \n\nurl.names &lt;- \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.names\"\n\nnames &lt;- read_table(url.names, col_names = FALSE) %&gt;% suppressWarnings()\nnames &lt;- names %&gt;% filter(X2 == \"continuous.\") %&gt;% select(X1)\nnames &lt;- names %&gt;% mutate(X1 = gsub(pattern = \"\\\\:\", replacement = \"\", x = X1) )\n\ndf &lt;- read_csv(url.data, \n               col_names = c(names[[\"X1\"]], \"spam_cat\"), \n               trim_ws = TRUE) %&gt;% suppressWarnings()\n\nglimpse(df)\n\nRows: 4,601\nColumns: 58\n$ word_freq_make             &lt;dbl&gt; 0.00, 0.21, 0.06, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_address          &lt;dbl&gt; 0.64, 0.28, 0.00, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_all              &lt;dbl&gt; 0.64, 0.50, 0.71, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_3d               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_our              &lt;dbl&gt; 0.32, 0.14, 1.23, 0.63, 0.63, 1.85, 1.92, 1…\n$ word_freq_over             &lt;dbl&gt; 0.00, 0.28, 0.19, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_remove           &lt;dbl&gt; 0.00, 0.21, 0.19, 0.31, 0.31, 0.00, 0.00, 0…\n$ word_freq_internet         &lt;dbl&gt; 0.00, 0.07, 0.12, 0.63, 0.63, 1.85, 0.00, 1…\n$ word_freq_order            &lt;dbl&gt; 0.00, 0.00, 0.64, 0.31, 0.31, 0.00, 0.00, 0…\n$ word_freq_mail             &lt;dbl&gt; 0.00, 0.94, 0.25, 0.63, 0.63, 0.00, 0.64, 0…\n$ word_freq_receive          &lt;dbl&gt; 0.00, 0.21, 0.38, 0.31, 0.31, 0.00, 0.96, 0…\n$ word_freq_will             &lt;dbl&gt; 0.64, 0.79, 0.45, 0.31, 0.31, 0.00, 1.28, 0…\n$ word_freq_people           &lt;dbl&gt; 0.00, 0.65, 0.12, 0.31, 0.31, 0.00, 0.00, 0…\n$ word_freq_report           &lt;dbl&gt; 0.00, 0.21, 0.00, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_addresses        &lt;dbl&gt; 0.00, 0.14, 1.75, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_free             &lt;dbl&gt; 0.32, 0.14, 0.06, 0.31, 0.31, 0.00, 0.96, 0…\n$ word_freq_business         &lt;dbl&gt; 0.00, 0.07, 0.06, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_email            &lt;dbl&gt; 1.29, 0.28, 1.03, 0.00, 0.00, 0.00, 0.32, 0…\n$ word_freq_you              &lt;dbl&gt; 1.93, 3.47, 1.36, 3.18, 3.18, 0.00, 3.85, 0…\n$ word_freq_credit           &lt;dbl&gt; 0.00, 0.00, 0.32, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_your             &lt;dbl&gt; 0.96, 1.59, 0.51, 0.31, 0.31, 0.00, 0.64, 0…\n$ word_freq_font             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_000              &lt;dbl&gt; 0.00, 0.43, 1.16, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_money            &lt;dbl&gt; 0.00, 0.43, 0.06, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_hp               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_hpl              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_george           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_650              &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_lab              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_labs             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_telnet           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_857              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_data             &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_415              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_85               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_technology       &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_1999             &lt;dbl&gt; 0.00, 0.07, 0.00, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_parts            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_pm               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_direct           &lt;dbl&gt; 0.00, 0.00, 0.06, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_cs               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_meeting          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_original         &lt;dbl&gt; 0.00, 0.00, 0.12, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_project          &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_re               &lt;dbl&gt; 0.00, 0.00, 0.06, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_edu              &lt;dbl&gt; 0.00, 0.00, 0.06, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_table            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_conference       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ `char_freq_;`              &lt;dbl&gt; 0.000, 0.000, 0.010, 0.000, 0.000, 0.000, 0…\n$ `char_freq_(`              &lt;dbl&gt; 0.000, 0.132, 0.143, 0.137, 0.135, 0.223, 0…\n$ `char_freq_[`              &lt;dbl&gt; 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0…\n$ `char_freq_!`              &lt;dbl&gt; 0.778, 0.372, 0.276, 0.137, 0.135, 0.000, 0…\n$ `char_freq_$`              &lt;dbl&gt; 0.000, 0.180, 0.184, 0.000, 0.000, 0.000, 0…\n$ `char_freq_#`              &lt;dbl&gt; 0.000, 0.048, 0.010, 0.000, 0.000, 0.000, 0…\n$ capital_run_length_average &lt;dbl&gt; 3.756, 5.114, 9.821, 3.537, 3.537, 3.000, 1…\n$ capital_run_length_longest &lt;dbl&gt; 61, 101, 485, 40, 40, 15, 4, 11, 445, 43, 6…\n$ capital_run_length_total   &lt;dbl&gt; 278, 1028, 2259, 191, 191, 54, 112, 49, 125…\n$ spam_cat                   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n\n\nThere are 58 column, the last column ‘spam_cat’ is the target, which is a label classification of spam (1) or not spam (0).\n\ndim(df)\n\n[1] 4601   58\n\n\nEach variable represents specific words associated with spam and their percentage present in the message.\n\n\n12.2.2 The project\nObjective: Create a spam detector using AI models. Create a tool that can get any text as input and estimates the probability of that message being classified as spam or not.\nCase: Company which send a lot of commercial email wants to reduce their emails to be spam. The dataset provided by the company contain some words with percentages and if were classified as spam or not.\n\n\n12.2.3 The algorithm\nThis is a classification problem, then It has to be used a model which classifies. Random Forest will be it.",
    "crumbs": [
      "Annotations",
      "Data Wrangling and Model",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Modeling to Deploy</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/14_modeling2deploy.html#preparing-data-for-modeling",
    "href": "to_book/notebooks/14_modeling2deploy.html#preparing-data-for-modeling",
    "title": "11  Modeling to Deploy",
    "section": "12.3 Preparing data for modeling",
    "text": "12.3 Preparing data for modeling\nWe know our objective, then we have to wrangle the data to get there.\n\ntidyverse: data wrangling and visualization.\nskimr: create descriptive statistics summary.\npatchwork: to put graphics side by side.\nrandomForest: to create the model.\ncaret: to create the confusion matrix.\nROCR: to plot ROC curve.\n\n\ndim(df)   # Dataset previously loaded\n\n[1] 4601   58\n\n\n\nglimpse(df)\n\nRows: 4,601\nColumns: 58\n$ word_freq_make             &lt;dbl&gt; 0.00, 0.21, 0.06, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_address          &lt;dbl&gt; 0.64, 0.28, 0.00, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_all              &lt;dbl&gt; 0.64, 0.50, 0.71, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_3d               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_our              &lt;dbl&gt; 0.32, 0.14, 1.23, 0.63, 0.63, 1.85, 1.92, 1…\n$ word_freq_over             &lt;dbl&gt; 0.00, 0.28, 0.19, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_remove           &lt;dbl&gt; 0.00, 0.21, 0.19, 0.31, 0.31, 0.00, 0.00, 0…\n$ word_freq_internet         &lt;dbl&gt; 0.00, 0.07, 0.12, 0.63, 0.63, 1.85, 0.00, 1…\n$ word_freq_order            &lt;dbl&gt; 0.00, 0.00, 0.64, 0.31, 0.31, 0.00, 0.00, 0…\n$ word_freq_mail             &lt;dbl&gt; 0.00, 0.94, 0.25, 0.63, 0.63, 0.00, 0.64, 0…\n$ word_freq_receive          &lt;dbl&gt; 0.00, 0.21, 0.38, 0.31, 0.31, 0.00, 0.96, 0…\n$ word_freq_will             &lt;dbl&gt; 0.64, 0.79, 0.45, 0.31, 0.31, 0.00, 1.28, 0…\n$ word_freq_people           &lt;dbl&gt; 0.00, 0.65, 0.12, 0.31, 0.31, 0.00, 0.00, 0…\n$ word_freq_report           &lt;dbl&gt; 0.00, 0.21, 0.00, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_addresses        &lt;dbl&gt; 0.00, 0.14, 1.75, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_free             &lt;dbl&gt; 0.32, 0.14, 0.06, 0.31, 0.31, 0.00, 0.96, 0…\n$ word_freq_business         &lt;dbl&gt; 0.00, 0.07, 0.06, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_email            &lt;dbl&gt; 1.29, 0.28, 1.03, 0.00, 0.00, 0.00, 0.32, 0…\n$ word_freq_you              &lt;dbl&gt; 1.93, 3.47, 1.36, 3.18, 3.18, 0.00, 3.85, 0…\n$ word_freq_credit           &lt;dbl&gt; 0.00, 0.00, 0.32, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_your             &lt;dbl&gt; 0.96, 1.59, 0.51, 0.31, 0.31, 0.00, 0.64, 0…\n$ word_freq_font             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_000              &lt;dbl&gt; 0.00, 0.43, 1.16, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_money            &lt;dbl&gt; 0.00, 0.43, 0.06, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_hp               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_hpl              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_george           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_650              &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_lab              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_labs             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_telnet           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_857              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_data             &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_415              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_85               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_technology       &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_1999             &lt;dbl&gt; 0.00, 0.07, 0.00, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_parts            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_pm               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_direct           &lt;dbl&gt; 0.00, 0.00, 0.06, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_cs               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_meeting          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_original         &lt;dbl&gt; 0.00, 0.00, 0.12, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_project          &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_re               &lt;dbl&gt; 0.00, 0.00, 0.06, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_edu              &lt;dbl&gt; 0.00, 0.00, 0.06, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_table            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_conference       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ `char_freq_;`              &lt;dbl&gt; 0.000, 0.000, 0.010, 0.000, 0.000, 0.000, 0…\n$ `char_freq_(`              &lt;dbl&gt; 0.000, 0.132, 0.143, 0.137, 0.135, 0.223, 0…\n$ `char_freq_[`              &lt;dbl&gt; 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0…\n$ `char_freq_!`              &lt;dbl&gt; 0.778, 0.372, 0.276, 0.137, 0.135, 0.000, 0…\n$ `char_freq_$`              &lt;dbl&gt; 0.000, 0.180, 0.184, 0.000, 0.000, 0.000, 0…\n$ `char_freq_#`              &lt;dbl&gt; 0.000, 0.048, 0.010, 0.000, 0.000, 0.000, 0…\n$ capital_run_length_average &lt;dbl&gt; 3.756, 5.114, 9.821, 3.537, 3.537, 3.000, 1…\n$ capital_run_length_longest &lt;dbl&gt; 61, 101, 485, 40, 40, 15, 4, 11, 445, 43, 6…\n$ capital_run_length_total   &lt;dbl&gt; 278, 1028, 2259, 191, 191, 54, 112, 49, 125…\n$ spam_cat                   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n\n\nVariables representing frequencies will maintain the double class, capital* variable will be set to Integer and spam_cat to factor.\n\nvars.int &lt;- c(\"capital_run_length_average\", \n              \"capital_run_length_longest\", \n              \"capital_run_length_total\")\ndf &lt;- df %&gt;%\n  mutate_at(\"spam_cat\", as.factor) %&gt;%\n  mutate_at(vars.int, as.integer)\n\nanyNA(df)\n\n[1] FALSE\n\n\nThere are no missing values NA, then we can proceed with descriptive statistics using skim().\n\n# Disable scientific notion\noptions( scipen = 999, digits = 4 )\n\nskim(df)\n\n\nData summary\n\n\nName\ndf\n\n\nNumber of rows\n4601\n\n\nNumber of columns\n58\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n57\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nspam_cat\n0\n1\nFALSE\n2\n0: 2788, 1: 1813\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nword_freq_make\n0\n1\n0.10\n0.31\n0\n0\n0.00\n0.00\n4.54\n▇▁▁▁▁\n\n\nword_freq_address\n0\n1\n0.21\n1.29\n0\n0\n0.00\n0.00\n14.28\n▇▁▁▁▁\n\n\nword_freq_all\n0\n1\n0.28\n0.50\n0\n0\n0.00\n0.42\n5.10\n▇▁▁▁▁\n\n\nword_freq_3d\n0\n1\n0.07\n1.40\n0\n0\n0.00\n0.00\n42.81\n▇▁▁▁▁\n\n\nword_freq_our\n0\n1\n0.31\n0.67\n0\n0\n0.00\n0.38\n10.00\n▇▁▁▁▁\n\n\nword_freq_over\n0\n1\n0.10\n0.27\n0\n0\n0.00\n0.00\n5.88\n▇▁▁▁▁\n\n\nword_freq_remove\n0\n1\n0.11\n0.39\n0\n0\n0.00\n0.00\n7.27\n▇▁▁▁▁\n\n\nword_freq_internet\n0\n1\n0.11\n0.40\n0\n0\n0.00\n0.00\n11.11\n▇▁▁▁▁\n\n\nword_freq_order\n0\n1\n0.09\n0.28\n0\n0\n0.00\n0.00\n5.26\n▇▁▁▁▁\n\n\nword_freq_mail\n0\n1\n0.24\n0.64\n0\n0\n0.00\n0.16\n18.18\n▇▁▁▁▁\n\n\nword_freq_receive\n0\n1\n0.06\n0.20\n0\n0\n0.00\n0.00\n2.61\n▇▁▁▁▁\n\n\nword_freq_will\n0\n1\n0.54\n0.86\n0\n0\n0.10\n0.80\n9.67\n▇▁▁▁▁\n\n\nword_freq_people\n0\n1\n0.09\n0.30\n0\n0\n0.00\n0.00\n5.55\n▇▁▁▁▁\n\n\nword_freq_report\n0\n1\n0.06\n0.34\n0\n0\n0.00\n0.00\n10.00\n▇▁▁▁▁\n\n\nword_freq_addresses\n0\n1\n0.05\n0.26\n0\n0\n0.00\n0.00\n4.41\n▇▁▁▁▁\n\n\nword_freq_free\n0\n1\n0.25\n0.83\n0\n0\n0.00\n0.10\n20.00\n▇▁▁▁▁\n\n\nword_freq_business\n0\n1\n0.14\n0.44\n0\n0\n0.00\n0.00\n7.14\n▇▁▁▁▁\n\n\nword_freq_email\n0\n1\n0.18\n0.53\n0\n0\n0.00\n0.00\n9.09\n▇▁▁▁▁\n\n\nword_freq_you\n0\n1\n1.66\n1.78\n0\n0\n1.31\n2.64\n18.75\n▇▁▁▁▁\n\n\nword_freq_credit\n0\n1\n0.09\n0.51\n0\n0\n0.00\n0.00\n18.18\n▇▁▁▁▁\n\n\nword_freq_your\n0\n1\n0.81\n1.20\n0\n0\n0.22\n1.27\n11.11\n▇▁▁▁▁\n\n\nword_freq_font\n0\n1\n0.12\n1.03\n0\n0\n0.00\n0.00\n17.10\n▇▁▁▁▁\n\n\nword_freq_000\n0\n1\n0.10\n0.35\n0\n0\n0.00\n0.00\n5.45\n▇▁▁▁▁\n\n\nword_freq_money\n0\n1\n0.09\n0.44\n0\n0\n0.00\n0.00\n12.50\n▇▁▁▁▁\n\n\nword_freq_hp\n0\n1\n0.55\n1.67\n0\n0\n0.00\n0.00\n20.83\n▇▁▁▁▁\n\n\nword_freq_hpl\n0\n1\n0.27\n0.89\n0\n0\n0.00\n0.00\n16.66\n▇▁▁▁▁\n\n\nword_freq_george\n0\n1\n0.77\n3.37\n0\n0\n0.00\n0.00\n33.33\n▇▁▁▁▁\n\n\nword_freq_650\n0\n1\n0.12\n0.54\n0\n0\n0.00\n0.00\n9.09\n▇▁▁▁▁\n\n\nword_freq_lab\n0\n1\n0.10\n0.59\n0\n0\n0.00\n0.00\n14.28\n▇▁▁▁▁\n\n\nword_freq_labs\n0\n1\n0.10\n0.46\n0\n0\n0.00\n0.00\n5.88\n▇▁▁▁▁\n\n\nword_freq_telnet\n0\n1\n0.06\n0.40\n0\n0\n0.00\n0.00\n12.50\n▇▁▁▁▁\n\n\nword_freq_857\n0\n1\n0.05\n0.33\n0\n0\n0.00\n0.00\n4.76\n▇▁▁▁▁\n\n\nword_freq_data\n0\n1\n0.10\n0.56\n0\n0\n0.00\n0.00\n18.18\n▇▁▁▁▁\n\n\nword_freq_415\n0\n1\n0.05\n0.33\n0\n0\n0.00\n0.00\n4.76\n▇▁▁▁▁\n\n\nword_freq_85\n0\n1\n0.11\n0.53\n0\n0\n0.00\n0.00\n20.00\n▇▁▁▁▁\n\n\nword_freq_technology\n0\n1\n0.10\n0.40\n0\n0\n0.00\n0.00\n7.69\n▇▁▁▁▁\n\n\nword_freq_1999\n0\n1\n0.14\n0.42\n0\n0\n0.00\n0.00\n6.89\n▇▁▁▁▁\n\n\nword_freq_parts\n0\n1\n0.01\n0.22\n0\n0\n0.00\n0.00\n8.33\n▇▁▁▁▁\n\n\nword_freq_pm\n0\n1\n0.08\n0.43\n0\n0\n0.00\n0.00\n11.11\n▇▁▁▁▁\n\n\nword_freq_direct\n0\n1\n0.06\n0.35\n0\n0\n0.00\n0.00\n4.76\n▇▁▁▁▁\n\n\nword_freq_cs\n0\n1\n0.04\n0.36\n0\n0\n0.00\n0.00\n7.14\n▇▁▁▁▁\n\n\nword_freq_meeting\n0\n1\n0.13\n0.77\n0\n0\n0.00\n0.00\n14.28\n▇▁▁▁▁\n\n\nword_freq_original\n0\n1\n0.05\n0.22\n0\n0\n0.00\n0.00\n3.57\n▇▁▁▁▁\n\n\nword_freq_project\n0\n1\n0.08\n0.62\n0\n0\n0.00\n0.00\n20.00\n▇▁▁▁▁\n\n\nword_freq_re\n0\n1\n0.30\n1.01\n0\n0\n0.00\n0.11\n21.42\n▇▁▁▁▁\n\n\nword_freq_edu\n0\n1\n0.18\n0.91\n0\n0\n0.00\n0.00\n22.05\n▇▁▁▁▁\n\n\nword_freq_table\n0\n1\n0.01\n0.08\n0\n0\n0.00\n0.00\n2.17\n▇▁▁▁▁\n\n\nword_freq_conference\n0\n1\n0.03\n0.29\n0\n0\n0.00\n0.00\n10.00\n▇▁▁▁▁\n\n\nchar_freq_;\n0\n1\n0.04\n0.24\n0\n0\n0.00\n0.00\n4.38\n▇▁▁▁▁\n\n\nchar_freq_(\n0\n1\n0.14\n0.27\n0\n0\n0.06\n0.19\n9.75\n▇▁▁▁▁\n\n\nchar_freq_[\n0\n1\n0.02\n0.11\n0\n0\n0.00\n0.00\n4.08\n▇▁▁▁▁\n\n\nchar_freq_!\n0\n1\n0.27\n0.82\n0\n0\n0.00\n0.32\n32.48\n▇▁▁▁▁\n\n\nchar_freq_$\n0\n1\n0.08\n0.25\n0\n0\n0.00\n0.05\n6.00\n▇▁▁▁▁\n\n\nchar_freq_#\n0\n1\n0.04\n0.43\n0\n0\n0.00\n0.00\n19.83\n▇▁▁▁▁\n\n\ncapital_run_length_average\n0\n1\n4.76\n31.74\n1\n1\n2.00\n3.00\n1102.00\n▇▁▁▁▁\n\n\ncapital_run_length_longest\n0\n1\n52.17\n194.89\n1\n6\n15.00\n43.00\n9989.00\n▇▁▁▁▁\n\n\ncapital_run_length_total\n0\n1\n283.29\n606.35\n1\n35\n95.00\n266.00\n15841.00\n▇▁▁▁▁\n\n\n\n\n\n\nMost of the p50 and p75 values are close to 0, meanwhile the mean is higher. Suggests a high right tail on the data.\nThere are 1813 spam emails (39.4%)\nHigh Standard deviation suggests outliers and spread data.",
    "crumbs": [
      "Annotations",
      "Data Wrangling and Model",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Modeling to Deploy</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/14_modeling2deploy.html#exploring-the-data-with-visuals",
    "href": "to_book/notebooks/14_modeling2deploy.html#exploring-the-data-with-visuals",
    "title": "11  Modeling to Deploy",
    "section": "12.4 Exploring the data with visuals",
    "text": "12.4 Exploring the data with visuals\n\nfor (var in colnames(select_if(df[,1:47], is.numeric) ) ) {\n    hist( unlist( df[,var]), col = \"blue\",\n          main = paste(\"Histogram of\", var),\n          xlab = var)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo use ggplot2 we need that dataset in tidy format, to do that we must convert this wide format to long format with the columns spam for ‘spam_cat’, word for every column name, and freq for the values.\n\ndf.long &lt;- df %&gt;%\n    pivot_longer( cols = 1:57, names_to = \"words\", values_to = \"pct\")\nhead(df.long, 9)\n\n# A tibble: 9 × 3\n  spam_cat words                pct\n  &lt;fct&gt;    &lt;chr&gt;              &lt;dbl&gt;\n1 1        word_freq_make      0   \n2 1        word_freq_address   0.64\n3 1        word_freq_all       0.64\n4 1        word_freq_3d        0   \n5 1        word_freq_our       0.32\n6 1        word_freq_over      0   \n7 1        word_freq_remove    0   \n8 1        word_freq_internet  0   \n9 1        word_freq_order     0   \n\n\nNow its ready for plotting with ggplot2, then we can create boxplots. Boxplot are useful to see the outliers that can distort our classifier (Random Forest), also which words appear more frequently which impact the classification. We can filter only the ‘word_’ from words column and spam==1.\n\ndf.long %&gt;%\n    filter( str_detect(words, \"word_\") & spam_cat == 1) %&gt;%\n    ggplot() +\n        geom_boxplot( aes( y = reorder(words, pct), x = pct, fill = spam_cat ) ) +\n        labs( title = \"Percentages of words and their association with spam emails\",\n              subtitle = \"The frequency of appearance of some words in emails is more associated with spam\",\n              x = \"Percentage\",\n              y = \"Word\" ) +\n        theme_classic() +\n        theme( plot.subtitle = element_text( color = \"darkgray\", size = 10 ),\n               legend.position = \"none\")\n\n\n\n\n\n\n\n\nAfter the 24th record all the boxplots have their medians too close to zero, so they do not impact the spam classification too much. Then we know that the top 23 words can have more impact so we have to compare spam and not spam in this words to see how they impact the entire data.\n\ntemp &lt;- df.long %&gt;% \n    filter( str_detect(words, \"word_\") & spam_cat == 1) %&gt;%\n    group_by(words) %&gt;% \n    summarise( pct_sum = sum(pct) ) %&gt;%\n    arrange( desc(pct_sum) )\n\ntop.words &lt;- c(temp$words[1:23], \"spam_cat\")\nremove(temp)\n\nIn this next code we generate a boxplot comparing spam and not spam only on the selected top words. It can be seen as a focus to see more easily the differences.\n\ndf.top &lt;- df %&gt;%\n    select( all_of(top.words) ) %&gt;%\n    mutate( top_words_pct = rowSums( across( where(is.numeric) ) ) )\n\ng1 &lt;- ggplot(df.top) + \n    geom_boxplot( aes( y = factor(spam_cat), x = top_words_pct),\n                  fill = c(\"turquoise\", \"coral\") ) +\n    labs( title = \"How the presence of words associated with spam emails \n          impacts the classification (TOP23)\",\n          subtitle = \"The spam emails(1) have a ghigher percentage of those words.\") +\n    theme_classic()\n\nIn the same way with this second boxplot we compare spam and not spam but with all the words.\n\ndf.2 &lt;- df %&gt;%\n    mutate(word_pct = rowSums( across( where(is.numeric) ) ) )\n\ng2 &lt;- ggplot(df.2) +\n    geom_boxplot( aes( y = factor(spam_cat), x = word_pct ),\n                  fill = c(\"turquoise\", \"coral\") ) +\n    labs( title = \"How spam associated words impacts the classification\",\n          subtitle = \"The spam emails(1) have a ghigher percentage of those words.\") +\n    theme_classic()\n\n\n# Patchwork library, putting the objects into a parenthesis with a pipe\n\n(g1 | g2)\n\n\n\n\n\n\n\n\nIt can be seen a large median of those words dissociated with spam emails. To be sure that difference is statistically significant we can perform Kolmogorov-Smirnov test which compare distributions, or U-Mann Whitney which compare medians.\n\npos.spam &lt;- df.top[df.top[\"spam_cat\"] == 1,][[\"top_words_pct\"]]\nneg.spam &lt;- df.top[df.top[\"spam_cat\"] == 0,][[\"top_words_pct\"]]\n\nks.test(pos.spam, neg.spam)\n\nWarning in ks.test.default(pos.spam, neg.spam): p-value will be approximate in\nthe presence of ties\n\n\n\n    Asymptotic two-sample Kolmogorov-Smirnov test\n\ndata:  pos.spam and neg.spam\nD = 0.55, p-value &lt;0.0000000000000002\nalternative hypothesis: two-sided\n\nwilx &lt;- wilcox.test(pos.spam, neg.spam)\nwilx\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  pos.spam and neg.spam\nW = 4182287, p-value &lt;0.0000000000000002\nalternative hypothesis: true location shift is not equal to 0\n\n# Size effect formula by Wendt\nrbis &lt;- sum(-1, (2 * wilx$statistic) / (length(pos.spam) * length(neg.spam)) )\ncat(\"U Mann Whitney effect size r = \", rbis)\n\nU Mann Whitney effect size r =  0.6548\n\n\nBoth test show p &lt; 0.05 thus both groups have different distribution and median according to KS-test and U Mann Whitney test respectively. Also, the size effect Biserial-Rank correlation of U Mann-Whitney is big meaning there are a significance difference between the groups. The spam group values tend to be considerably bigger than no-spam group. Then, the variable spam have a high impact in the difference of the groups.\nSince we know the words impact now we can test the characters in a similar way:\n\ndf.long %&gt;%\n    filter( str_detect(words, \"char_\") & spam_cat == 1) %&gt;%\n    ggplot() +\n        geom_boxplot( aes( y = reorder(words, pct), x = pct, fill = spam_cat ) ) +\n        labs( title = \"Percentages of special characters and their association with spam emails\",\n              subtitle = \"The frequency of appearance of some characters in emails is more associated with spam\",\n              x = \"Percentage\",\n              y = \"Character\" ) +\n        theme_classic() +\n        theme( plot.subtitle = element_text( color = \"darkgray\", size = 10 ),\n               legend.position = \"none\")\n\n\n\n\n\n\n\n\n\ndf.char &lt;- df %&gt;%\n    select_if(str_detect(colnames(df), pattern = \"char_|^spam_cat\")) %&gt;%\n    mutate( char_pct = rowSums( across( where(is.numeric) ) ) )\n\ng3 &lt;- ggplot(df.char) + \n    geom_boxplot( aes( y = factor(spam_cat), x = char_pct),\n                  fill = c(\"turquoise\", \"coral\") ) +\n    labs( title = \"How the presence of characters associated with spam emails \n          impacts the classification\",\n          subtitle = \"The spam emails(1) have a ghigher percentage of those characters.\") +\n    theme_classic()\n\ng3\n\n\n\n\n\n\n\n\n\ndf.char.zero &lt;- df.char %&gt;% filter(spam_cat == 0)\ndf.char.one &lt;- df.char %&gt;% filter(spam_cat == 1)\n\nfor (var in colnames(df.char.zero[,1:6])) {\n    wilx &lt;- wilcox.test(df.char.zero[[var]], df.char.one[[var]])\n    rbis &lt;- abs(sum(-1, (2 * wilx$statistic) / (length(pos.spam) * length(neg.spam)) ))\n    cat(\"\\nMann-Whitney U between spam and not spam on:\", var, \n        \"\\np-value:\", sprintf(\"%6.4f\", wilx$p.value),\n        \"\\nr:\", rbis)\n}\n\n\nMann-Whitney U between spam and not spam on: char_freq_; \np-value: 0.0001 \nr: 0.04412\nMann-Whitney U between spam and not spam on: char_freq_( \np-value: 0.0269 \nr: 0.03721\nMann-Whitney U between spam and not spam on: char_freq_[ \np-value: 0.0000 \nr: 0.07279\nMann-Whitney U between spam and not spam on: char_freq_! \np-value: 0.0000 \nr: 0.6581\nMann-Whitney U between spam and not spam on: char_freq_$ \np-value: 0.0000 \nr: 0.5443\nMann-Whitney U between spam and not spam on: char_freq_# \np-value: 0.0000 \nr: 0.2027\n\n\n\ng4 &lt;- df.char.zero %&gt;% \n    pivot_longer(cols = 1:6, names_to = \"chars\", values_to = \"pct\") %&gt;%\n    ggplot() +\n    geom_boxplot( aes( y = chars, x = pct) ) +\n    labs( y = \"Characters in not spam emails\",\n          x = \"\") +\n    theme_classic()\n\ng5 &lt;- df.char.one %&gt;% \n    pivot_longer(cols = 1:6, names_to = \"chars\", values_to = \"pct\") %&gt;%\n    ggplot() +\n    geom_boxplot( aes( y = chars, x = pct) ) +\n    labs( x = \"Frequencies\",\n          y = \"Characters in spam emails\") +\n    theme_classic()\n\n(g4 / g5)\n\n\n\n\n\n\n\n\nAt this point we know that the number of symbols, and words are statistically different for each group in our classification.",
    "crumbs": [
      "Annotations",
      "Data Wrangling and Model",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Modeling to Deploy</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/14_modeling2deploy.html#selecting-the-best-variables",
    "href": "to_book/notebooks/14_modeling2deploy.html#selecting-the-best-variables",
    "title": "11  Modeling to Deploy",
    "section": "12.5 Selecting the best variables",
    "text": "12.5 Selecting the best variables\nWhen we checked the variables with boxplots and test comparisons we show how them impact the classifications the most. We should use those variables that have the highest difference between both groups so that it’s easier for the algorithm to find a clearer separations between the two groups. The conclusion we show is that 23 words maximize the difference as well as uppercase and the presence of too many symbols.\nWe are prepare to create a dataset for modeling. Taking the original dataframe, binding top_words_pct, the target variable, the character and uppercase variables:\n\ndf.model &lt;- df %&gt;%\n    bind_cols( top_words_pct = df.top$top_words_pct ) %&gt;%\n    select( spam_cat, top_words_pct, \n            `char_freq_!`, `char_freq_(`, `char_freq_$`,\n            capital_run_length_total, capital_run_length_longest ) %&gt;%\n    mutate(spam_cat = ifelse(spam_cat == 1, \"is_spam\", \"no_spam\")) %&gt;%\n    mutate_at(vars(spam_cat), as.factor)\n\n\ncolnames(df.model)[3:5] &lt;- c(\"char_freq_exclam\", \"char_freq_paren\", \"char_freq_dollar\")\nslice_sample(df.model, n = 9)\n\n# A tibble: 9 × 7\n  spam_cat top_words_pct char_freq_exclam char_freq_paren char_freq_dollar\n  &lt;fct&gt;            &lt;dbl&gt;            &lt;dbl&gt;           &lt;dbl&gt;            &lt;dbl&gt;\n1 is_spam           5.22            0.764           0.382            0    \n2 no_spam           1.62            0.087           0.087            0    \n3 no_spam           0               0               1.08             0    \n4 is_spam           7.27            0.297           0                0    \n5 is_spam           9.61            0.12            0.06             0.541\n6 no_spam           0               0.862           0.862            0    \n7 no_spam           3.33            0.302           0.06             0    \n8 is_spam          10.8             0.057           0                0.057\n9 no_spam           3.68            0               0                0    \n# ℹ 2 more variables: capital_run_length_total &lt;int&gt;,\n#   capital_run_length_longest &lt;int&gt;",
    "crumbs": [
      "Annotations",
      "Data Wrangling and Model",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Modeling to Deploy</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/14_modeling2deploy.html#modeling",
    "href": "to_book/notebooks/14_modeling2deploy.html#modeling",
    "title": "11  Modeling to Deploy",
    "section": "12.6 Modeling",
    "text": "12.6 Modeling\n\n12.6.1 Training\nIn the last code we just replace 1 with ‘is_spam’ and 0 with ‘no_spam’. This is necessary for random forest in this case to classify.\n# Alternative code to replace values:\ndf.model &lt;- df.model %&gt;%\n    mutate( spam_cat = recode(spam_cat, '1'=\"is_spam\", '0'=\"no_spam\") )\n\ntrain: subset used to present the model with the patterns and the labels associated with it so that it can study how to classify each observation according to the patterns that occur.\ntest: subset where new data is presented to the trained model so that we can measure how accurate it is or how much it has learned.\n\nThe dataset have aprox 60-40 as not-spam and spam respectively. In this case, we won’t apply any category balancing technique, this imbalance will not affect the result.\nIn R there is no function for splitting the dataset into train and test\nWe are going to use 80% for training and 20% for testing.\n\nn.rows &lt;- nrow(df.model)\nidx &lt;- sample(1:n.rows, size = 0.8 * n.rows)\n\ntrain.80 &lt;- df.model[idx,]\ntest.20 &lt;- df.model[-idx,]\n\nNow we can check if the proportions are similar to the original dataset:\n\nwriteLines(\"Original set:\")\n\nOriginal set:\n\nprop.table( table(df$spam_cat) )\n\n\n    0     1 \n0.606 0.394 \n\nwriteLines(\"==================\")\n\n==================\n\nwriteLines(\"Train set:\")\n\nTrain set:\n\nprop.table( table(train.80$spam_cat) )\n\n\nis_spam no_spam \n 0.4019  0.5981 \n\nwriteLines(\"==================\")\n\n==================\n\nwriteLines(\"Test set:\")\n\nTest set:\n\nprop.table( table(test.20$spam_cat) )\n\n\nis_spam no_spam \n 0.3626  0.6374 \n\n\n\nrandomForest() It must be passed the target variable with ~ . meaning the target will be explained by the rest of the data represented with ‘.’\n\ndata= the data set to use, will be the training set.\nimportance= if TRUE it will calculate the importance of the variables.\nntree= the number of decision trees to create with this model.\n\n\n\nmodel.rf &lt;- randomForest( spam_cat ~ ., data = train.80,\n                          importance = TRUE,\n                          ntree = 250)\n\n\nplot(model.rf)\n\n\n\n\n\n\n\n\nThis plot shows the performance of the model. After the 50th tree, the error stabilizes.\n\nvarImpPlot(): to plot the feature’s importance or which variables are more importance to the model.\n\n\nvarImpPlot(model.rf)\n\n\n\n\n\n\n\n\nThis plot shows the importance of each variable.\n\nMean Decrease Accuracy: it calculates how much the model’s accuracy decreases when the values of a particular feature are randomly shuffled. A higher value indicates that a feature is more important for the model’s accuracy. Then the model relies heavily on that features to make accurate predictions.\nMean Decrease Gini: A higher value indicates that a feature is more important for the model’s ability to separate the classes. Features with high MeanDecreaseGini values are those that are useful for creating decision boundaries in the model.\n\nFor the library randomForest the default for classification models is the Gini index.\n\n\n12.6.2 Testing and evaluating the model\nThe most used metrics to evaluate a classification model are accuracy, confusion matrix, and ROC curve. - predict(): to generate a prediction given as arguments the model and the test set. The object assigned can be used to extract information.\n\nconfusionMatrix(): given a prediction and the target column from the test set shows the information of that prediction.\n\n\n# Prediction:\nmodel.preds &lt;- predict(model.rf, test.20)\n\nconfusionMatrix(model.preds, test.20$spam_cat, positive = \"no_spam\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction is_spam no_spam\n   is_spam     292      28\n   no_spam      42     559\n                                             \n               Accuracy : 0.924              \n                 95% CI : (0.905, 0.94)      \n    No Information Rate : 0.637              \n    P-Value [Acc &gt; NIR] : &lt;0.0000000000000002\n                                             \n                  Kappa : 0.834              \n                                             \n Mcnemar's Test P-Value : 0.12               \n                                             \n            Sensitivity : 0.952              \n            Specificity : 0.874              \n         Pos Pred Value : 0.930              \n         Neg Pred Value : 0.912              \n             Prevalence : 0.637              \n         Detection Rate : 0.607              \n   Detection Prevalence : 0.653              \n      Balanced Accuracy : 0.913              \n                                             \n       'Positive' Class : no_spam            \n                                             \n\n\nThe confusion matrix show how were the prediction. For ‘is_spam’ predicted value are 315 ‘is_spam’ as reference, then this 315 are True Positive. The next value is for the predicted ‘is_spam’ and ‘no_spam’ as reference with 38 matches, corresponding False Positive known as Type 1 Error. The next two values are False Negative (Type 2 Error) and True Negative with 52 and 516 matches respectively.\nThe accuracy is 0.9 aprox, then out of the 100 predictions, there will be around 10 errors with a confidence interval about 12 to 8 errors.\n\nAccuracy: How many of the total number of classifications the model predicted correctly. Is an overall sense of the model’s performance.\nSensitivity and Specificity: provide insights into the model’s performance on the positive and negative classes, respectively. In many cases, depending on the problem, one metric or the other will be more important. (e.g. Medicine, the true positive rate might be more critical then we’ll look sensitivity.)\nKappa: by accounting for chance agreement helps us to understand the model’s performance. A high kappa indicates a strong agreement between the predicted and actual labels.\nBalanced Accuracy: useful when the labels are unbalanced. It provides a balanced measure of the model’s performance.\n\nThe relatively high accuracy (0.902) and a strong kappa value (0.795), it suggests that the model is performing well overall. The sensitivity (0.858) and specificity (0.931) values indicate that the model detect both positive and negative instances quite well.\nNow it’s time to look at ROC curve to check the performance based on the rate of true positive and false positives (Sensitivity and Specificity).\n\nprediction() given the data frame with probability predictions with the positive class (‘no_spam’ according to confusionMatrix) and the real labels which is the test set selecting the target.\nperformance() given the object with prediction() and the labels for the axes which are ‘tpr’ and ‘fpr’ for true positive rate and false positive rate.\nabline() for the visualization will draw a diagonal that separates the graphic 50/50\n\n\nmodel.df.preds &lt;- data.frame( predict(model.rf, test.20, type='prob') )\n\nmodel.roc &lt;- prediction(model.df.preds$no_spam, test.20$spam_cat)\n\nroc &lt;- performance(model.roc, 'tpr', 'fpr')\nauc &lt;- performance(model.roc, \"auc\")\nplot(roc, colorize = T, lwd = 2,\n     main = \"ROC curve. 'no_spam' as positive class.\",\n     sub = paste( \"AUC =\", auc@y.values[[1]]) )\nabline(0.0, 1.0)\n\n\n\n\n\n\n\n\nAt the bottom we can see the Area Under the Receiver Operating Characteristic Curve (AUC-ROC). If AUC=1 represents a perfect classifier meanwhile AUC=0.5 is a random classifier. In this case is a pretty good classier, this value indicates the performance of the classifier.\n\n\n12.6.3 Predicting\nAfter evaluating, it’s time to test the model.\nThe input to predict must to be in the very same format as the data frame used in the Random Forest. Then, if we did any transformations this have to be repeated before we can input the new data.\nTo do that, we can create a custom function with the steps of preparation and modeling that we took until create ‘df.model’. To that custom function we pass whatever we want to predict depending the objective (spam in this case, then we pass a mail) and the function will perform every step to model the data.\nThe next function called prepare_input() is from this book’s code, which takes a string of text and the spam_words vector as input. Then, it reads the text and it will count the selected columns we modeled before, such as !, $, (), uppercase letters, the longest sequence of uppercase, and words in the spam list. Finally, returning a data frame for input in the model we create.\nThe next objects will be the emails to check and the spam_words to use:\n\ntext1 &lt;- 'SALE!! SALE!! SALE!! SUPER SALEEEE!! This is one of the best sales of the year! More than #3000# products with discounts up to $500 off!! Visit our page and Save $$$ now! Order your product NOW (here) and get one for free !'\n\ntext2 &lt;- 'DEAR MR. JOHN, You will find enclosed the file we talked about during your meeting earlier today. The attachment received here is also available in our web site at this address: www.DUMMYSITE.com. Sale.'\n\nspam.words &lt;- c('you', 'your', 'will', 'free', 'our', 'all', 'mail', 'email', 'business', 'remove', '000', 'font', 'money', 'internet', 'credit', 'over', 'order', '3d', 'address', 'make', 'people', 're', 'receive', 'sale')\n\nIt is time to predict:\n\ninput &lt;- prepare_input(text1, spam.words)\n\ndata.frame( predict(model.rf, input, type = \"prob\") )\n\n  is_spam no_spam\n1   0.796   0.204\n\n\n\ninput &lt;- prepare_input(text2, spam.words)\n\ndata.frame( predict(model.rf, input, type = \"prob\") )\n\n  is_spam no_spam\n1   0.668   0.332\n\n\nWe can see in this case that spam email (text1) is with 70% probability spam, although text2 which is no spam the model tell us is with 0.51% no spam. Looking the text2 there are some uppercase letters which can fool the model a little bit.\nThe next step is save this model and be ready for deployment. To save the model we can use saveRDS() function and input the model’s name and the name of the output filename. Later with Shiny application we can deploy the model to do it’s job (receiving emails from a user or whatever).\n\nsaveRDS(model.rf, \"../scripts/model_rf_spam.rds\")",
    "crumbs": [
      "Annotations",
      "Data Wrangling and Model",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Modeling to Deploy</span>"
    ]
  },
  {
    "objectID": "to_book/notebooks/14_modeling2deploy.html#useful-links",
    "href": "to_book/notebooks/14_modeling2deploy.html#useful-links",
    "title": "11  Modeling to Deploy",
    "section": "12.7 Useful Links",
    "text": "12.7 Useful Links\n\nMode deep in Supervised and Unsupervised learning, with evaluation measures: https://www.geeksforgeeks.org/supervised-unsupervised-learning/",
    "crumbs": [
      "Annotations",
      "Data Wrangling and Model",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Modeling to Deploy</span>"
    ]
  }
]
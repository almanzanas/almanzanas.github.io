---
title: "Exploratory Data Analysis"
---

#### Libraries {.unnumbered}

```{r message=FALSE, warning=FALSE}
library(tidyverse, quietly = TRUE)
library(skimr, quietly = TRUE)
library(statsr, quietly = TRUE)
library(GGally, quietly = TRUE)
library(corrplot, quietly = TRUE)
```

Dataset: <https://github.com/fivethirtyeight/data/tree/master/college-majors>

| Header                 | Description                                                                |
|--------------------|----------------------------------------------------|
| `Rank`                 | Rank by median earnings                                                    |
| `Major_code`           | Major code, FO1DP in ACS PUMS                                              |
| `Major`                | Major description                                                          |
| `Major_category`       | Category of major from Carnevale et al                                     |
| `Total`                | Total number of people with major                                          |
| `Sample_size`          | Sample size (unweighted) of full-time, year-round ONLY (used for earnings) |
| `Men`                  | Male graduates                                                             |
| `Women`                | Female graduates                                                           |
| `ShareWomen`           | Women as share of total                                                    |
| `Employed`             | Number employed (ESR == 1 or 2)                                            |
| `Full_time`            | Employed 35 hours or more                                                  |
| `Part_time`            | Employed less than 35 hours                                                |
| `Full_time_year_round` | Employed at least 50 weeks (WKW == 1) and at least 35 hours (WKHP \>= 35)  |
| `Unemployed`           | Number unemployed (ESR == 3)                                               |
| `Unemployment_rate`    | Unemployed / (Unemployed + Employed)                                       |
| `Median`               | Median earnings of full-time, year-round workers                           |
| `P25th`                | 25th percentile of earnings                                                |
| `P75th`                | 75th percentile of earnings                                                |
| `College_jobs`         | Number with job requiring a college degree                                 |
| `Non_college_jobs`     | Number with job not requiring a college degree                             |
| `Low_wage_jobs`        | Number in low-wage service jobs                                            |

The intent of this section is to go over a practical project, following the steps: loading the dataset, understanding the data, treating missing values, exploring and visualizing, and the analysis report.

# Loading the dataset

To continue we need the libraries mentioned before:

-   `tidyverse` : eight libraries included, to work with data
-   `skimr` : descriptive statistics
-   `statsr` : statistical tools and data sampling
-   `GGally` : for pair plots
-   `corrplot` : correlation plots

```{r}
# Reading a csv from a web
url <- "https://raw.githubusercontent.com/fivethirtyeight/data/master/college-majors/recent-grads.csv"

df <- read_csv(url)

df.original <- df
```

We'll keep a copy of the original data frame just in case we need to recover some columns or for comparison reasons.

# Understanding the data

Let's check the firsts observations:

```{r}
df %>%
    head()
```

We can see that the data is **rectangular**. At first we do not see any problems with language encoding (strange symbols). And the CSV was read correctly because the columns are fine delimited.

-   `glimpse()`: similar to `str()`, returns the columns, types and some values.

```{r}
glimpse(df)
```

Looking the previous output we can see two character columns (Mayor, Mayor_category), this columns will be factor but also 'Major_code' which is an identification. We can keep all three as character to avoid problems with data handling, or we could work with factors taking care about this.

```{r}
cols.factor <- c("Major", "Major_code", "Major_category")

df <- df %>%
    mutate_at(cols.factor, factor)
```

Before to start with descriptive statistics let's disable scientific notations and establishing 4 decimals.

```{r}
options(scipen=999, digits=4)
```

-   `skim()`: returns a summary table for the given data frame

Lets store the skim table in an object and calculate manually the coefficient of variation for the variables:

```{r}
stats <- skim(df)
stats$numeric.sd / stats$numeric.mean
# The returned NA values are the character/factor variables
```

-   CV close to 0: there is a little variability in the data which is around the mean.
-   CV between 0 and 1: moderate variability. Closer to 1 the greater dispersion.
-   CV close to 2: high variability suggesting highly dispersion. The mean may not be a reliable representation.

```{r}
# Descriptive statistics

skim(df)
```

We can see the is a missing value in four variables, we need to know if it is the same observation. Also, there are more women than men and more people employed than unemployed. The jobs requiring a college degree are balanced with the jobs which no require a degree. Looking the Coefficient of Variation, almost every variable have a high value, greater than 1, which suggest high dispersion; and a very few variables have 0.3 or less which suggest the data is around the mean on this variables.

# Treating missing data

The missing values also have meaning, could be a human mistake or an omitted response of the user.

```{r}
df[which(is.na(df$Total)),]
```

Since there is only one observation we can drop it because represent less than 1%. It is recommended to use other treatment if missing values are greater than 5% of the observations.

```{r}
df.clean <- df %>% drop_na()

dim(df.clean)
```

# Exploring and visualizing the data

Here we are going to create some questions to lead the exploration.

## Univariate Analysis

It is to look one variable at a time. Let's graph some histograms of the numeric variables with a *for* loop.

```{r}
for (var in colnames(select_if(df.clean, is.numeric)) ) {
    g = ggplot(df.clean) + 
        geom_histogram( aes( unlist(df.clean[, var]) ), bins=20,
                        fill = "darkblue", color = "lightblue", alpha = 0.5) +
        labs( title = paste("Histogram of", var),
              x = var)
    plot(g)
}
```

And to be able to see outliers the boxplot will be our aproach.

```{r}
for ( var in colnames( select_if(df.clean, is.numeric) ) ) {
    g = ggplot(df.clean) +
        geom_boxplot( aes( y = unlist(df.clean[, var]) ),
                      fill = "lightblue", color = "blue") +
        labs(title = paste("Boxplot of", var),
             y = var)
    plot(g)
}

```

We can add a QQ plot as a visual indication of normality.

```{r}
for (var in colnames( select_if(df.clean, is.numeric)) ) {
    g = ggplot(df.clean, aes( sample = unlist( df.clean[, var])) ) +
        stat_qq() +
        stat_qq_line() +
        labs( title = paste("QQ-plot of", var) )
    plot(g)
}
```

At this point we can see that the distributions are right skewed with many outliers. To finish, we can use statistic tests to be sure if a variable distribution is normal.

```{r}
norm.stats <- data.frame()
for (var in colnames( select_if( df.clean, is.numeric) ) ) {
    ks <- ks.test( unlist( df.clean[, var] ), y = pnorm) 
    sh <- shapiro.test( unlist( df.clean[, var]) )
    vals <- data.frame(
        ks_test = sprintf("%6.5f" ,c(ks$p.value)),
        shapiro_test = sprintf("%6.5f", c(sh$p.value) ),
        row.names = var
    )
    norm.stats <- bind_rows(norm.stats, vals)
}
remove(ks)
remove(sh)
remove(vals)
norm.stats
```

## Multivariate Analysis

> In a project, the idea is to explore how the explanatory variables (X) affect the response variable (y).

Here we want to know how the variables affect to 'unemployment_rate'. For this task could be useful the scatter-plot to see the pattern created when two variables are compared. And the correlation coefficient which tell us how much relation they have.

-   `ggpairs()`: from `GGally` returns a matrix with scatterplot and correlations.

```{r}
df.clean %>%
    select(Men, Women, Part_time, Unemployment_rate, 
           Non_college_jobs, Low_wage_jobs) %>%
    ggpairs() 

```

There are only select some variables, it have been discarded the character ones and cherry-pick variables with some correlation. The full correlation matrix could not be seen.

```{r}
corrs <- round( cor(df.clean[, -c(1, 2, 3, 7)]), 3)
corrplot (corrs, method = "number", type = "lower", 
          tl.cex = 0.8, number.cex = 0.6)
```

Looking into 'Unemployment_rate' the correlations are so lightly, in this case a linear regression would not be the best approach.

When two explanatory variables have high correlations means both variables can explain the same variance in the target variable, creating redundancy and making it more difficult to determine the effect of each individual explanatory feature. Then, it must be eliminated before modeling.

## Exploring

When we want to explore more deeply the dataset we can start doing questions and looking for answers:

-   What are the top 10 majors with the lower unemployment rate?
-   And with the higher unemployment rate?
-   What are the majors with more specialized jobs (requiring college)?
-   What are the best median-value-paying jobs?
-   Do the majors with more share of women enrolled have a higher or lower unemployed rate?
-   Do the majors with more share of women have higher rate of part time workers?
-   Do the majors with a greater share of women enrolled have a similar salary median?

#### What are the top 10 majors with the lower unemployment rate? {.unnumbered}

```{r}
# Selecting top 10
top10.hig.emp <- df.clean %>%
  select(Major, Unemployment_rate) %>%
  arrange( desc(Unemployment_rate) ) %>%
  head(10)

# Selecting top 10
top10.low.emp <- df.clean %>%
  select(Major, Unemployment_rate) %>%
  arrange( Unemployment_rate ) %>%
  head(10)
top10.low.emp
```

```{r}
# plot top 10 higher Unemployment rate
ggplot(top10.hig.emp) +
  geom_col( aes(x = Unemployment_rate,
                y = reorder( Major, +Unemployment_rate) ),
            color = "darkblue", fill = "lightblue" ) +
  labs( y = "Major" ) +
  geom_text( aes( x = Unemployment_rate,
                  y = Major, label = round(Unemployment_rate, 3) ),
             size = 2, hjust = 1.2) +
  ggtitle("Unemployment rate by Major")
```

```{r}
# plot top 10 lower Unemployment rate
ggplot(top10.low.emp) +
  geom_col( aes(x = Unemployment_rate,
                y = reorder( Major, -Unemployment_rate) ),
            color = "darkblue", fill = "lightblue" ) +
  labs( y = "Major" ) +
  geom_text( aes( x = Unemployment_rate,
                  y = Major, label = round(Unemployment_rate, 3) ),
             size = 2, hjust = 1.2) +
  ggtitle("Unemployment rate by Major")
```

The first graph of the top 10 unemployment rate looks fine but the second one not at all. Let's try the same but using the proportion of total employees for ordering the data within unemployment rate.

```{r}
top10.hig.prop <- df.clean %>%
  mutate (prop = Total / sum(Total) ) %>%
  select (Major, Unemployment_rate, prop) %>%
  arrange(desc(prop), desc(Unemployment_rate)) %>%
  head(10)

# plot top 10 higher Unemployment rate by proportion
ggplot(top10.hig.prop) +
  geom_col( aes(x = Unemployment_rate,
                y = reorder( Major, +Unemployment_rate) ),
            color = "darkblue", fill = "lightblue" ) +
  labs( y = "Major" ) +
  geom_text( aes( x = Unemployment_rate,
                  y = Major, label = round(Unemployment_rate, 3) ),
             size = 2, hjust = 1.2) +
  ggtitle("Unemployment rate by Major")
```

```{r}
top10.low.prop <- df.clean %>%
  mutate (prop = Total / sum(Total) ) %>%
  select (Major, Unemployment_rate, prop) %>%
  arrange(desc(prop), Unemployment_rate) %>%
  head(10)

# plot top 10 lower Unemployment rate by proportion
ggplot(top10.low.prop) +
  geom_col( aes(x = Unemployment_rate,
                y = reorder( Major, -Unemployment_rate) ),
            color = "darkblue", fill = "lightblue" ) +
  labs( y = "Major" ) +
  geom_text( aes( x = Unemployment_rate,
                  y = Major, label = round(Unemployment_rate, 3) ),
             size = 2, hjust = 1.2) +
  ggtitle("Unemployment rate by Major")
```

It would not be necessary the second visualization, It have the same values because it is ordered by proportion. It show the most popular majors ordered by employment rate, and nursing with the best rate.

#### More specialized jobs {.unnumbered}

Just like the last time, we are going to need new columns such as proportions of college jobs:

```{r}
df.clean <- df.clean %>%
  mutate ( College_jobs_pct = College_jobs / (College_jobs + Non_college_jobs),
           Employees_pct = Total / sum(Total) )
```

```{r}
df.clean %>%
  select(Major_category, Major, College_jobs_pct) %>%
  group_by(Major_category) %>%
  summarize(mean_college_jobs_pct = mean(College_jobs_pct)) %>%
  arrange(desc(mean_college_jobs_pct))
```

#### Best median-value-paying jobs {.unnumbered}

First we have to add a variable for median salaries grouped by major category, and then use that to generate a boxplot visualization for comparison.

```{r}
df.clean <- df.clean %>%
  group_by(Major_category) %>%
  mutate( Salary_mdn = median(Median) )

ggplot(data = df.clean) +
  geom_boxplot( aes( x = reorder(Major_category, Salary_mdn),
                     y = Median),
                fill = "lightblue", color = "darkblue") +
  labs( x = "Major_category" ) +
  ggtitle ("Median by Major Category") +
  theme( axis.text.x = element_text( angle = 45, hjust = 1.2 ) ) +
  expand_limits( x = c(0, NA), y = c(0, NA) ) +
  scale_y_continuous( labels = scales::comma )
```

We can observe that Engineering, Maths, Business, Physics are the top paying jobs.

Other approaching is to do multiple comparison (post-hoc) of the median.

```{r}
pairwise.wilcox.test(df.clean$Median, df.clean$Major_category, p.adjust.method = "holm") %>% suppressWarnings()
```

On the last output we look for p \< 0.05 meaning there are differences between the medians of that groups. With the boxplot we can see which are greater visually but with these comparisons we can actually know which is greater than another statistically.

#### Relatioship between Majors with more share of women and unemployed rate {.unnumbered}

This task should be do it with correlations, and then we can plot to see its graphic representations.

```{r}
#cor(df.clean$ShareWomen, df.clean$Unemployment_rate, method = "pearson")

cor(df.clean$ShareWomen, df.clean$Unemployment_rate,
    method = "spearman")
```

The spearman correlation is non-parametric and it is suitable for when the relationship of two variables are non-linear, or do not follow the norm distribution.

```{r}
ggplot( data = df.clean ) +
  geom_point( aes( x = ShareWomen, y = Unemployment_rate ),
              color = "darkblue") +
  ggtitle ("Share of women vs. Unemployment rate") +
  labs ( subtitle = "There is no linear relationship, the graphic is spread", color = "darkgray", size = 8) +
  theme( plot.subtitle = element_text( color = "darkgray", 
                                               size = 10) )
```

#### Do the majors with more share of women have higher rate of part time workers? {.unnumbered}

Just like the last question, we can see the relationship of this two variables with a correlation.

```{r}
cor(df.clean$ShareWomen, df.clean$Part_time, method = "spearman")
```

It can be seen a medium-slight positive correlation between both Share of woman enrolled and workers with part time contract. This is how looks like the correlation:

```{r}
ggplot(data = df.clean) +
    geom_point( aes( x = ShareWomen, y = Part_time),
                color = "darkblue") +
    labs( title = "Share of Women vs. Part-Time jobs",
          subtitle = "The relationship between the variables is slightly weak [0.33],\n siggesting that when the share of women is higher also there are more part-jobs") +
    theme( plot.subtitle = element_text( color = "darkgray", size = 8) )
```

#### Majors with a greater share of women and salary median {.unnumbered}

```{r}
cor(df.clean$ShareWomen, df.clean$Median)
cor(df.clean$ShareWomen, df.clean$Low_wage_jobs)
```

The first correlation show when the greater is the share of women enrolled, the less are the Median salary; and in the second correlation, almost without strength if the share of women is high there are a slight relationship (almost none) with more low wage jobs.

```{r}
ggplot(data = df.clean) +
    geom_point( aes( x = ShareWomen, y = Low_wage_jobs, 
                     color = "Low Wage Jobs"),
                alpha = 1, shape = "o", size = 2) +
    geom_point( aes( x = ShareWomen, y = Median, 
                     color = "Median Salary"), alpha = 0.5) +
    labs(title = "Share of women vs. low wage, and vs. Median Salary",
         y = "", color = "Share of Women vs.:") +
    scale_color_manual(values = c("Low Wage Jobs" = "darkblue", 
                                  "Median Salary" = "darkred") ) +
    theme(legend.title = element_text())

```

The next step could be to test if the median of lower share of women is greater or not than higher share of women. To verify this we can run hypothesis tests. First we are going to add a column with two categories, one for higher share and other for lower share of women. Then we have to be sure which test we can use, for that we have to check the normality and if its fine, the homoscedasticity.

```{r}
df.clean <- df.clean %>% 
    mutate( ShareWomen_cat = ifelse(ShareWomen > 0.5, "higher", "lower") )

df.higher.w <- df.clean %>%
    filter( ShareWomen_cat == "higher") %>%
    select( Major, Major_category, ShareWomen, 
            ShareWomen_cat, Low_wage_jobs, Median)

df.lower.w <- df.clean %>%
    filter( ShareWomen_cat == "lower") %>%
    select( Major, Major_category, ShareWomen, 
            ShareWomen_cat, Low_wage_jobs, Median)
```

```{r}
ks.test( df.higher.w$Low_wage_jobs, "pnorm") %>% suppressWarnings()
ks.test( df.lower.w$Low_wage_jobs, "pnorm" ) %>% suppressWarnings()
ks.test( df.higher.w$Median, "pnorm" ) %>% suppressWarnings()
ks.test( df.lower.w$Median, "pnorm" ) %>% suppressWarnings()
```

In the las output we show that every test was lower than 0.05 thus there are no normality on these groups. Let's check homoscedasticity:

```{r}
with(df.clean, car::leveneTest(Median, ShareWomen_cat) )  %>% suppressWarnings()
with(df.clean, car::leveneTest(Low_wage_jobs, ShareWomen_cat) )  %>% suppressWarnings()
```

The first result for Median on ShareWomen_cat levels have p \< 0.05 then we reject the Null Hypothesis; nevertheless, the second result for Low_wage_jobs on ShareWomen_cat levels the p is 0.17, then we maintain the Null Hypothesis of Homogeneity of Variances.

Now we know that for compare Median we will need a robust test, but with Low_wage_jobs we might use t-test because the sample is greater than 50 and that will not affect so much the results.

```{r}
wilcox.test(df.higher.w$Median, df.lower.w$Median)
cat("Median of Higher Share Woman: ", median(df.higher.w$Median),
    "\nMedian of Lower Share Woman: ", median(df.lower.w$Median), "\n" )
t.test(df.higher.w$Low_wage_jobs, df.lower.w$Low_wage_jobs)
```

For Median there are differences between the median of the groups, but for Low Wage seem like there are not differences between the mean of the groups with this sample.

Let's try another approach. Now using rep_sample_n() we are going to take 100 samples of 1000 observations allowing repetitions, and then taking the mean.

```{r}
df.higher.samp <- df.higher.w %>%
    rep_sample_n( size = 1000, reps = 200, replace = TRUE ) %>%
    summarize( mu_Median = mean(Median), 
               mu_Low_wage = mean(Low_wage_jobs) )

df.lower.samp <- df.lower.w %>%
    rep_sample_n( size = 1000, reps = 200, replace = TRUE ) %>%
    summarize( mu_Median = mean(Median), 
               mu_Low_wage = mean(Low_wage_jobs) )
```

```{r}
shapiro.test(df.lower.samp$mu_Median)
shapiro.test(df.higher.samp$mu_Median)
shapiro.test(df.lower.samp$mu_Low_wage)
shapiro.test(df.higher.samp$mu_Low_wage)
```

```{r}
t.test(df.higher.samp$mu_Median, df.lower.samp$mu_Median)
t.test(df.higher.samp$mu_Low_wage, df.lower.samp$mu_Low_wage)
```

Both p-value are close to zero, thus we can reject the null hypothesis of t-test. Then we can say the groups mean are different with this sample.

> We can infer with 95% confidence that the majors with 50% or more of those enrolled being women have, on average, more low-wage jobs and less median salary.

```{r}
df.salary.w <- df.clean %>%
  group_by(ShareWomen_cat) %>%
  summarize(Salary_mean = mean(Median) )
```

```{r}
ggplot( data = df.clean ) +
  geom_boxplot( aes( x = ShareWomen_cat, y = Median), 
                fill = "lightblue", color = "darkblue" ) +
  labs( title = "Average Salary When 
        ShareWomen is lower/higher than 50%",
        subtitle = "The average salary for majors with more women enrolled \n
        is lower than the majors with less women, reinforcing the perception \n 
        that women are getting lower salaries.",
        x = "50% Share of Women",
        y = "Mean Salary" ) +
  geom_text( data = df.salary.w, aes( x = ShareWomen_cat,
                                      y = Salary_mean,
                                      label = round(Salary_mean)),
             size = 2.2, vjust = 1, color = "black" ) +
  theme(plot.subtitle = element_text( color = "darkgray", size = 9 ) )
  
```

To close this Exploratory Data Analysis we just need to compose the Analysis report.

# Analysis Report

This Data Science project is about the EDA, composed of data wrangling and exploration to answer business questions and extract insights to support the decision-making process. When we write a Analysis Report we have to highlight the findings:

------------------------------------------------------------------------

Dataset: College Majors (<https://github.com/fivethirtyeight/data/tree/master/college-majors>)

Dimensions: 173 observations, 21 variables

Data dictionary:

| Header                 | Type        | Description                                                                |
|-----------------|----------------|----------------------------------------|
| `Rank`                 | Ordinal     | Rank by median earnings                                                    |
| `Major_code`           | Categorical | Major code, FO1DP in ACS PUMS                                              |
| `Major`                | Categorical | Major description                                                          |
| `Major_category`       | Categorical | Category of major from Carnevale et al                                     |
| `Total`                | Numeric     | Total number of people with major                                          |
| `Sample_size`          | Numeric     | Sample size (unweighted) of full-time, year-round ONLY (used for earnings) |
| `Men`                  | Numeric     | Male graduates                                                             |
| `Women`                | Numeric     | Female graduates                                                           |
| `ShareWomen`           | Numeric     | Women as share of total                                                    |
| `Employed`             | Numeric     | Number employed (ESR == 1 or 2)                                            |
| `Full_time`            | Numeric     | Employed 35 hours or more                                                  |
| `Part_time`            | Numeric     | Employed less than 35 hours                                                |
| `Full_time_year_round` | Numeric     | Employed at least 50 weeks (WKW == 1) and at least 35 hours (WKHP \>= 35)  |
| `Unemployed`           | Numeric     | Number unemployed (ESR == 3)                                               |
| `Unemployment_rate`    | Numeric     | Unemployed / (Unemployed + Employed)                                       |
| `Median`               | Numeric     | Median earnings of full-time, year-round workers                           |
| `P25th`                | Numeric     | 25th percentile of earnings                                                |
| `P75th`                | Numeric     | 75th percentile of earnings                                                |
| `College_jobs`         | Numeric     | Number with job requiring a college degree                                 |
| `Non_college_jobs`     | Numeric     | Number with job not requiring a college degree                             |
| `Low_wage_jobs`        | Numeric     | Number in low-wage service jobs                                            |

Missing Data: There is one observation with missing data on Men, Women, ShareWomen and Total variables. The Major with this missing observation is FOOD SCIENCE (code: 1104).

## Report {.unnumbered}

EDA of a complete dataset, the missing values are less than 1% which was excluded to preserve the original data only without adding calculated numbers.

The descriptive statistics show high variances due to the presence of outliers. The women in college represents the 52%. The unemployment rate is around 6% presenting high variance.

Are jobs more specialized which require a degree such as Education, Maths, Engineering. The engineering is the top-paying category followed by Computer and Maths, Physical Science, and Business. Nevertheless, there are no statistical differences between categories on salary median except comparing Engineering which is statistically higher.

Comparing gender it can be shown that majors with more women enrolled have more low wage jobs and also majors with more share of women seems to have lower salaries. Moreover, the unemployment rate are lower in majors with more share of women.

## Next steps {.unnumbered}

If further analysis are required which needs modeling, for linear regression attention has to be paid to multicollinear variables which can affect its reliability.

---

The last text was an example of Analysis report, it could be filled with more content if its necessary. But this is the idea.
















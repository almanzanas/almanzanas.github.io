{
  "hash": "517352f77fcae6615f17738e776f9e56",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Data into R\"\n---\n\n\n\n# Getting Data into and out of R\n\nWhen we gather data for analysis, we have to track it's history meaning where was acquired, from what source and on what date; this is known as *data's provenance*. A place to keep this information can be in the scripts.\n\n## Tabular Data\n\nOne of the most common data we use come in the form of rectangular or tabular data, which is observations in rows and measurements (variables) in columns. Lent's read tabular data files (ASCII text) into R data frames and to try some approaches when is not working.\n\n### Files with Delimiters\n\nCommonly a observation is represented by a single row which fields are separated by a delimiter such a comma, tab, semicolon, pipe, or exclamation point; and the end of each observation is marked with a end-of-line character which can vary across platforms.\n\n-   `read.table(), read.csv(), read.delim()` : returns a data frame. Each function differs on it's default settings. Arguments for read.table():\n    -   `header=` if `TRUE` will use the first row of the file as column headers.\n    -   `sep=` the separator character. The default value is `\" \"`.\n    -   `quote=` character to be used to surround quotes. By default is set for `\"` and `'` .\n    -   `comment.char=` character which introduce a comment line\n    -   `stringAsFactors=` if `FALSE` will left the character columns as character and `TRUE` for Factor class.\n    -   `colClasses=` expect a vector with the class of each column.\n    -   `na.strings=` expect a vector with the indicators of missing value in the input data.\n-   `read.csv2()` : for European style comma. Use semi-colon as delimiter.\n-   `read.delim2()` : for European style comma.\n\nUsually, the quote argument is set to `\"\\\"\"` to recognize the double quotation marks because `'` can be used as apostrophe or turned off with `\"\"`. Also the comment argument is turned off with `\"\"` because it is not usual found `#` in data files and sometimes in fields there are '#1' or similar characters.\n\n### Column Classes\n\nWe almost always pass `stringAsFactor=FALSE` when we are reading data, maybe except when we know the data is numeric and pre-cleaned and using `colClasses`. \n\nThe `colClasses` argument allows to specify the column type for each for the columns. To get an idea what are in the columns we can pass `nrow=` argument to read a dozen of rows with read.table() and inspect the resulting data frame. It can be specified numeric, character, logical, Date, POSIXct, and also can convert classes as well (see help(read.table)). `colClasses` recycle it's elements, to start you can pass \"character\" and watch what is inside.\n\n### Common Problems\n\n#### Embedded Delimiters {.unnumbered}\n\nProblems can arise with simple quote mark, # characters and also a comma as part of some text. This kind of difficulties come often from spreadsheets. If the text with comma is between double quotation marks we can use `quote=\"\\\"\"`.\n\n#### Unknown Missing Value Indicator {.unnumbered}\n\nBy default R expects missing values as NA. A spreadsheet from Excel can have `#NULL!, #N/A, #VALUE!`, this three could be included in `na.strings=` argument.\n\n#### Empy or Nearly Empy Filds {.unnumbered}\n\nEmpty fields will be NA values in numeric fields. But in character fields are difficult to perceive. \n\nOne of the firsts tasks is to extract the column classes and compare that with what we see:\n\n```r \ntable ( sapply(df, function(x) class(x)) )\n```\n\nIf we know that a column should be numeric (NumID) but it is not, we can tabulate the elements that R is unable to convert:\n\n```r\ntable ( df$NumID[is.na(as.numeric(df$NumID))] )\n```\n\nExamining the set of missing value indicators in the data can be helpful to include that values to `na.stings` argument in another call to `read.table()`.\n\n#### Blank Lines {.unnumbered}\n\nWhen there are blank rows by default `read.table()` will skip it. If wee need that the lines from two different files correspond we can pass `skip.blank.lines=FALSE` argument which will set NA in numeric columns and `\"\"` in character ones.\n\n#### Row Names {.unnumbered}\n\nBy default `read.table()` will create a row names starting from 1 and up, unless the header has one fewer filed than the rows, in which case R uses the first column as row names. Row names must be unique, if in the data set the first column of row names are not unique we can pass `row.names=NULL` to create an integer type row names. To specify a column to be the row names (or a vector) we can use `row.names=` argument.\n\n### When `read.table()` go wrong\n\nIf we check the file 'data/addresses.csv' we can see is a comma delimited file with headers, then we can specified some arguments:\n\n```r\nread.table (\"../data/addresses.csv\", header = TRUE, sep = \",\", quote = \"\",\n            comment.char = \"\", stringsAsFactors = FALSE)\n```\n\nWe can check the first line to be sure that are headers:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nread.table (\"../data/addresses.csv\", header = F, sep = \",\", quote = \"\",\n            comment.char = \"\", stringsAsFactors = FALSE, nrows = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  V1       V2      V3   V4    V5\n1 ID LastName Address City State\n```\n\n\n:::\n:::\n\n\n\n-   `count.fields()` : returns the fields per row.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncount.fields (\"../data/addresses.csv\", sep = \",\", quote = \"\", comment.char = \"\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5 5 6 5 6 5\n```\n\n\n:::\n:::\n\n\n\nOK, something is wrong in the third and fifth row, let's see what it is:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nread.table (\"../data/addresses.csv\", header = F, sep = \",\", quote = \"\",\n            comment.char = \"\", stringsAsFactors = FALSE, \n            nrows = 1, skip = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  V1     V2           V3       V4       V5 V6\n1 11 Macina 401 1st Ave.  Apt 13G New York NY\n```\n\n\n:::\n:::\n\n\n\nNow we could conclude that in this 3rd row there are embedded commas in V3 corresponding to Address column.\n\nWe can force when creating the data frame using the largest number of columns possible by passing `fill=TRUE` argument. The rows with the last column filled tell us that there are problems in the input data:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(\nadd <- read.table (\"../data/addresses.csv\", header = T, sep = \",\", quote = \"\",\n                   comment.char = \"\", stringsAsFactors = FALSE, fill = T)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           ID          LastName       Address     City State\n001 O'Higgins      48 Grant Rd.    Des Moines       IA      \n011    Macina      401 1st Ave.       Apt 13G New York    NY\n242    Roeder    71 Quebec Ave.   E. Thetford       VT      \n146  Stephens   1234 Smythe St.            #5  Detroit    MI\n241  Ishikawa 986 OceanView Dr. Pacific Grove       CA      \n```\n\n\n:::\n:::\n\n\n\nThe IDs have become row names because there are a less field in headers. In the case the ID was duplicated we have to pass `row.names=NULL` argument. \n\nTo continue, we are going to identify the bad formatted rows, then we're going to `paste()` the columns 2 and 3 from that rows which had embedded comma, and to finish changing the positions of columns 4-5 to 3-4.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Vector with logical values:\nfixc <- add$State != \"\"\n# Paste column 2 and 3 from that rows\nadd[fixc, 2] <- paste (add[fixc, 2], add[fixc, 3])\n# Changing the order\nadd[fixc, 3:4] <- add[fixc, 4:5]\nadd\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           ID              LastName       Address City State\n001 O'Higgins          48 Grant Rd.    Des Moines   IA      \n011    Macina 401 1st Ave.  Apt 13G      New York   NY    NY\n242    Roeder        71 Quebec Ave.   E. Thetford   VT      \n146  Stephens   1234 Smythe St.  #5       Detroit   MI    MI\n241  Ishikawa     986 OceanView Dr. Pacific Grove   CA      \n```\n\n\n:::\n:::\n\n\n\nOur last step will be to delete the last column after saving the column names. And remove the variables that we won't need any more.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Saving column names and deleting State column\nmycols <- colnames (add)\nadd$State <- NULL\n# ID column will take the values of the previus rownames, then assing colnames\nadd <- data.frame (ID = rownames (add), add)\ncolnames (add) <- mycols\n# Replace old rownames:\nrownames (add) <- NULL\n# Removing objects:\nrm (fixc, mycols)\nadd\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   ID  LastName               Address          City State\n1 001 O'Higgins          48 Grant Rd.    Des Moines    IA\n2 011    Macina 401 1st Ave.  Apt 13G      New York    NY\n3 242    Roeder        71 Quebec Ave.   E. Thetford    VT\n4 146  Stephens   1234 Smythe St.  #5       Detroit    MI\n5 241  Ishikawa     986 OceanView Dr. Pacific Grove    CA\n```\n\n\n:::\n:::\n\n\n\n#### Using Scan {.unnumbered}\n\nGiven the previous data set, let's explore the content of that file with `scan()`, a general-purpose data input tool.\n\n-   `sep=\"\\\"` : to read entire lines\n-   `what=character()` or `what=\"\"` : by default scan() expect numbers, so we specify that it will encounter characters.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(\nadd.scan <- scan (\"../data/addresses.csv\", what = character(), sep = \"\\n\",\n                  quote = \"\", comment.char = \"\")\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"ID,LastName,Address,City,State\"                 \n[2] \"001,O'Higgins,48 Grant Rd.,Des Moines,IA\"       \n[3] \"011,Macina,401 1st Ave., Apt 13G,New York,NY\"   \n[4] \"242,Roeder,71 Quebec Ave.,E. Thetford,VT\"       \n[5] \"146,Stephens,1234 Smythe St., #5,Detroit,MI\"    \n[6] \"241,Ishikawa,986 OceanView Dr.,Pacific Grove,CA\"\n```\n\n\n:::\n:::\n\n\n\nWe can fix it replacing the third comma:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# locating all the commas:\ncommas <- gregexpr (\",\", add.scan)\n# Extracting long rows:\ncomma.5 <- lengths (commas) == 5\n# Locating the third comma:\ncomma.gone <- sapply (commas[comma.5], function(x) x[3])\n# Replacing comma for semi-colon:\nsubstring (add.scan[comma.5], comma.gone, comma.gone) <- \";\"\nadd.scan\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"ID,LastName,Address,City,State\"                 \n[2] \"001,O'Higgins,48 Grant Rd.,Des Moines,IA\"       \n[3] \"011,Macina,401 1st Ave.; Apt 13G,New York,NY\"   \n[4] \"242,Roeder,71 Quebec Ave.,E. Thetford,VT\"       \n[5] \"146,Stephens,1234 Smythe St.; #5,Detroit,MI\"    \n[6] \"241,Ishikawa,986 OceanView Dr.,Pacific Grove,CA\"\n```\n\n\n:::\n:::\n\n\n\nThe next step will be `read.table()` from text, also we can save our progress with `write.table()` in case we will use it again o for other users.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nread.table (text = add.scan, header = TRUE, sep = \",\", quote = \"\",\n            comment = \"\", stringsAsFactors = FALSE,\n            colClasses = c(ID = \"character\") )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   ID  LastName               Address          City State\n1 001 O'Higgins          48 Grant Rd.    Des Moines    IA\n2 011    Macina 401 1st Ave.; Apt 13G      New York    NY\n3 242    Roeder        71 Quebec Ave.   E. Thetford    VT\n4 146  Stephens   1234 Smythe St.; #5       Detroit    MI\n5 241  Ishikawa     986 OceanView Dr. Pacific Grove    CA\n```\n\n\n:::\n:::\n\n\n\n### Writing Delimited Files\n\nTo perform this task we use `write.table(), write.csv(), write.csv2()` and it's analogs. Also with `write.table()` can be passed a matrix. Saving a file with `write.table()` generally it be passed `sep=` argument specifying the delimiter, `quote=FALSE` to not save the quotes in character fields but sometimes it is necessary for numbers with leading zeros or embedded commas. Rarely we want to save the row names, then with row.names=FALSE we omit them.\n\n#### Fixed width Files {.unnumbered}\n\nA fixed-width file have a specific number of character for each field, for example, ID have 4 characters, name 15, account 12... For this files we use `read.fwf()` that has many of the same arguments as `read.table()`, and the most important is `widths=` which expect an integer vector with the lengths of the fields.\n\n### End-of-Line Characters:\n\nWindows has `\\r` and `\\n`. OS X and Linux only `\\n`. Depending the platform and application we should be aware, but R is flexible with `read.table()` and `scan()` functions which permits some flexibility.\n\n## Non-Tabular Data\n\nSometimes files are too big to fit into the main memory, then we will need some techniques to work with that files. If we need a subset of records, we can filter the data set without reading it all into memory. On the other hand, there are files not suitable for `read.table()` like JSON and XML, or binary data.\n\n-   `file()` and relatives : return a connection object that stores all the information that R needs. \n    -   `open=` whether the file is to be read, written or append to (\"r\", \"w\", \"a\"). It can be passed with '+' to read and write, and adding t or b for text or binary modes. `open=\"a+b\"` opens a binary file for reading and appending.\n-   `close()` : to close a connection. Is a good practice to close the connection when we are finish with it.\n\nThe function `readLines()` opens the file, reads as many lines we want and closes the file. The argument `n=` mean number of lines and `n=-1` for all lines. We can use `readLines()` over a connection to consecutively read lines from that connected file because will remain open. The analogous function is `writeLines()` and `writeChar()` which adds a null character after it's end-of-line character but using `eos=NULL` argument the string is written without terminator.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreadLines (\"../data/addresses.csv\", n = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"ID,LastName,Address,City,State\"\n```\n\n\n:::\n\n```{.r .cell-code}\ncon.add <- file (\"../data/addresses.csv\", open = \"r\")\nreadLines (con.add, n = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"ID,LastName,Address,City,State\"          \n[2] \"001,O'Higgins,48 Grant Rd.,Des Moines,IA\"\n```\n\n\n:::\n\n```{.r .cell-code}\nreadLines (con.add, n = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"011,Macina,401 1st Ave., Apt 13G,New York,NY\"\n[2] \"242,Roeder,71 Quebec Ave.,E. Thetford,VT\"    \n```\n\n\n:::\n\n```{.r .cell-code}\nclose (con.add)\n```\n:::\n\n\n\nWhen a file is open R maintains a 'pointer' that describes the location, one for read and one for write. With `seek()` function we can get the current location of the file, and passing where= argument we can choose a position which is useful to jump to a prespecified position. But help tell us that `seek()` on windows is discouraged. Finally, `flush()` function can be used after a file output to ensure the write on disk operation.\n\n### Different encodings\n\nFor most of the previous functions (read.table or scan) we can pass `fileEncoding=` argument to specify the encoding in the file we are going to work with, and `encoding=` argument specifies the encoding of the R object that contains the data.\n\nTo write on file we have to be certain what encoding have and pass to `file()` the `encoding=` argument and read using `readLines()` and again `enconding=\"UTF-8\"`. In the same way to write we use file to open a connection selecting the encoding and passing `useBytes=TRUE` argument which prevent R to convert encoded strings back to the native encoding before writing.\n\n### Null Character and Binary Data\n\nIn hexadecimal are represented as `00` or `0x00` by R, that's not R's NULL value. By default with `scan()` and `read.table()` will stop reading a NULL character and it will continue with the next field. The argument `skipNul=TRUE` allows to skip over NULL which is a safe choice for delimited data. For intended NULL in text we have to read the file as binary.\n\n-   `readBin()` : to read binary data. Requires the number of bytes to read because it won't recognize the end-of-line characters. It will return a vector with class `raw`.\n\nOnce read the file, we can write it back with `writeBin()` or convert it into data.\n\n-   `rawToChar()` : when we know that the raw data represents text, unless there are embedded nulls.\n\nWith a raw vector we can look for NULLs and replace it with space or other character that we want:\n\n```r\nvec[vec == 0x00] <- as.raw(0x20)\n```\n\nIf all the previous steps has gone well, after write the data back or converted to text we can use `read.fwf()` or `readLines()`.\n\n## Reading data from Relational Databases\n\nIt is not necessary to load all the data into R, if the data tables are small, it doesn't matter but with large amount of data it is better using the database as much as possible. We will need the package `RODBC` to be installed into R.\n\nTo use `odbcConnect()` we need to have installed ODBC drivers from Microsoft, and then open de application and configure a new DSN which will be the first argument. This DSN contain the database, language and additional configurations. It will create a handle to be used with other commands. The alternative is `odbcDiverConnect()` which provide us more flexibility; the first argument is a connection string with ODBC configuration.\n\n```r\nconnection <- odbcConnect(\"SQL2022\", uid=\"rstats\", pwd=\"P@ssw0rd\")\nadv.tab <- sqlTables(connection, tableType = \"TABLE\")\nadv.tab[4:8,]\n```\n```r\nsqlColumns(connection, \"Employee\")[3:10,4:ncol(sqlColumns(connection, \"Employee\"))]\n```\n\nOnce we are done with the connection (handle) it is recommended to close it with `close(connection)` or `odbcClose(connection)`\n\n### SQL Commands\n\nWe are going to interact with the database using ODBC functions such as `sqlQuery()` passing a character string.\n\n```r\nemp <- sqlQuery (connection, \"\n                 SELECT * FROM HumanResources.Employee\n                 \")\ns10 <- sample(nrow(emp), 10)\nemp[s10,]\n```\n\n```r\nemp.mal <- sqlQuery (connection, \"\n                    SELECT BusinessEntityID, LoginID, VacationHours, SickLeaveHours\n                    FROM HumanResources.Employee\n                    WHERE Gender = 'M'\n                    \", stringsAsFactors = FALSE)\ns10 <- sample(nrow(emp.mal), 10)\nemp.mal[s10,]\n```\n\nThe `sqlQuery()` function supports simple arithmetic operations such as count, max or min, combine columns arithmetically, logarithms, aggregate data into groups...\n\n#### Joining Tables {.unnumbered}\n\nJoin in SQL is like `merge()` function in R. It is to match up two tables according to the value of a column in each one.\n\n```r\nemp.sales <- sqlQuery (connection, \"\n                    SELECT *\n                    FROM Sales.SalesPerson\n                    LEFT JOIN HumanResources.Employee \n                    ON Sales.SalesPerson.BusinessEntityID = HumanResources.Employee.BusinessEntityID\n                    \", stringsAsFactors = FALSE)\n# s10 <- sample(nrow(emp.sales), 10)\nemp.sales\n```\n\n#### Results in Pieces {.unnumbered}\n\nWhen we call to `sqlQuery()` performs two tasks, first it sends the query and then fetch the results. If we are going to work with very large tables we can do this tasks separately. \n\n-   `sqlFetch()` : get the first batch with a given number of rows. Subsequent calls should be made to `sqlFetchMore()` with `max=` argument.\n\nAlso, calling `sqlQuery()` passing `max=` for complicated queries, we can follow it with `sqlGetResults()` and `rows_at_time=` argument with an integer between 1 and 1024 (number of rows to fetch at a time).\n\n```r\nsqlFetch(connection, \"Sales.Customer\", max = 10)\n```\n```r\nsqlFetchMore(connection, max = 10)\n```\n\n#### SQLite {.unnumbered}\n\nSQLite is a 'serverless' database which data is stored in one large file well suited to smaller applications. The `RSQLite` package connects R to SQLite databases. Some useful function are:\n\n-   `dbConnect()` : passing as first argument `SQLite()` and the second `dbname=` argument is the name of the file containing the data. Returns a handle for other functions.\n-   `dbListTables()` : list the tables. The first argument can be the handle.\n-   `dbListFields()` : return the fields (columns) in a table.\n-   `dbGetQuery()` : analogous to `sqlQuery()`, executes a query and returns the data.\n-   `dbSendQuery()` and `dbFetch()` : create a query and receive the data. `dbSendQuery()` does not returns data, it prepares the database to return data with `dbFetch()`.\n\n## Large Numbers of Input Files\n\nAs Data Scientists will have to perform tasks in directories and files which sometimes there are thousands of them. For example, unzipping files and using its content.\n\n-   `list.files()` : list files matching a pattern (regular expression).\n-   `unzip()` : to unzip zipped files.\n-   Other file. family functions such as `file.copy(), file.remove(), file.rename()`...\n\nThe datasets used for this example are from: https://github.com/wesm/pydata-book/tree/3rd-edition/datasets/babynames\n\nTo continue we are going to unzip the babynames files from 1950s to 2000s and load the comma separated data. First we need to know what are their names and the names of the files inside each file.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# listing files corresponding with babynames data\nlist.files(path = \"../\" , \n           pattern = \"^babynames.*\\\\.zip$\",\n           recursive = TRUE, full.names = F)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"data/babynames/babynames-1950.zip\" \"data/babynames/babynames-1960.zip\"\n[3] \"data/babynames/babynames-1970.zip\" \"data/babynames/babynames-1980.zip\"\n[5] \"data/babynames/babynames-1990.zip\" \"data/babynames/babynames-2000.zip\"\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Exploring one .zip file\nunzip(\"../data/babynames/babynames-1950.zip\", list = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Name Length                Date\n1  yob1951.txt 135349 2024-08-27 17:19:00\n2  yob1952.txt 137658 2024-08-27 17:19:00\n3  yob1953.txt 140015 2024-08-27 17:19:00\n4  yob1954.txt 141564 2024-08-27 17:19:00\n5  yob1955.txt 143552 2024-08-27 17:19:00\n6  yob1956.txt 146612 2024-08-27 17:19:00\n7  yob1957.txt 149179 2024-08-27 17:19:00\n8  yob1958.txt 148618 2024-08-27 17:19:00\n9  yob1959.txt 151965 2024-08-27 17:19:00\n10 yob1960.txt 154001 2024-08-27 17:19:00\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Listing .zip files\nzipfiles <- list.files(path = \"../\" , \n                       pattern = \"^babynames.*\\\\.zip$\",\n                       recursive = TRUE, full.names = T)\n\n# Unzip every file\nfor (f in 1:length(zipfiles)) {\n    unzip (zipfiles[f])\n}\n\n# Listing .txt files \ntxtfiles <- list.files(path = \"../\",\n                       pattern = \"^yob.*\\\\.txt$\",\n                       recursive = TRUE, full.names = TRUE)\n# Extracting names and years\ntxtnames <- sub (\"../notebooks/\", \"\", txtfiles)\nyr <- substring (txtnames, 4, 7)\n\n# creating a data frame with every file using year as a column\nresult <- NULL\nfor (i in 1:length (txtfiles) ) {\n    dat <- data.frame (Year = yr[i], read.csv (txtfiles[i], \n                                               header = FALSE, \n                                               stringsAsFactor=FALSE))\n    result <- rbind (result, dat)\n}\n\ns10 <- sample (nrow(result), 10)\nresult[s10,]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        Year        V1 V2  V3\n139941  1963      Aide  F  14\n1070491 2006   Antonyo  M  22\n352045  1977  Enrrique  M   7\n391701  1980     Sally  F 738\n112566  1961     Delia  F 467\n27557   1953   Verdine  F   5\n24846   1953      Cori  F  10\n473485  1984 Charletta  F  18\n837795  1999   Kindall  F  16\n527310  1986    Kartik  M   7\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncolnames(result) <- c(\"Year\", \"Name\", \"Sex\", \"Count\")\n# cleaning exported files\nrem <- file.remove(txtfiles)\nremove(rem)\nresult[s10,]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        Year      Name Sex Count\n139941  1963      Aide   F    14\n1070491 2006   Antonyo   M    22\n352045  1977  Enrrique   M     7\n391701  1980     Sally   F   738\n112566  1961     Delia   F   467\n27557   1953   Verdine   F     5\n24846   1953      Cori   F    10\n473485  1984 Charletta   F    18\n837795  1999   Kindall   F    16\n527310  1986    Kartik   M     7\n```\n\n\n:::\n:::\n\n\n\n## Other Formats\n\nThe **clipboard** can be used for moving text between programs. R sees the clipboard as a file named \"clipboard\" which can be used with `read.table(\"clipboard\")` or `write.table(df, \"clipboard\")` and then paste it into a spreadsheet. For MAC and Linux there are different approaches.\n\nFrom **spreadsheets** like Microsoft Excel with format .xls or .xlsx we can use `read.xls()` or `read.xlsx()` functions from `gdata` package. The spreadsheets have to be rectangular as a data frame must be. \n\nAlso we can acquire a **web** page with `getURI()` function of the `RCurl` package which will return a character vector of length 1. For HTML tables `readHTMLTable()` function from `XML` package will be our usual tool. \n\nWorking with **XML** using functions provided by `XML` package we can use `xmlTreeParse()` for read in a file and returns a tree-like object of class `XMLDocument`. This object acts like an R list where we can use its names to extract fields. The `xmlValue()` function converts the list element into text.\nOther approach is to use `xmlParse()` function and then `xpathSApply()` to search with RE or basic search resulting a list of object of class `xmlNode.`\n\nFor **JSON** can be used `rjson`, `RJSONIO` and `jsonlite` packages which read and write JSON objects. If we have a file containing a whole set of JSON messages, one per line we can use `scan(..., sep=\"\\n\", what=\"\")` and apply a conversion `fromJSON()` to each message.\n\nTo finish, to extract data using a **REST API** can be used `RCurl` and `httr` packages. For other statistical packages such as SAS, SPSS or Minitab the recommended package is `foreign.`\n\n## R Data\n\n-   `save()` : the output is a file with the objects stored in.\n-   `load()` : restore all the objects stored on disk by `save()` (can overwrite objects)\n-   `saveRSD()` : take an object and process a disk file with its data and attributes.\n-   `readRDS()` : returns the object just like was saved.\n-   `save.image()` : to save objects into the workspace, and by default creates a `.RData` file.\n-   `attach()` : to add an R data file to the search path (where R look for objects)\n\n",
    "supporting": [
      "02_data_into_r_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}
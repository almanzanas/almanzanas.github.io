{
  "hash": "ece16b4a5abe836d176e7dd408524693",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Modeling to Deploy\"\n---\n\n\n# Building a Model\n\n#### Dataset {.unnumbered}\n\nHopkins, M., Reeber, E., Forman, G., & Suermondt, J. (1999). Spambase [Dataset]. UCI Machine Learning Repository. <https://doi.org/10.24432/C53G6X> .\n\n#### Libraries {.unnumbered}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(skimr)\nlibrary(randomForest)\nlibrary(caret)\nlibrary(ROCR)\n```\n:::\n\n\n## Machine Learning concepts\n\nModeling data involves finding patterns that can help us explain a response (most probable outcome). One of the most important things is that the input data is clean and representative of the reality we are trying to model.\n\n-   **Classification models**: Used for categorical output. We transform the input data into patterns that will be separated into groups. Thus, each observation will be classified into a group, according to its patterns.\n    -   KNN, decision tree, random forest, logistic regression, and support vector machine.\n-   **Regression models**: Used for numerical output. It will find patterns in number and return a continuous output. As summary, it will captures the relationship between the input variables and calculate an estimate of the output.\n    -   Linear regression, polynomial regression, and regression tree.\n\nAlgorithm types:\n\n-   **Supervised**: algorithm which receives data containing variables that can explain an outcome, the content and the answers to learn. Once it learn the patterns, with new data it will generalize the solution. Can be used classification and regression models.\n-   **Unsupervised**: algorithm which do not receive labeled variable, instead read the dataset looking for patterns that can help explain the data. Clustering models use this algorithm.\n-   **Reinforcement**: algorithm which learn by trial and error. It will perform an actions and check how its going. Positive is rewarded, negative is penalized. It tries to reduce the penalties to the minimum possible. Useful for video games.\n\n## Understanding the project\n\n>When starting a project, we need a purpose which is the goal we want to reach at the end.\n\n### The dataset\n\n\n::: {.cell}\n\n```{.r .cell-code}\nurl.data <- \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\" \n\nurl.names <- \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.names\"\n\nnames <- read_table(url.names, col_names = FALSE) %>% suppressWarnings()\nnames <- names %>% filter(X2 == \"continuous.\") %>% select(X1)\nnames <- names %>% mutate(X1 = gsub(pattern = \"\\\\:\", replacement = \"\", x = X1) )\n\ndf <- read_csv(url.data, \n               col_names = c(names[[\"X1\"]], \"spam_cat\"), \n               trim_ws = TRUE) %>% suppressWarnings()\n\nglimpse(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 4,601\nColumns: 58\n$ word_freq_make             <dbl> 0.00, 0.21, 0.06, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_address          <dbl> 0.64, 0.28, 0.00, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_all              <dbl> 0.64, 0.50, 0.71, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_3d               <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_our              <dbl> 0.32, 0.14, 1.23, 0.63, 0.63, 1.85, 1.92, 1…\n$ word_freq_over             <dbl> 0.00, 0.28, 0.19, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_remove           <dbl> 0.00, 0.21, 0.19, 0.31, 0.31, 0.00, 0.00, 0…\n$ word_freq_internet         <dbl> 0.00, 0.07, 0.12, 0.63, 0.63, 1.85, 0.00, 1…\n$ word_freq_order            <dbl> 0.00, 0.00, 0.64, 0.31, 0.31, 0.00, 0.00, 0…\n$ word_freq_mail             <dbl> 0.00, 0.94, 0.25, 0.63, 0.63, 0.00, 0.64, 0…\n$ word_freq_receive          <dbl> 0.00, 0.21, 0.38, 0.31, 0.31, 0.00, 0.96, 0…\n$ word_freq_will             <dbl> 0.64, 0.79, 0.45, 0.31, 0.31, 0.00, 1.28, 0…\n$ word_freq_people           <dbl> 0.00, 0.65, 0.12, 0.31, 0.31, 0.00, 0.00, 0…\n$ word_freq_report           <dbl> 0.00, 0.21, 0.00, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_addresses        <dbl> 0.00, 0.14, 1.75, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_free             <dbl> 0.32, 0.14, 0.06, 0.31, 0.31, 0.00, 0.96, 0…\n$ word_freq_business         <dbl> 0.00, 0.07, 0.06, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_email            <dbl> 1.29, 0.28, 1.03, 0.00, 0.00, 0.00, 0.32, 0…\n$ word_freq_you              <dbl> 1.93, 3.47, 1.36, 3.18, 3.18, 0.00, 3.85, 0…\n$ word_freq_credit           <dbl> 0.00, 0.00, 0.32, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_your             <dbl> 0.96, 1.59, 0.51, 0.31, 0.31, 0.00, 0.64, 0…\n$ word_freq_font             <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_000              <dbl> 0.00, 0.43, 1.16, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_money            <dbl> 0.00, 0.43, 0.06, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_hp               <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_hpl              <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_george           <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_650              <dbl> 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_lab              <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_labs             <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_telnet           <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_857              <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_data             <dbl> 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_415              <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_85               <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_technology       <dbl> 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_1999             <dbl> 0.00, 0.07, 0.00, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_parts            <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_pm               <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_direct           <dbl> 0.00, 0.00, 0.06, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_cs               <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_meeting          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_original         <dbl> 0.00, 0.00, 0.12, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_project          <dbl> 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_re               <dbl> 0.00, 0.00, 0.06, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_edu              <dbl> 0.00, 0.00, 0.06, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_table            <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_conference       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ `char_freq_;`              <dbl> 0.000, 0.000, 0.010, 0.000, 0.000, 0.000, 0…\n$ `char_freq_(`              <dbl> 0.000, 0.132, 0.143, 0.137, 0.135, 0.223, 0…\n$ `char_freq_[`              <dbl> 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0…\n$ `char_freq_!`              <dbl> 0.778, 0.372, 0.276, 0.137, 0.135, 0.000, 0…\n$ `char_freq_$`              <dbl> 0.000, 0.180, 0.184, 0.000, 0.000, 0.000, 0…\n$ `char_freq_#`              <dbl> 0.000, 0.048, 0.010, 0.000, 0.000, 0.000, 0…\n$ capital_run_length_average <dbl> 3.756, 5.114, 9.821, 3.537, 3.537, 3.000, 1…\n$ capital_run_length_longest <dbl> 61, 101, 485, 40, 40, 15, 4, 11, 445, 43, 6…\n$ capital_run_length_total   <dbl> 278, 1028, 2259, 191, 191, 54, 112, 49, 125…\n$ spam_cat                   <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n```\n\n\n:::\n:::\n\n\nThere are 58 column, the last column 'spam_cat' is the target, which is a label classification of spam (1) or not spam (0).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4601   58\n```\n\n\n:::\n:::\n\n\nEach variable represents specific words associated with spam and their percentage present in the message.\n\n### The project\n\nObjective: Create a spam detector using AI models. Create a tool that can get any text as input and estimates the probability of that message being classified as spam or not.\n\n\nCase: Company which send a lot of commercial email wants to reduce their emails to be spam. The dataset provided by the company contain some words with percentages and if were classified as spam or not.\n\n### The algorithm\n\nThis is a classification problem, then It has to be used a model which classifies. Random Forest will be it.\n\n## Preparing data for modeling\n\nWe know our objective, then we have to wrangle the data to get there.\n\n-   `tidyverse`: data wrangling and visualization.\n-   `skimr`: create descriptive statistics summary.\n-   `patchwork`: to put graphics side by side.\n-   `randomForest`: to create the model.\n-   `caret`: to create the confusion matrix.\n-   `ROCR`: to plot ROC curve.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(df)   # Dataset previously loaded\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4601   58\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 4,601\nColumns: 58\n$ word_freq_make             <dbl> 0.00, 0.21, 0.06, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_address          <dbl> 0.64, 0.28, 0.00, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_all              <dbl> 0.64, 0.50, 0.71, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_3d               <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_our              <dbl> 0.32, 0.14, 1.23, 0.63, 0.63, 1.85, 1.92, 1…\n$ word_freq_over             <dbl> 0.00, 0.28, 0.19, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_remove           <dbl> 0.00, 0.21, 0.19, 0.31, 0.31, 0.00, 0.00, 0…\n$ word_freq_internet         <dbl> 0.00, 0.07, 0.12, 0.63, 0.63, 1.85, 0.00, 1…\n$ word_freq_order            <dbl> 0.00, 0.00, 0.64, 0.31, 0.31, 0.00, 0.00, 0…\n$ word_freq_mail             <dbl> 0.00, 0.94, 0.25, 0.63, 0.63, 0.00, 0.64, 0…\n$ word_freq_receive          <dbl> 0.00, 0.21, 0.38, 0.31, 0.31, 0.00, 0.96, 0…\n$ word_freq_will             <dbl> 0.64, 0.79, 0.45, 0.31, 0.31, 0.00, 1.28, 0…\n$ word_freq_people           <dbl> 0.00, 0.65, 0.12, 0.31, 0.31, 0.00, 0.00, 0…\n$ word_freq_report           <dbl> 0.00, 0.21, 0.00, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_addresses        <dbl> 0.00, 0.14, 1.75, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_free             <dbl> 0.32, 0.14, 0.06, 0.31, 0.31, 0.00, 0.96, 0…\n$ word_freq_business         <dbl> 0.00, 0.07, 0.06, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_email            <dbl> 1.29, 0.28, 1.03, 0.00, 0.00, 0.00, 0.32, 0…\n$ word_freq_you              <dbl> 1.93, 3.47, 1.36, 3.18, 3.18, 0.00, 3.85, 0…\n$ word_freq_credit           <dbl> 0.00, 0.00, 0.32, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_your             <dbl> 0.96, 1.59, 0.51, 0.31, 0.31, 0.00, 0.64, 0…\n$ word_freq_font             <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_000              <dbl> 0.00, 0.43, 1.16, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_money            <dbl> 0.00, 0.43, 0.06, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_hp               <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_hpl              <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_george           <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_650              <dbl> 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_lab              <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_labs             <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_telnet           <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_857              <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_data             <dbl> 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_415              <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_85               <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_technology       <dbl> 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_1999             <dbl> 0.00, 0.07, 0.00, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_parts            <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_pm               <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_direct           <dbl> 0.00, 0.00, 0.06, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_cs               <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_meeting          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_original         <dbl> 0.00, 0.00, 0.12, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_project          <dbl> 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_re               <dbl> 0.00, 0.00, 0.06, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_edu              <dbl> 0.00, 0.00, 0.06, 0.00, 0.00, 0.00, 0.00, 0…\n$ word_freq_table            <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ word_freq_conference       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ `char_freq_;`              <dbl> 0.000, 0.000, 0.010, 0.000, 0.000, 0.000, 0…\n$ `char_freq_(`              <dbl> 0.000, 0.132, 0.143, 0.137, 0.135, 0.223, 0…\n$ `char_freq_[`              <dbl> 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0…\n$ `char_freq_!`              <dbl> 0.778, 0.372, 0.276, 0.137, 0.135, 0.000, 0…\n$ `char_freq_$`              <dbl> 0.000, 0.180, 0.184, 0.000, 0.000, 0.000, 0…\n$ `char_freq_#`              <dbl> 0.000, 0.048, 0.010, 0.000, 0.000, 0.000, 0…\n$ capital_run_length_average <dbl> 3.756, 5.114, 9.821, 3.537, 3.537, 3.000, 1…\n$ capital_run_length_longest <dbl> 61, 101, 485, 40, 40, 15, 4, 11, 445, 43, 6…\n$ capital_run_length_total   <dbl> 278, 1028, 2259, 191, 191, 54, 112, 49, 125…\n$ spam_cat                   <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n```\n\n\n:::\n:::\n\n\nVariables representing frequencies will maintain the double class, capital* variable will be set to Integer and spam_cat to factor.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvars.int <- c(\"capital_run_length_average\", \n              \"capital_run_length_longest\", \n              \"capital_run_length_total\")\ndf <- df %>%\n  mutate_at(\"spam_cat\", as.factor) %>%\n  mutate_at(vars.int, as.integer)\n\nanyNA(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] FALSE\n```\n\n\n:::\n:::\n\n\nThere are no missing values NA, then we can proceed with descriptive statistics using skim().\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Disable scientific notion\noptions( scipen = 999, digits = 4 )\n\nskim(df)\n```\n\n::: {.cell-output-display}\n\nTable: Data summary\n\n|                         |     |\n|:------------------------|:----|\n|Name                     |df   |\n|Number of rows           |4601 |\n|Number of columns        |58   |\n|_______________________  |     |\n|Column type frequency:   |     |\n|factor                   |1    |\n|numeric                  |57   |\n|________________________ |     |\n|Group variables          |None |\n\n\n**Variable type: factor**\n\n|skim_variable | n_missing| complete_rate|ordered | n_unique|top_counts       |\n|:-------------|---------:|-------------:|:-------|--------:|:----------------|\n|spam_cat      |         0|             1|FALSE   |        2|0: 2788, 1: 1813 |\n\n\n**Variable type: numeric**\n\n|skim_variable              | n_missing| complete_rate|   mean|     sd| p0| p25|   p50|    p75|     p100|hist  |\n|:--------------------------|---------:|-------------:|------:|------:|--:|---:|-----:|------:|--------:|:-----|\n|word_freq_make             |         0|             1|   0.10|   0.31|  0|   0|  0.00|   0.00|     4.54|▇▁▁▁▁ |\n|word_freq_address          |         0|             1|   0.21|   1.29|  0|   0|  0.00|   0.00|    14.28|▇▁▁▁▁ |\n|word_freq_all              |         0|             1|   0.28|   0.50|  0|   0|  0.00|   0.42|     5.10|▇▁▁▁▁ |\n|word_freq_3d               |         0|             1|   0.07|   1.40|  0|   0|  0.00|   0.00|    42.81|▇▁▁▁▁ |\n|word_freq_our              |         0|             1|   0.31|   0.67|  0|   0|  0.00|   0.38|    10.00|▇▁▁▁▁ |\n|word_freq_over             |         0|             1|   0.10|   0.27|  0|   0|  0.00|   0.00|     5.88|▇▁▁▁▁ |\n|word_freq_remove           |         0|             1|   0.11|   0.39|  0|   0|  0.00|   0.00|     7.27|▇▁▁▁▁ |\n|word_freq_internet         |         0|             1|   0.11|   0.40|  0|   0|  0.00|   0.00|    11.11|▇▁▁▁▁ |\n|word_freq_order            |         0|             1|   0.09|   0.28|  0|   0|  0.00|   0.00|     5.26|▇▁▁▁▁ |\n|word_freq_mail             |         0|             1|   0.24|   0.64|  0|   0|  0.00|   0.16|    18.18|▇▁▁▁▁ |\n|word_freq_receive          |         0|             1|   0.06|   0.20|  0|   0|  0.00|   0.00|     2.61|▇▁▁▁▁ |\n|word_freq_will             |         0|             1|   0.54|   0.86|  0|   0|  0.10|   0.80|     9.67|▇▁▁▁▁ |\n|word_freq_people           |         0|             1|   0.09|   0.30|  0|   0|  0.00|   0.00|     5.55|▇▁▁▁▁ |\n|word_freq_report           |         0|             1|   0.06|   0.34|  0|   0|  0.00|   0.00|    10.00|▇▁▁▁▁ |\n|word_freq_addresses        |         0|             1|   0.05|   0.26|  0|   0|  0.00|   0.00|     4.41|▇▁▁▁▁ |\n|word_freq_free             |         0|             1|   0.25|   0.83|  0|   0|  0.00|   0.10|    20.00|▇▁▁▁▁ |\n|word_freq_business         |         0|             1|   0.14|   0.44|  0|   0|  0.00|   0.00|     7.14|▇▁▁▁▁ |\n|word_freq_email            |         0|             1|   0.18|   0.53|  0|   0|  0.00|   0.00|     9.09|▇▁▁▁▁ |\n|word_freq_you              |         0|             1|   1.66|   1.78|  0|   0|  1.31|   2.64|    18.75|▇▁▁▁▁ |\n|word_freq_credit           |         0|             1|   0.09|   0.51|  0|   0|  0.00|   0.00|    18.18|▇▁▁▁▁ |\n|word_freq_your             |         0|             1|   0.81|   1.20|  0|   0|  0.22|   1.27|    11.11|▇▁▁▁▁ |\n|word_freq_font             |         0|             1|   0.12|   1.03|  0|   0|  0.00|   0.00|    17.10|▇▁▁▁▁ |\n|word_freq_000              |         0|             1|   0.10|   0.35|  0|   0|  0.00|   0.00|     5.45|▇▁▁▁▁ |\n|word_freq_money            |         0|             1|   0.09|   0.44|  0|   0|  0.00|   0.00|    12.50|▇▁▁▁▁ |\n|word_freq_hp               |         0|             1|   0.55|   1.67|  0|   0|  0.00|   0.00|    20.83|▇▁▁▁▁ |\n|word_freq_hpl              |         0|             1|   0.27|   0.89|  0|   0|  0.00|   0.00|    16.66|▇▁▁▁▁ |\n|word_freq_george           |         0|             1|   0.77|   3.37|  0|   0|  0.00|   0.00|    33.33|▇▁▁▁▁ |\n|word_freq_650              |         0|             1|   0.12|   0.54|  0|   0|  0.00|   0.00|     9.09|▇▁▁▁▁ |\n|word_freq_lab              |         0|             1|   0.10|   0.59|  0|   0|  0.00|   0.00|    14.28|▇▁▁▁▁ |\n|word_freq_labs             |         0|             1|   0.10|   0.46|  0|   0|  0.00|   0.00|     5.88|▇▁▁▁▁ |\n|word_freq_telnet           |         0|             1|   0.06|   0.40|  0|   0|  0.00|   0.00|    12.50|▇▁▁▁▁ |\n|word_freq_857              |         0|             1|   0.05|   0.33|  0|   0|  0.00|   0.00|     4.76|▇▁▁▁▁ |\n|word_freq_data             |         0|             1|   0.10|   0.56|  0|   0|  0.00|   0.00|    18.18|▇▁▁▁▁ |\n|word_freq_415              |         0|             1|   0.05|   0.33|  0|   0|  0.00|   0.00|     4.76|▇▁▁▁▁ |\n|word_freq_85               |         0|             1|   0.11|   0.53|  0|   0|  0.00|   0.00|    20.00|▇▁▁▁▁ |\n|word_freq_technology       |         0|             1|   0.10|   0.40|  0|   0|  0.00|   0.00|     7.69|▇▁▁▁▁ |\n|word_freq_1999             |         0|             1|   0.14|   0.42|  0|   0|  0.00|   0.00|     6.89|▇▁▁▁▁ |\n|word_freq_parts            |         0|             1|   0.01|   0.22|  0|   0|  0.00|   0.00|     8.33|▇▁▁▁▁ |\n|word_freq_pm               |         0|             1|   0.08|   0.43|  0|   0|  0.00|   0.00|    11.11|▇▁▁▁▁ |\n|word_freq_direct           |         0|             1|   0.06|   0.35|  0|   0|  0.00|   0.00|     4.76|▇▁▁▁▁ |\n|word_freq_cs               |         0|             1|   0.04|   0.36|  0|   0|  0.00|   0.00|     7.14|▇▁▁▁▁ |\n|word_freq_meeting          |         0|             1|   0.13|   0.77|  0|   0|  0.00|   0.00|    14.28|▇▁▁▁▁ |\n|word_freq_original         |         0|             1|   0.05|   0.22|  0|   0|  0.00|   0.00|     3.57|▇▁▁▁▁ |\n|word_freq_project          |         0|             1|   0.08|   0.62|  0|   0|  0.00|   0.00|    20.00|▇▁▁▁▁ |\n|word_freq_re               |         0|             1|   0.30|   1.01|  0|   0|  0.00|   0.11|    21.42|▇▁▁▁▁ |\n|word_freq_edu              |         0|             1|   0.18|   0.91|  0|   0|  0.00|   0.00|    22.05|▇▁▁▁▁ |\n|word_freq_table            |         0|             1|   0.01|   0.08|  0|   0|  0.00|   0.00|     2.17|▇▁▁▁▁ |\n|word_freq_conference       |         0|             1|   0.03|   0.29|  0|   0|  0.00|   0.00|    10.00|▇▁▁▁▁ |\n|char_freq_;                |         0|             1|   0.04|   0.24|  0|   0|  0.00|   0.00|     4.38|▇▁▁▁▁ |\n|char_freq_(                |         0|             1|   0.14|   0.27|  0|   0|  0.06|   0.19|     9.75|▇▁▁▁▁ |\n|char_freq_[                |         0|             1|   0.02|   0.11|  0|   0|  0.00|   0.00|     4.08|▇▁▁▁▁ |\n|char_freq_!                |         0|             1|   0.27|   0.82|  0|   0|  0.00|   0.32|    32.48|▇▁▁▁▁ |\n|char_freq_$                |         0|             1|   0.08|   0.25|  0|   0|  0.00|   0.05|     6.00|▇▁▁▁▁ |\n|char_freq_#                |         0|             1|   0.04|   0.43|  0|   0|  0.00|   0.00|    19.83|▇▁▁▁▁ |\n|capital_run_length_average |         0|             1|   4.76|  31.74|  1|   1|  2.00|   3.00|  1102.00|▇▁▁▁▁ |\n|capital_run_length_longest |         0|             1|  52.17| 194.89|  1|   6| 15.00|  43.00|  9989.00|▇▁▁▁▁ |\n|capital_run_length_total   |         0|             1| 283.29| 606.35|  1|  35| 95.00| 266.00| 15841.00|▇▁▁▁▁ |\n\n\n:::\n:::\n\n\n-   Most of the p50 and p75 values are close to 0, meanwhile the mean is higher. Suggests a high right tail on the data.\n-   There are 1813 spam emails (39.4%)\n-   High Standard deviation suggests outliers and spread data.\n\n## Exploring the data with visuals\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfor (var in colnames(select_if(df[,1:47], is.numeric) ) ) {\n    hist( unlist( df[,var]), col = \"blue\",\n          main = paste(\"Histogram of\", var),\n          xlab = var)\n}\n```\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-8-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-8-3.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-8-4.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-8-5.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-8-6.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-8-7.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-8-8.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-8-9.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-8-10.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-8-11.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-8-12.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-8-13.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-8-14.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-8-15.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-8-16.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-8-17.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-8-18.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-8-19.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-8-20.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-8-21.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-8-22.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-8-23.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-8-24.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-8-25.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-8-26.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-8-27.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-8-28.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-8-29.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-8-30.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-8-31.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-8-32.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-8-33.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-8-34.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-8-35.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-8-36.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-8-37.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-8-38.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-8-39.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-8-40.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-8-41.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-8-42.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-8-43.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-8-44.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-8-45.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-8-46.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-8-47.png){width=672}\n:::\n:::\n\n\nTo use ggplot2 we need that dataset in tidy format, to do that we must convert this wide format to long format with the columns *spam* for 'spam_cat', *word* for every column name, and *freq* for the values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf.long <- df %>%\n    pivot_longer( cols = 1:57, names_to = \"words\", values_to = \"pct\")\nhead(df.long, 9)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 9 × 3\n  spam_cat words                pct\n  <fct>    <chr>              <dbl>\n1 1        word_freq_make      0   \n2 1        word_freq_address   0.64\n3 1        word_freq_all       0.64\n4 1        word_freq_3d        0   \n5 1        word_freq_our       0.32\n6 1        word_freq_over      0   \n7 1        word_freq_remove    0   \n8 1        word_freq_internet  0   \n9 1        word_freq_order     0   \n```\n\n\n:::\n:::\n\n\nNow its ready for plotting with ggplot2, then we can create boxplots. Boxplot are useful to see the outliers that can distort our classifier (Random Forest), also which words appear more frequently which impact the classification. We can filter only the 'word_' from words column and `spam==1`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf.long %>%\n    filter( str_detect(words, \"word_\") & spam_cat == 1) %>%\n    ggplot() +\n        geom_boxplot( aes( y = reorder(words, pct), x = pct, fill = spam_cat ) ) +\n        labs( title = \"Percentages of words and their association with spam emails\",\n              subtitle = \"The frequency of appearance of some words in emails is more associated with spam\",\n              x = \"Percentage\",\n              y = \"Word\" ) +\n        theme_classic() +\n        theme( plot.subtitle = element_text( color = \"darkgray\", size = 10 ),\n               legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nAfter the 24th record all the boxplots have their medians too close to zero, so they do not impact the spam classification too much. Then we know that the top 23 words can have more impact so we have to compare spam and not spam in this words to see how they impact the entire data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntemp <- df.long %>% \n    filter( str_detect(words, \"word_\") & spam_cat == 1) %>%\n    group_by(words) %>% \n    summarise( pct_sum = sum(pct) ) %>%\n    arrange( desc(pct_sum) )\n\ntop.words <- c(temp$words[1:23], \"spam_cat\")\nremove(temp)\n```\n:::\n\n\nIn this next code we generate a boxplot comparing spam and not spam only on the selected top words. It can be seen as a focus to see more easily the differences.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf.top <- df %>%\n    select( all_of(top.words) ) %>%\n    mutate( top_words_pct = rowSums( across( where(is.numeric) ) ) )\n\ng1 <- ggplot(df.top) + \n    geom_boxplot( aes( y = factor(spam_cat), x = top_words_pct),\n                  fill = c(\"turquoise\", \"coral\") ) +\n    labs( title = \"How the presence of words associated with spam emails \n          impacts the classification (TOP23)\",\n          subtitle = \"The spam emails(1) have a ghigher percentage of those words.\") +\n    theme_classic()\n```\n:::\n\n\nIn the same way with this second boxplot we compare spam and not spam but with all the words.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf.2 <- df %>%\n    mutate(word_pct = rowSums( across( where(is.numeric) ) ) )\n\ng2 <- ggplot(df.2) +\n    geom_boxplot( aes( y = factor(spam_cat), x = word_pct ),\n                  fill = c(\"turquoise\", \"coral\") ) +\n    labs( title = \"How spam associated words impacts the classification\",\n          subtitle = \"The spam emails(1) have a ghigher percentage of those words.\") +\n    theme_classic()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Patchwork library, putting the objects into a parenthesis with a pipe\n\n(g1 | g2)\n```\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-14-1.png){width=960}\n:::\n:::\n\n\nIt can be seen a large median of those words dissociated with spam emails. To be sure that difference is statistically significant we can perform Kolmogorov-Smirnov test which compare distributions, or U-Mann Whitney which compare medians.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npos.spam <- df.top[df.top[\"spam_cat\"] == 1,][[\"top_words_pct\"]]\nneg.spam <- df.top[df.top[\"spam_cat\"] == 0,][[\"top_words_pct\"]]\n\nks.test(pos.spam, neg.spam)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in ks.test.default(pos.spam, neg.spam): p-value will be approximate in\nthe presence of ties\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tAsymptotic two-sample Kolmogorov-Smirnov test\n\ndata:  pos.spam and neg.spam\nD = 0.55, p-value <0.0000000000000002\nalternative hypothesis: two-sided\n```\n\n\n:::\n\n```{.r .cell-code}\nwilx <- wilcox.test(pos.spam, neg.spam)\nwilx\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWilcoxon rank sum test with continuity correction\n\ndata:  pos.spam and neg.spam\nW = 4182287, p-value <0.0000000000000002\nalternative hypothesis: true location shift is not equal to 0\n```\n\n\n:::\n\n```{.r .cell-code}\n# Size effect formula by Wendt\nrbis <- sum(-1, (2 * wilx$statistic) / (length(pos.spam) * length(neg.spam)) )\ncat(\"U Mann Whitney effect size r = \", rbis)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nU Mann Whitney effect size r =  0.6548\n```\n\n\n:::\n:::\n\n\nBoth test show p < 0.05 thus both groups have different distribution and median according to KS-test and U Mann Whitney test respectively. Also, the size effect Biserial-Rank correlation of U Mann-Whitney is big meaning there are a significance difference between the groups. The spam group values tend to be considerably bigger than no-spam group. Then, the variable spam have a high impact in the difference of the groups.\n\nSince we know the words impact now we can test the characters in a similar way:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf.long %>%\n    filter( str_detect(words, \"char_\") & spam_cat == 1) %>%\n    ggplot() +\n        geom_boxplot( aes( y = reorder(words, pct), x = pct, fill = spam_cat ) ) +\n        labs( title = \"Percentages of special characters and their association with spam emails\",\n              subtitle = \"The frequency of appearance of some characters in emails is more associated with spam\",\n              x = \"Percentage\",\n              y = \"Character\" ) +\n        theme_classic() +\n        theme( plot.subtitle = element_text( color = \"darkgray\", size = 10 ),\n               legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndf.char <- df %>%\n    select_if(str_detect(colnames(df), pattern = \"char_|^spam_cat\")) %>%\n    mutate( char_pct = rowSums( across( where(is.numeric) ) ) )\n\ng3 <- ggplot(df.char) + \n    geom_boxplot( aes( y = factor(spam_cat), x = char_pct),\n                  fill = c(\"turquoise\", \"coral\") ) +\n    labs( title = \"How the presence of characters associated with spam emails \n          impacts the classification\",\n          subtitle = \"The spam emails(1) have a ghigher percentage of those characters.\") +\n    theme_classic()\n\ng3\n```\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndf.char.zero <- df.char %>% filter(spam_cat == 0)\ndf.char.one <- df.char %>% filter(spam_cat == 1)\n\nfor (var in colnames(df.char.zero[,1:6])) {\n    wilx <- wilcox.test(df.char.zero[[var]], df.char.one[[var]])\n    rbis <- abs(sum(-1, (2 * wilx$statistic) / (length(pos.spam) * length(neg.spam)) ))\n    cat(\"\\nMann-Whitney U between spam and not spam on:\", var, \n        \"\\np-value:\", sprintf(\"%6.4f\", wilx$p.value),\n        \"\\nr:\", rbis)\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nMann-Whitney U between spam and not spam on: char_freq_; \np-value: 0.0001 \nr: 0.04412\nMann-Whitney U between spam and not spam on: char_freq_( \np-value: 0.0269 \nr: 0.03721\nMann-Whitney U between spam and not spam on: char_freq_[ \np-value: 0.0000 \nr: 0.07279\nMann-Whitney U between spam and not spam on: char_freq_! \np-value: 0.0000 \nr: 0.6581\nMann-Whitney U between spam and not spam on: char_freq_$ \np-value: 0.0000 \nr: 0.5443\nMann-Whitney U between spam and not spam on: char_freq_# \np-value: 0.0000 \nr: 0.2027\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ng4 <- df.char.zero %>% \n    pivot_longer(cols = 1:6, names_to = \"chars\", values_to = \"pct\") %>%\n    ggplot() +\n    geom_boxplot( aes( y = chars, x = pct) ) +\n    labs( y = \"Characters in not spam emails\",\n          x = \"\") +\n    theme_classic()\n\ng5 <- df.char.one %>% \n    pivot_longer(cols = 1:6, names_to = \"chars\", values_to = \"pct\") %>%\n    ggplot() +\n    geom_boxplot( aes( y = chars, x = pct) ) +\n    labs( x = \"Frequencies\",\n          y = \"Characters in spam emails\") +\n    theme_classic()\n\n(g4 / g5)\n```\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-19-1.png){width=768}\n:::\n:::\n\n\n\nAt this point we know that the number of symbols, and words are statistically different for each group in our classification.\n\n## Selecting the best variables\n\nWhen we checked the variables with boxplots and test comparisons we show how them impact the classifications the most. We should use those variables that have the highest difference between both groups so that it's easier for the algorithm to find a clearer separations between the two groups. The conclusion we show is that 23 words maximize the difference as well as uppercase and the presence of too many symbols.\n\nWe are prepare to create a dataset for modeling. Taking the original dataframe, binding top_words_pct, the target variable, the character and uppercase variables:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf.model <- df %>%\n    bind_cols( top_words_pct = df.top$top_words_pct ) %>%\n    select( spam_cat, top_words_pct, \n            `char_freq_!`, `char_freq_(`, `char_freq_$`,\n            capital_run_length_total, capital_run_length_longest ) %>%\n    mutate(spam_cat = ifelse(spam_cat == 1, \"is_spam\", \"no_spam\")) %>%\n    mutate_at(vars(spam_cat), as.factor)\n\n\ncolnames(df.model)[3:5] <- c(\"char_freq_exclam\", \"char_freq_paren\", \"char_freq_dollar\")\nslice_sample(df.model, n = 9)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 9 × 7\n  spam_cat top_words_pct char_freq_exclam char_freq_paren char_freq_dollar\n  <fct>            <dbl>            <dbl>           <dbl>            <dbl>\n1 no_spam           2.32            0.314           0                0    \n2 is_spam          13.0             0.659           0.085            0.114\n3 is_spam          12.0             0.487           0.081            0    \n4 no_spam           3.96            0               0.146            0    \n5 no_spam           3.2             0.072           0.108            0    \n6 no_spam           5.14            0               0.237            0    \n7 no_spam           4.18            0               0.371            0    \n8 is_spam          10.9             0.407           0                0    \n9 is_spam           3.78            0.926           0.411            0    \n# ℹ 2 more variables: capital_run_length_total <int>,\n#   capital_run_length_longest <int>\n```\n\n\n:::\n:::\n\n\n## Modeling\n\n### Training\n\nIn the last code we just replace 1 with 'is_spam' and 0 with 'no_spam'. This is necessary for random forest in this case to classify.\n\n```r\n# Alternative code to replace values:\ndf.model <- df.model %>%\n    mutate( spam_cat = recode(spam_cat, '1'=\"is_spam\", '0'=\"no_spam\") )\n```\n\n-   train: subset used to present the model with the patterns and the labels associated with it so that it can study how to classify each observation according to the patterns that occur.\n-   test: subset where new data is presented to the trained model so that we can measure how accurate it is or how much it has learned.\n\nThe dataset have aprox 60-40 as not-spam and spam respectively. In this case, we won't apply any category balancing technique, this imbalance will not affect the result.\n\n**In R there is no function for splitting the dataset into train and test**\n\nWe are going to use 80% for training and 20% for testing.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn.rows <- nrow(df.model)\nidx <- sample(1:n.rows, size = 0.8 * n.rows)\n\ntrain.80 <- df.model[idx,]\ntest.20 <- df.model[-idx,]\n```\n:::\n\n\nNow we can check if the proportions are similar to the original dataset:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwriteLines(\"Original set:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOriginal set:\n```\n\n\n:::\n\n```{.r .cell-code}\nprop.table( table(df$spam_cat) )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n    0     1 \n0.606 0.394 \n```\n\n\n:::\n\n```{.r .cell-code}\nwriteLines(\"==================\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n==================\n```\n\n\n:::\n\n```{.r .cell-code}\nwriteLines(\"Train set:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrain set:\n```\n\n\n:::\n\n```{.r .cell-code}\nprop.table( table(train.80$spam_cat) )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nis_spam no_spam \n 0.3894  0.6106 \n```\n\n\n:::\n\n```{.r .cell-code}\nwriteLines(\"==================\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n==================\n```\n\n\n:::\n\n```{.r .cell-code}\nwriteLines(\"Test set:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTest set:\n```\n\n\n:::\n\n```{.r .cell-code}\nprop.table( table(test.20$spam_cat) )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nis_spam no_spam \n 0.4126  0.5874 \n```\n\n\n:::\n:::\n\n\n-   `randomForest()` It must be passed the target variable with `~ .` meaning the target will be explained by the rest of the data represented with '`.`'\n    -   `data=` the data set to use, will be the training set.\n    -   `importance=` if TRUE it will calculate the importance of the variables.\n    -   `ntree=` the number of decision trees to create with this model.\n    \n\n::: {.cell}\n\n```{.r .cell-code}\nmodel.rf <- randomForest( spam_cat ~ ., data = train.80,\n                          importance = TRUE,\n                          ntree = 250)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(model.rf)\n```\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n\nThis plot shows the performance of the model. After the 50th tree, the error stabilizes.\n\n-   `varImpPlot()`: to plot the feature's importance or which variables are more importance to the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvarImpPlot(model.rf)\n```\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-25-1.png){width=768}\n:::\n:::\n\n\nThis plot shows the importance of each variable.\n\n-   Mean Decrease Accuracy: it calculates how much the model's accuracy decreases when the values of a particular feature are randomly shuffled. A higher value indicates that a feature is more important for the model's accuracy. Then the model relies heavily on that features to make accurate predictions.\n-   Mean Decrease Gini: A higher value indicates that a feature is more important for the model's ability to separate the classes. Features with high MeanDecreaseGini values are those that are useful for creating decision boundaries in the model.\n\nFor the library `randomForest` the default for classification models is the Gini index.\n\n### Testing and evaluating the model\n\nThe most used metrics to evaluate a classification model are *accuracy, confusion matrix, and ROC curve*. \n-   `predict()`: to generate a prediction given as arguments the model and the test set. The object assigned can be used to extract information.\n\n-   `confusionMatrix()`: given a prediction and the target column from the test set shows the information of that prediction.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Prediction:\nmodel.preds <- predict(model.rf, test.20)\n\nconfusionMatrix(model.preds, test.20$spam_cat, positive = \"no_spam\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction is_spam no_spam\n   is_spam     328      27\n   no_spam      52     514\n                                              \n               Accuracy : 0.914               \n                 95% CI : (0.894, 0.932)      \n    No Information Rate : 0.587               \n    P-Value [Acc > NIR] : < 0.0000000000000002\n                                              \n                  Kappa : 0.821               \n                                              \n Mcnemar's Test P-Value : 0.00693             \n                                              \n            Sensitivity : 0.950               \n            Specificity : 0.863               \n         Pos Pred Value : 0.908               \n         Neg Pred Value : 0.924               \n             Prevalence : 0.587               \n         Detection Rate : 0.558               \n   Detection Prevalence : 0.615               \n      Balanced Accuracy : 0.907               \n                                              \n       'Positive' Class : no_spam             \n                                              \n```\n\n\n:::\n:::\n\n\nThe confusion matrix show how were the prediction. For 'is_spam' predicted value are 315 'is_spam' as reference, then this 315 are True Positive. The next value is for the predicted 'is_spam' and 'no_spam' as reference with 38 matches, corresponding False Positive known as Type 1 Error. The next two values are False Negative (Type 2 Error) and True Negative with 52 and 516 matches respectively.\n\nThe accuracy is 0.9 aprox, then out of the 100 predictions, there will be around 10 errors with a confidence interval about 12 to 8 errors.\n\n-   **Accuracy**: How many of the total number of classifications the model predicted correctly. Is an overall sense of the model's performance.\n-   **Sensitivity** and **Specificity**: provide insights into the model's performance on the positive and negative classes, respectively. In many cases, depending on the problem, one metric or the other will be more important. (e.g. Medicine, the true positive rate might be more critical then we'll look sensitivity.)\n-   **Kappa**: by accounting for chance agreement helps us to understand the model's performance. A high kappa indicates a strong agreement between the predicted and actual labels.\n-   **Balanced Accuracy**: useful when the labels are unbalanced. It provides a balanced measure of the model's performance.\n\nThe relatively high accuracy (0.902) and a strong kappa value (0.795), it suggests that the model is performing well overall. The sensitivity (0.858) and specificity (0.931) values indicate that the model detect both positive and negative instances quite well.\n\nNow it's time to look at ROC curve to check the performance based on the rate of true positive and false positives (Sensitivity and Specificity).\n\n-   `prediction()` given the data frame with probability predictions with the positive class ('no_spam' according to confusionMatrix) and the real labels which is the test set selecting the target.\n-   `performance()` given the object with `prediction()` and the labels for the axes which are 'tpr' and 'fpr' for true positive rate and false positive rate.\n-   `abline()` for the visualization will draw a diagonal that separates the graphic 50/50\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel.df.preds <- data.frame( predict(model.rf, test.20, type='prob') )\n\nmodel.roc <- prediction(model.df.preds$no_spam, test.20$spam_cat)\n\nroc <- performance(model.roc, 'tpr', 'fpr')\nauc <- performance(model.roc, \"auc\")\nplot(roc, colorize = T, lwd = 2,\n     main = \"ROC curve. 'no_spam' as positive class.\",\n     sub = paste( \"AUC =\", auc@y.values[[1]]) )\nabline(0.0, 1.0)\n```\n\n::: {.cell-output-display}\n![](14_modeling2deploy_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n:::\n\n\nAt the bottom we can see the Area Under the Receiver Operating Characteristic Curve (AUC-ROC). If `AUC=1` represents a perfect classifier meanwhile `AUC=0.5` is a random classifier. In this case is a pretty good classier, this value indicates the performance of the classifier.\n\n### Predicting\n\nAfter evaluating, it's time to test the model. \n\nThe input to predict must to be in the very same format as the data frame used in the Random Forest. Then, if we did any transformations this have to be repeated before we can input the new data.\n\nTo do that, we can create a custom function with the steps of preparation and modeling that we took until create 'df.model'. To that custom function we pass whatever we want to predict depending the objective (spam in this case, then we pass a mail) and the function will perform every step to model the data.\n\nThe next function called `prepare_input()` is from this book's code, which takes a string of text and the spam_words vector as input. Then, it reads the text and it will count the selected columns we modeled before, such as !, $, (), uppercase letters, the longest sequence of uppercase, and words in the spam list. Finally, returning a data frame for input in the model we create.\n\n\n::: {.cell}\n\n:::\n\n\nThe next objects will be the emails to check and the spam_words to use:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntext1 <- 'SALE!! SALE!! SALE!! SUPER SALEEEE!! This is one of the best sales of the year! More than #3000# products with discounts up to $500 off!! Visit our page and Save $$$ now! Order your product NOW (here) and get one for free !'\n\ntext2 <- 'DEAR MR. JOHN, You will find enclosed the file we talked about during your meeting earlier today. The attachment received here is also available in our web site at this address: www.DUMMYSITE.com. Sale.'\n\nspam.words <- c('you', 'your', 'will', 'free', 'our', 'all', 'mail', 'email', 'business', 'remove', '000', 'font', 'money', 'internet', 'credit', 'over', 'order', '3d', 'address', 'make', 'people', 're', 'receive', 'sale')\n```\n:::\n\n\nIt is time to predict: \n\n\n::: {.cell}\n\n```{.r .cell-code}\ninput <- prepare_input(text1, spam.words)\n\ndata.frame( predict(model.rf, input, type = \"prob\") )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  is_spam no_spam\n1   0.668   0.332\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ninput <- prepare_input(text2, spam.words)\n\ndata.frame( predict(model.rf, input, type = \"prob\") )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  is_spam no_spam\n1   0.512   0.488\n```\n\n\n:::\n:::\n\n\nWe can see in this case that spam email (text1) is with 70% probability spam, although text2 which is no spam the model tell us is with 0.51% no spam. Looking the text2 there are some uppercase letters which can fool the model a little bit.\n\nThe next step is save this model and be ready for deployment. To save the model we can use `saveRDS()` function and input the model's name and the name of the output filename. Later with Shiny application we can deploy the model to do it's job (receiving emails from a user or whatever).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsaveRDS(model.rf, \"../scripts/model_rf_spam.rds\")\n```\n:::\n\n\n## Useful Links\n\n-   Mode deep in Supervised and Unsupervised learning, with evaluation measures: https://www.geeksforgeeks.org/supervised-unsupervised-learning/\n\n\n\n# Build an Application with Shiny\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "supporting": [
      "14_modeling2deploy_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
{
  "hash": "932cacb93959977c9e979cbf8fbc2e42",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Exploratory Data Analysis\"\n---\n\n\n#### Libraries {.unnumbered}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse, quietly = TRUE)\nlibrary(skimr, quietly = TRUE)\nlibrary(statsr, quietly = TRUE)\nlibrary(GGally, quietly = TRUE)\nlibrary(corrplot, quietly = TRUE)\n```\n:::\n\n\nDataset: <https://github.com/fivethirtyeight/data/tree/master/college-majors>\n\n| Header                 | Description                                                                |\n|--------------------|----------------------------------------------------|\n| `Rank`                 | Rank by median earnings                                                    |\n| `Major_code`           | Major code, FO1DP in ACS PUMS                                              |\n| `Major`                | Major description                                                          |\n| `Major_category`       | Category of major from Carnevale et al                                     |\n| `Total`                | Total number of people with major                                          |\n| `Sample_size`          | Sample size (unweighted) of full-time, year-round ONLY (used for earnings) |\n| `Men`                  | Male graduates                                                             |\n| `Women`                | Female graduates                                                           |\n| `ShareWomen`           | Women as share of total                                                    |\n| `Employed`             | Number employed (ESR == 1 or 2)                                            |\n| `Full_time`            | Employed 35 hours or more                                                  |\n| `Part_time`            | Employed less than 35 hours                                                |\n| `Full_time_year_round` | Employed at least 50 weeks (WKW == 1) and at least 35 hours (WKHP \\>= 35)  |\n| `Unemployed`           | Number unemployed (ESR == 3)                                               |\n| `Unemployment_rate`    | Unemployed / (Unemployed + Employed)                                       |\n| `Median`               | Median earnings of full-time, year-round workers                           |\n| `P25th`                | 25th percentile of earnings                                                |\n| `P75th`                | 75th percentile of earnings                                                |\n| `College_jobs`         | Number with job requiring a college degree                                 |\n| `Non_college_jobs`     | Number with job not requiring a college degree                             |\n| `Low_wage_jobs`        | Number in low-wage service jobs                                            |\n\nThe intent of this section is to go over a practical project, following the steps: loading the dataset, understanding the data, treating missing values, exploring and visualizing, and the analysis report.\n\n# Loading the dataset\n\nTo continue we need the libraries mentioned before:\n\n-   `tidyverse` : eight libraries included, to work with data\n-   `skimr` : descriptive statistics\n-   `statsr` : statistical tools and data sampling\n-   `GGally` : for pair plots\n-   `corrplot` : correlation plots\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Reading a csv from a web\nurl <- \"https://raw.githubusercontent.com/fivethirtyeight/data/master/college-majors/recent-grads.csv\"\n\ndf <- read_csv(url)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 173 Columns: 21\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): Major, Major_category\ndbl (19): Rank, Major_code, Total, Men, Women, ShareWomen, Sample_size, Empl...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\ndf.original <- df\n```\n:::\n\n\nWe'll keep a copy of the original data frame just in case we need to recover some columns or for comparison reasons.\n\n# Understanding the data\n\nLet's check the firsts observations:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf %>%\n    head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 21\n   Rank Major_code Major Total   Men Women Major_category ShareWomen Sample_size\n  <dbl>      <dbl> <chr> <dbl> <dbl> <dbl> <chr>               <dbl>       <dbl>\n1     1       2419 PETR…  2339  2057   282 Engineering         0.121          36\n2     2       2416 MINI…   756   679    77 Engineering         0.102           7\n3     3       2415 META…   856   725   131 Engineering         0.153           3\n4     4       2417 NAVA…  1258  1123   135 Engineering         0.107          16\n5     5       2405 CHEM… 32260 21239 11021 Engineering         0.342         289\n6     6       2418 NUCL…  2573  2200   373 Engineering         0.145          17\n# ℹ 12 more variables: Employed <dbl>, Full_time <dbl>, Part_time <dbl>,\n#   Full_time_year_round <dbl>, Unemployed <dbl>, Unemployment_rate <dbl>,\n#   Median <dbl>, P25th <dbl>, P75th <dbl>, College_jobs <dbl>,\n#   Non_college_jobs <dbl>, Low_wage_jobs <dbl>\n```\n\n\n:::\n:::\n\n\nWe can see that the data is **rectangular**. At first we do not see any problems with language encoding (strange symbols). And the CSV was read correctly because the columns are fine delimited.\n\n-   `glimpse()`: similar to `str()`, returns the columns, types and some values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 173\nColumns: 21\n$ Rank                 <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15…\n$ Major_code           <dbl> 2419, 2416, 2415, 2417, 2405, 2418, 6202, 5001, 2…\n$ Major                <chr> \"PETROLEUM ENGINEERING\", \"MINING AND MINERAL ENGI…\n$ Total                <dbl> 2339, 756, 856, 1258, 32260, 2573, 3777, 1792, 91…\n$ Men                  <dbl> 2057, 679, 725, 1123, 21239, 2200, 2110, 832, 803…\n$ Women                <dbl> 282, 77, 131, 135, 11021, 373, 1667, 960, 10907, …\n$ Major_category       <chr> \"Engineering\", \"Engineering\", \"Engineering\", \"Eng…\n$ ShareWomen           <dbl> 0.1205643, 0.1018519, 0.1530374, 0.1073132, 0.341…\n$ Sample_size          <dbl> 36, 7, 3, 16, 289, 17, 51, 10, 1029, 631, 399, 14…\n$ Employed             <dbl> 1976, 640, 648, 758, 25694, 1857, 2912, 1526, 764…\n$ Full_time            <dbl> 1849, 556, 558, 1069, 23170, 2038, 2924, 1085, 71…\n$ Part_time            <dbl> 270, 170, 133, 150, 5180, 264, 296, 553, 13101, 1…\n$ Full_time_year_round <dbl> 1207, 388, 340, 692, 16697, 1449, 2482, 827, 5463…\n$ Unemployed           <dbl> 37, 85, 16, 40, 1672, 400, 308, 33, 4650, 3895, 2…\n$ Unemployment_rate    <dbl> 0.018380527, 0.117241379, 0.024096386, 0.05012531…\n$ Median               <dbl> 110000, 75000, 73000, 70000, 65000, 65000, 62000,…\n$ P25th                <dbl> 95000, 55000, 50000, 43000, 50000, 50000, 53000, …\n$ P75th                <dbl> 125000, 90000, 105000, 80000, 75000, 102000, 7200…\n$ College_jobs         <dbl> 1534, 350, 456, 529, 18314, 1142, 1768, 972, 5284…\n$ Non_college_jobs     <dbl> 364, 257, 176, 102, 4440, 657, 314, 500, 16384, 1…\n$ Low_wage_jobs        <dbl> 193, 50, 0, 0, 972, 244, 259, 220, 3253, 3170, 98…\n```\n\n\n:::\n:::\n\n\nLooking the previous output we can see two character columns (Mayor, Mayor_category), this columns will be factor but also 'Major_code' which is an identification. We can keep all three as character to avoid problems with data handling, or we could work with factors taking care about this.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncols.factor <- c(\"Major\", \"Major_code\", \"Major_category\")\n\ndf <- df %>%\n    mutate_at(cols.factor, factor)\n```\n:::\n\n\nBefore to start with descriptive statistics let's disable scientific notations and establishing 4 decimals.\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptions(scipen=999, digits=4)\n```\n:::\n\n\n-   `skim()`: returns a summary table for the given data frame\n\nLets store the skim table in an object and calculate manually the coefficient of variation for the variables:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstats <- skim(df)\nstats$numeric.sd / stats$numeric.mean\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1]     NA     NA     NA 0.5757 1.6125 1.6816 1.8130 0.4427 1.7366 1.6246\n[11] 1.6470 1.6585 1.6838 1.7021 0.4448 0.2857 0.3107 0.2895 1.7285 1.7908\n[21] 1.7997\n```\n\n\n:::\n\n```{.r .cell-code}\n# The returned NA values are the character/factor variables\n```\n:::\n\n\n-   CV close to 0: there is a little variability in the data which is around the mean.\n-   CV between 0 and 1: moderate variability. Closer to 1 the greater dispersion.\n-   CV close to 2: high variability suggesting highly dispersion. The mean may not be a reliable representation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Descriptive statistics\n\nskim(df)\n```\n\n::: {.cell-output-display}\n\nTable: Data summary\n\n|                         |     |\n|:------------------------|:----|\n|Name                     |df   |\n|Number of rows           |173  |\n|Number of columns        |21   |\n|_______________________  |     |\n|Column type frequency:   |     |\n|factor                   |3    |\n|numeric                  |18   |\n|________________________ |     |\n|Group variables          |None |\n\n\n**Variable type: factor**\n\n|skim_variable  | n_missing| complete_rate|ordered | n_unique|top_counts                         |\n|:--------------|---------:|-------------:|:-------|--------:|:----------------------------------|\n|Major_code     |         0|             1|FALSE   |      173|110: 1, 110: 1, 110: 1, 110: 1     |\n|Major          |         0|             1|FALSE   |      173|ACC: 1, ACT: 1, ADV: 1, AER: 1     |\n|Major_category |         0|             1|FALSE   |       16|Eng: 29, Edu: 16, Hum: 15, Bio: 14 |\n\n\n**Variable type: numeric**\n\n|skim_variable        | n_missing| complete_rate|     mean|       sd|    p0|      p25|      p50|      p75|      p100|hist  |\n|:--------------------|---------:|-------------:|--------:|--------:|-----:|--------:|--------:|--------:|---------:|:-----|\n|Rank                 |         0|          1.00|    87.00|    50.08|     1|    44.00|    87.00|   130.00|    173.00|▇▇▇▇▇ |\n|Total                |         1|          0.99| 39370.08| 63483.49|   124|  4549.75| 15104.00| 38909.75| 393735.00|▇▁▁▁▁ |\n|Men                  |         1|          0.99| 16723.41| 28122.43|   119|  2177.50|  5434.00| 14631.00| 173809.00|▇▁▁▁▁ |\n|Women                |         1|          0.99| 22646.67| 41057.33|     0|  1778.25|  8386.50| 22553.75| 307087.00|▇▁▁▁▁ |\n|ShareWomen           |         1|          0.99|     0.52|     0.23|     0|     0.34|     0.53|     0.70|      0.97|▂▆▆▇▃ |\n|Sample_size          |         0|          1.00|   356.08|   618.36|     2|    39.00|   130.00|   338.00|   4212.00|▇▁▁▁▁ |\n|Employed             |         0|          1.00| 31192.76| 50675.00|     0|  3608.00| 11797.00| 31433.00| 307933.00|▇▁▁▁▁ |\n|Full_time            |         0|          1.00| 26029.31| 42869.66|   111|  3154.00| 10048.00| 25147.00| 251540.00|▇▁▁▁▁ |\n|Part_time            |         0|          1.00|  8832.40| 14648.18|     0|  1030.00|  3299.00|  9948.00| 115172.00|▇▁▁▁▁ |\n|Full_time_year_round |         0|          1.00| 19694.43| 33160.94|   111|  2453.00|  7413.00| 16891.00| 199897.00|▇▁▁▁▁ |\n|Unemployed           |         0|          1.00|  2416.33|  4112.80|     0|   304.00|   893.00|  2393.00|  28169.00|▇▁▁▁▁ |\n|Unemployment_rate    |         0|          1.00|     0.07|     0.03|     0|     0.05|     0.07|     0.09|      0.18|▂▇▆▁▁ |\n|Median               |         0|          1.00| 40151.45| 11470.18| 22000| 33000.00| 36000.00| 45000.00| 110000.00|▇▅▁▁▁ |\n|P25th                |         0|          1.00| 29501.45|  9166.01| 18500| 24000.00| 27000.00| 33000.00|  95000.00|▇▂▁▁▁ |\n|P75th                |         0|          1.00| 51494.22| 14906.28| 22000| 42000.00| 47000.00| 60000.00| 125000.00|▅▇▂▁▁ |\n|College_jobs         |         0|          1.00| 12322.64| 21299.87|     0|  1675.00|  4390.00| 14444.00| 151643.00|▇▁▁▁▁ |\n|Non_college_jobs     |         0|          1.00| 13284.50| 23789.66|     0|  1591.00|  4595.00| 11783.00| 148395.00|▇▁▁▁▁ |\n|Low_wage_jobs        |         0|          1.00|  3859.02|  6945.00|     0|   340.00|  1231.00|  3466.00|  48207.00|▇▁▁▁▁ |\n\n\n:::\n:::\n\n\nWe can see the is a missing value in four variables, we need to know if it is the same observation. Also, there are more women than men and more people employed than unemployed. The jobs requiring a college degree are balanced with the jobs which no require a degree. Looking the Coefficient of Variation, almost every variable have a high value, greater than 1, which suggest high dispersion; and a very few variables have 0.3 or less which suggest the data is around the mean on this variables.\n\n# Treating missing data\n\nThe missing values also have meaning, could be a human mistake or an omitted response of the user.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf[which(is.na(df$Total)),]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 21\n   Rank Major_code Major Total   Men Women Major_category ShareWomen Sample_size\n  <dbl> <fct>      <fct> <dbl> <dbl> <dbl> <fct>               <dbl>       <dbl>\n1    22 1104       FOOD…    NA    NA    NA Agriculture &…         NA          36\n# ℹ 12 more variables: Employed <dbl>, Full_time <dbl>, Part_time <dbl>,\n#   Full_time_year_round <dbl>, Unemployed <dbl>, Unemployment_rate <dbl>,\n#   Median <dbl>, P25th <dbl>, P75th <dbl>, College_jobs <dbl>,\n#   Non_college_jobs <dbl>, Low_wage_jobs <dbl>\n```\n\n\n:::\n:::\n\n\nSince there is only one observation we can drop it because represent less than 1%. It is recommended to use other treatment if missing values are greater than 5% of the observations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf.clean <- df %>% drop_na()\n\ndim(df.clean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 172  21\n```\n\n\n:::\n:::\n\n\n# Exploring and visualizing the data\n\nHere we are going to create some questions to lead the exploration.\n\n## Univariate Analysis\n\nIt is to look one variable at a time. Let's graph some histograms of the numeric variables with a *for* loop.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfor (var in colnames(select_if(df.clean, is.numeric)) ) {\n    g = ggplot(df.clean) + \n        geom_histogram( aes( unlist(df.clean[, var]) ), bins=20,\n                        fill = \"darkblue\", color = \"lightblue\", alpha = 0.5) +\n        labs( title = paste(\"Histogram of\", var),\n              x = var)\n    plot(g)\n}\n```\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-11-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-11-3.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-11-4.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-11-5.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-11-6.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-11-7.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-11-8.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-11-9.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-11-10.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-11-11.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-11-12.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-11-13.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-11-14.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-11-15.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-11-16.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-11-17.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-11-18.png){width=672}\n:::\n:::\n\n\nAnd to be able to see outliers the boxplot will be our aproach.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfor ( var in colnames( select_if(df.clean, is.numeric) ) ) {\n    g = ggplot(df.clean) +\n        geom_boxplot( aes( y = unlist(df.clean[, var]) ),\n                      fill = \"lightblue\", color = \"blue\") +\n        labs(title = paste(\"Boxplot of\", var),\n             y = var)\n    plot(g)\n}\n```\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-12-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-12-3.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-12-4.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-12-5.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-12-6.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-12-7.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-12-8.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-12-9.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-12-10.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-12-11.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-12-12.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-12-13.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-12-14.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-12-15.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-12-16.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-12-17.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-12-18.png){width=672}\n:::\n:::\n\n\nWe can add a QQ plot as a visual indication of normality.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfor (var in colnames( select_if(df.clean, is.numeric)) ) {\n    g = ggplot(df.clean, aes( sample = unlist( df.clean[, var])) ) +\n        stat_qq() +\n        stat_qq_line() +\n        labs( title = paste(\"QQ-plot of\", var) )\n    plot(g)\n}\n```\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-13-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-13-3.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-13-4.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-13-5.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-13-6.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-13-7.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-13-8.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-13-9.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-13-10.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-13-11.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-13-12.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-13-13.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-13-14.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-13-15.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-13-16.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-13-17.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-13-18.png){width=672}\n:::\n:::\n\n\nAt this point we can see that the distributions are right skewed with many outliers. To finish, we can use statistic tests to be sure if a variable distribution is normal.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnorm.stats <- data.frame()\nfor (var in colnames( select_if( df.clean, is.numeric) ) ) {\n    ks <- ks.test( unlist( df.clean[, var] ), y = pnorm) \n    sh <- shapiro.test( unlist( df.clean[, var]) )\n    vals <- data.frame(\n        ks_test = sprintf(\"%6.5f\" ,c(ks$p.value)),\n        shapiro_test = sprintf(\"%6.5f\", c(sh$p.value) ),\n        row.names = var\n    )\n    norm.stats <- bind_rows(norm.stats, vals)\n}\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in ks.test.default(unlist(df.clean[, var]), y = pnorm): ties should not\nbe present for the one-sample Kolmogorov-Smirnov test\nWarning in ks.test.default(unlist(df.clean[, var]), y = pnorm): ties should not\nbe present for the one-sample Kolmogorov-Smirnov test\nWarning in ks.test.default(unlist(df.clean[, var]), y = pnorm): ties should not\nbe present for the one-sample Kolmogorov-Smirnov test\nWarning in ks.test.default(unlist(df.clean[, var]), y = pnorm): ties should not\nbe present for the one-sample Kolmogorov-Smirnov test\nWarning in ks.test.default(unlist(df.clean[, var]), y = pnorm): ties should not\nbe present for the one-sample Kolmogorov-Smirnov test\nWarning in ks.test.default(unlist(df.clean[, var]), y = pnorm): ties should not\nbe present for the one-sample Kolmogorov-Smirnov test\nWarning in ks.test.default(unlist(df.clean[, var]), y = pnorm): ties should not\nbe present for the one-sample Kolmogorov-Smirnov test\nWarning in ks.test.default(unlist(df.clean[, var]), y = pnorm): ties should not\nbe present for the one-sample Kolmogorov-Smirnov test\nWarning in ks.test.default(unlist(df.clean[, var]), y = pnorm): ties should not\nbe present for the one-sample Kolmogorov-Smirnov test\nWarning in ks.test.default(unlist(df.clean[, var]), y = pnorm): ties should not\nbe present for the one-sample Kolmogorov-Smirnov test\n```\n\n\n:::\n\n```{.r .cell-code}\nremove(ks)\nremove(sh)\nremove(vals)\nnorm.stats\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                     ks_test shapiro_test\nRank                 0.00000      0.00003\nTotal                0.00000      0.00000\nMen                  0.00000      0.00000\nWomen                0.00000      0.00000\nShareWomen           0.00000      0.00433\nSample_size          0.00000      0.00000\nEmployed             0.00000      0.00000\nFull_time            0.00000      0.00000\nPart_time            0.00000      0.00000\nFull_time_year_round 0.00000      0.00000\nUnemployed           0.00000      0.00000\nUnemployment_rate    0.00000      0.01973\nMedian               0.00000      0.00000\nP25th                0.00000      0.00000\nP75th                0.00000      0.00000\nCollege_jobs         0.00000      0.00000\nNon_college_jobs     0.00000      0.00000\nLow_wage_jobs        0.00000      0.00000\n```\n\n\n:::\n:::\n\n\n## Multivariate Analysis\n\n> In a project, the idea is to explore how the explanatory variables (X) affect the response variable (y).\n\nHere we want to know how the variables affect to 'unemployment_rate'. For this task could be useful the scatter-plot to see the pattern created when two variables are compared. And the correlation coefficient which tell us how much relation they have.\n\n-   `ggpairs()`: from `GGally` returns a matrix with scatterplot and correlations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf.clean %>%\n    select(Men, Women, Part_time, Unemployment_rate, \n           Non_college_jobs, Low_wage_jobs) %>%\n    ggpairs() \n```\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\nThere are only select some variables, it have been discarded the character ones and cherry-pick variables with some correlation. The full correlation matrix could not be seen.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncorrs <- round( cor(df.clean[, -c(1, 2, 3, 7)]), 3)\ncorrplot (corrs, method = \"number\", type = \"lower\", \n          tl.cex = 0.8, number.cex = 0.6)\n```\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\nLooking into 'Unemployment_rate' the correlations are so lightly, in this case a linear regression would not be the best approach.\n\nWhen two explanatory variables have high correlations means both variables can explain the same variance in the target variable, creating redundancy and making it more difficult to determine the effect of each individual explanatory feature. Then, it must be eliminated before modeling.\n\n## Exploring\n\nWhen we want to explore more deeply the dataset we can start doing questions and looking for answers:\n\n-   What are the top 10 majors with the lower unemployment rate?\n-   And with the higher unemployment rate?\n-   What are the majors with more specialized jobs (requiring college)?\n-   What are the best median-value-paying jobs?\n-   Do the majors with more share of women enrolled have a higher or lower unemployed rate?\n-   Do the majors with more share of women have higher rate of part time workers?\n-   Do the majors with a greater share of women enrolled have a similar salary median?\n\n#### What are the top 10 majors with the lower unemployment rate? {.unnumbered}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Selecting top 10\ntop10.hig.emp <- df.clean %>%\n  select(Major, Unemployment_rate) %>%\n  arrange( desc(Unemployment_rate) ) %>%\n  head(10)\n\n# Selecting top 10\ntop10.low.emp <- df.clean %>%\n  select(Major, Unemployment_rate) %>%\n  arrange( Unemployment_rate ) %>%\n  head(10)\ntop10.low.emp\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 2\n   Major                                      Unemployment_rate\n   <fct>                                                  <dbl>\n 1 MATHEMATICS AND COMPUTER SCIENCE                     0      \n 2 MILITARY TECHNOLOGIES                                0      \n 3 BOTANY                                               0      \n 4 SOIL SCIENCE                                         0      \n 5 EDUCATIONAL ADMINISTRATION AND SUPERVISION           0      \n 6 ENGINEERING MECHANICS PHYSICS AND SCIENCE            0.00633\n 7 COURT REPORTING                                      0.0117 \n 8 MATHEMATICS TEACHER EDUCATION                        0.0162 \n 9 PETROLEUM ENGINEERING                                0.0184 \n10 GENERAL AGRICULTURE                                  0.0196 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot top 10 higher Unemployment rate\nggplot(top10.hig.emp) +\n  geom_col( aes(x = Unemployment_rate,\n                y = reorder( Major, +Unemployment_rate) ),\n            color = \"darkblue\", fill = \"lightblue\" ) +\n  labs( y = \"Major\" ) +\n  geom_text( aes( x = Unemployment_rate,\n                  y = Major, label = round(Unemployment_rate, 3) ),\n             size = 2, hjust = 1.2) +\n  ggtitle(\"Unemployment rate by Major\")\n```\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot top 10 lower Unemployment rate\nggplot(top10.low.emp) +\n  geom_col( aes(x = Unemployment_rate,\n                y = reorder( Major, -Unemployment_rate) ),\n            color = \"darkblue\", fill = \"lightblue\" ) +\n  labs( y = \"Major\" ) +\n  geom_text( aes( x = Unemployment_rate,\n                  y = Major, label = round(Unemployment_rate, 3) ),\n             size = 2, hjust = 1.2) +\n  ggtitle(\"Unemployment rate by Major\")\n```\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\nThe first graph of the top 10 unemployment rate looks fine but the second one not at all. Let's try the same but using the proportion of total employees for ordering the data within unemployment rate.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntop10.hig.prop <- df.clean %>%\n  mutate (prop = Total / sum(Total) ) %>%\n  select (Major, Unemployment_rate, prop) %>%\n  arrange(desc(prop), desc(Unemployment_rate)) %>%\n  head(10)\n\n# plot top 10 higher Unemployment rate by proportion\nggplot(top10.hig.prop) +\n  geom_col( aes(x = Unemployment_rate,\n                y = reorder( Major, +Unemployment_rate) ),\n            color = \"darkblue\", fill = \"lightblue\" ) +\n  labs( y = \"Major\" ) +\n  geom_text( aes( x = Unemployment_rate,\n                  y = Major, label = round(Unemployment_rate, 3) ),\n             size = 2, hjust = 1.2) +\n  ggtitle(\"Unemployment rate by Major\")\n```\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntop10.low.prop <- df.clean %>%\n  mutate (prop = Total / sum(Total) ) %>%\n  select (Major, Unemployment_rate, prop) %>%\n  arrange(desc(prop), Unemployment_rate) %>%\n  head(10)\n\n# plot top 10 lower Unemployment rate by proportion\nggplot(top10.low.prop) +\n  geom_col( aes(x = Unemployment_rate,\n                y = reorder( Major, -Unemployment_rate) ),\n            color = \"darkblue\", fill = \"lightblue\" ) +\n  labs( y = \"Major\" ) +\n  geom_text( aes( x = Unemployment_rate,\n                  y = Major, label = round(Unemployment_rate, 3) ),\n             size = 2, hjust = 1.2) +\n  ggtitle(\"Unemployment rate by Major\")\n```\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\nIt would not be necessary the second visualization, It have the same values because it is ordered by proportion. It show the most popular majors ordered by employment rate, and nursing with the best rate.\n\n#### More specialized jobs {.unnumbered}\n\nJust like the last time, we are going to need new columns such as proportions of college jobs:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf.clean <- df.clean %>%\n  mutate ( College_jobs_pct = College_jobs / (College_jobs + Non_college_jobs),\n           Employees_pct = Total / sum(Total) )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndf.clean %>%\n  select(Major_category, Major, College_jobs_pct) %>%\n  group_by(Major_category) %>%\n  summarize(mean_college_jobs_pct = mean(College_jobs_pct)) %>%\n  arrange(desc(mean_college_jobs_pct))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 16 × 2\n   Major_category                      mean_college_jobs_pct\n   <fct>                                               <dbl>\n 1 Education                                           0.714\n 2 Engineering                                         0.680\n 3 Computers & Mathematics                             0.604\n 4 Biology & Life Science                              0.593\n 5 Interdisciplinary                                   0.570\n 6 Physical Sciences                                   0.532\n 7 Health                                              0.516\n 8 Psychology & Social Work                            0.508\n 9 Agriculture & Natural Resources                     0.406\n10 Humanities & Liberal Arts                           0.386\n11 Social Science                                      0.375\n12 Communications & Journalism                         0.348\n13 Arts                                                0.328\n14 Law & Public Policy                                 0.324\n15 Business                                            0.297\n16 Industrial Arts & Consumer Services               NaN    \n```\n\n\n:::\n:::\n\n\n#### Best median-value-paying jobs {.unnumbered}\n\nFirst we have to add a variable for median salaries grouped by major category, and then use that to generate a boxplot visualization for comparison.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf.clean <- df.clean %>%\n  group_by(Major_category) %>%\n  mutate( Salary_mdn = median(Median) )\n\nggplot(data = df.clean) +\n  geom_boxplot( aes( x = reorder(Major_category, Salary_mdn),\n                     y = Median),\n                fill = \"lightblue\", color = \"darkblue\") +\n  labs( x = \"Major_category\" ) +\n  ggtitle (\"Median by Major Category\") +\n  theme( axis.text.x = element_text( angle = 45, hjust = 1.2 ) ) +\n  expand_limits( x = c(0, NA), y = c(0, NA) ) +\n  scale_y_continuous( labels = scales::comma )\n```\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n\nWe can observe that Engineering, Maths, Business, Physics are the top paying jobs.\n\nOther approaching is to do multiple comparison (post-hoc) of the median.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npairwise.wilcox.test(df.clean$Median, df.clean$Major_category, p.adjust.method = \"holm\") %>% suppressWarnings()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPairwise comparisons using Wilcoxon rank sum test with continuity correction \n\ndata:  df.clean$Median and df.clean$Major_category \n\n                                    Agriculture & Natural Resources Arts \nArts                                1.000                           -    \nBiology & Life Science              1.000                           1.000\nBusiness                            0.801                           0.352\nCommunications & Journalism         1.000                           1.000\nComputers & Mathematics             0.295                           0.542\nEducation                           1.000                           1.000\nEngineering                         0.002                           0.007\nHealth                              1.000                           1.000\nHumanities & Liberal Arts           1.000                           1.000\nIndustrial Arts & Consumer Services 1.000                           1.000\nInterdisciplinary                   1.000                           1.000\nLaw & Public Policy                 1.000                           1.000\nPhysical Sciences                   1.000                           0.759\nPsychology & Social Work            1.000                           1.000\nSocial Science                      1.000                           1.000\n                                    Biology & Life Science Business\nArts                                -                      -       \nBiology & Life Science              -                      -       \nBusiness                            0.884                  -       \nCommunications & Journalism         1.000                  0.988   \nComputers & Mathematics             0.451                  1.000   \nEducation                           0.428                  0.008   \nEngineering                         0.000066               0.042   \nHealth                              1.000                  1.000   \nHumanities & Liberal Arts           0.367                  0.007   \nIndustrial Arts & Consumer Services 1.000                  1.000   \nInterdisciplinary                   1.000                  1.000   \nLaw & Public Policy                 1.000                  1.000   \nPhysical Sciences                   1.000                  1.000   \nPsychology & Social Work            0.884                  0.063   \nSocial Science                      1.000                  1.000   \n                                    Communications & Journalism\nArts                                -                          \nBiology & Life Science              -                          \nBusiness                            -                          \nCommunications & Journalism         -                          \nComputers & Mathematics             0.655                      \nEducation                           1.000                      \nEngineering                         0.143                      \nHealth                              1.000                      \nHumanities & Liberal Arts           1.000                      \nIndustrial Arts & Consumer Services 1.000                      \nInterdisciplinary                   1.000                      \nLaw & Public Policy                 1.000                      \nPhysical Sciences                   1.000                      \nPsychology & Social Work            1.000                      \nSocial Science                      1.000                      \n                                    Computers & Mathematics Education\nArts                                -                       -        \nBiology & Life Science              -                       -        \nBusiness                            -                       -        \nCommunications & Journalism         -                       -        \nComputers & Mathematics             -                       -        \nEducation                           0.004                   -        \nEngineering                         0.025                   0.000007 \nHealth                              1.000                   1.000    \nHumanities & Liberal Arts           0.005                   1.000    \nIndustrial Arts & Consumer Services 1.000                   1.000    \nInterdisciplinary                   1.000                   1.000    \nLaw & Public Policy                 1.000                   0.277    \nPhysical Sciences                   1.000                   0.014    \nPsychology & Social Work            0.057                   1.000    \nSocial Science                      1.000                   1.000    \n                                    Engineering Health\nArts                                -           -     \nBiology & Life Science              -           -     \nBusiness                            -           -     \nCommunications & Journalism         -           -     \nComputers & Mathematics             -           -     \nEducation                           -           -     \nEngineering                         -           -     \nHealth                              0.000499    -     \nHumanities & Liberal Arts           0.000011    0.867 \nIndustrial Arts & Consumer Services 0.023       1.000 \nInterdisciplinary                   1.000       1.000 \nLaw & Public Policy                 0.899       1.000 \nPhysical Sciences                   0.054       1.000 \nPsychology & Social Work            0.001       1.000 \nSocial Science                      0.004       1.000 \n                                    Humanities & Liberal Arts\nArts                                -                        \nBiology & Life Science              -                        \nBusiness                            -                        \nCommunications & Journalism         -                        \nComputers & Mathematics             -                        \nEducation                           -                        \nEngineering                         -                        \nHealth                              -                        \nHumanities & Liberal Arts           -                        \nIndustrial Arts & Consumer Services 1.000                    \nInterdisciplinary                   1.000                    \nLaw & Public Policy                 0.323                    \nPhysical Sciences                   0.017                    \nPsychology & Social Work            1.000                    \nSocial Science                      0.755                    \n                                    Industrial Arts & Consumer Services\nArts                                -                                  \nBiology & Life Science              -                                  \nBusiness                            -                                  \nCommunications & Journalism         -                                  \nComputers & Mathematics             -                                  \nEducation                           -                                  \nEngineering                         -                                  \nHealth                              -                                  \nHumanities & Liberal Arts           -                                  \nIndustrial Arts & Consumer Services -                                  \nInterdisciplinary                   1.000                              \nLaw & Public Policy                 1.000                              \nPhysical Sciences                   1.000                              \nPsychology & Social Work            1.000                              \nSocial Science                      1.000                              \n                                    Interdisciplinary Law & Public Policy\nArts                                -                 -                  \nBiology & Life Science              -                 -                  \nBusiness                            -                 -                  \nCommunications & Journalism         -                 -                  \nComputers & Mathematics             -                 -                  \nEducation                           -                 -                  \nEngineering                         -                 -                  \nHealth                              -                 -                  \nHumanities & Liberal Arts           -                 -                  \nIndustrial Arts & Consumer Services -                 -                  \nInterdisciplinary                   -                 -                  \nLaw & Public Policy                 1.000             -                  \nPhysical Sciences                   1.000             1.000              \nPsychology & Social Work            1.000             1.000              \nSocial Science                      1.000             1.000              \n                                    Physical Sciences Psychology & Social Work\nArts                                -                 -                       \nBiology & Life Science              -                 -                       \nBusiness                            -                 -                       \nCommunications & Journalism         -                 -                       \nComputers & Mathematics             -                 -                       \nEducation                           -                 -                       \nEngineering                         -                 -                       \nHealth                              -                 -                       \nHumanities & Liberal Arts           -                 -                       \nIndustrial Arts & Consumer Services -                 -                       \nInterdisciplinary                   -                 -                       \nLaw & Public Policy                 -                 -                       \nPhysical Sciences                   -                 -                       \nPsychology & Social Work            0.276             -                       \nSocial Science                      1.000             0.775                   \n\nP value adjustment method: holm \n```\n\n\n:::\n:::\n\n\nOn the last output we look for p \\< 0.05 meaning there are differences between the medians of that groups. With the boxplot we can see which are greater visually but with these comparisons we can actually know which is greater than another statistically.\n\n#### Relatioship between Majors with more share of women and unemployed rate {.unnumbered}\n\nThis task should be do it with correlations, and then we can plot to see its graphic representations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#cor(df.clean$ShareWomen, df.clean$Unemployment_rate, method = \"pearson\")\n\ncor(df.clean$ShareWomen, df.clean$Unemployment_rate,\n    method = \"spearman\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0663\n```\n\n\n:::\n:::\n\n\nThe spearman correlation is non-parametric and it is suitable for when the relationship of two variables are non-linear, or do not follow the norm distribution.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot( data = df.clean ) +\n  geom_point( aes( x = ShareWomen, y = Unemployment_rate ),\n              color = \"darkblue\") +\n  ggtitle (\"Share of women vs. Unemployment rate\") +\n  labs ( subtitle = \"There is no linear relationship, the graphic is spread\", color = \"darkgray\", size = 8) +\n  theme( plot.subtitle = element_text( color = \"darkgray\", \n                                               size = 10) )\n```\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n:::\n\n\n#### Do the majors with more share of women have higher rate of part time workers? {.unnumbered}\n\nJust like the last question, we can see the relationship of this two variables with a correlation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(df.clean$ShareWomen, df.clean$Part_time, method = \"spearman\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.3348\n```\n\n\n:::\n:::\n\n\nIt can be seen a medium-slight positive correlation between both Share of woman enrolled and workers with part time contract. This is how looks like the correlation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = df.clean) +\n    geom_point( aes( x = ShareWomen, y = Part_time),\n                color = \"darkblue\") +\n    labs( title = \"Share of Women vs. Part-Time jobs\",\n          subtitle = \"The relationship between the variables is slightly weak [0.33],\\n siggesting that when the share of women is higher also there are more part-jobs\") +\n    theme( plot.subtitle = element_text( color = \"darkgray\", size = 8) )\n```\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-29-1.png){width=672}\n:::\n:::\n\n\n#### Majors with a greater share of women and salary median {.unnumbered}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(df.clean$ShareWomen, df.clean$Median)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.6187\n```\n\n\n:::\n\n```{.r .cell-code}\ncor(df.clean$ShareWomen, df.clean$Low_wage_jobs)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1878\n```\n\n\n:::\n:::\n\n\nThe first correlation show when the greater is the share of women enrolled, the less are the Median salary; and in the second correlation, almost without strength if the share of women is high there are a slight relationship (almost none) with more low wage jobs.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = df.clean) +\n    geom_point( aes( x = ShareWomen, y = Low_wage_jobs, \n                     color = \"Low Wage Jobs\"),\n                alpha = 1, shape = \"o\", size = 2) +\n    geom_point( aes( x = ShareWomen, y = Median, \n                     color = \"Median Salary\"), alpha = 0.5) +\n    labs(title = \"Share of women vs. low wage, and vs. Median Salary\",\n         y = \"\", color = \"Share of Women vs.:\") +\n    scale_color_manual(values = c(\"Low Wage Jobs\" = \"darkblue\", \n                                  \"Median Salary\" = \"darkred\") ) +\n    theme(legend.title = element_text())\n```\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-31-1.png){width=672}\n:::\n:::\n\n\nThe next step could be to test if the median of lower share of women is greater or not than higher share of women. To verify this we can run hypothesis tests. First we are going to add a column with two categories, one for higher share and other for lower share of women. Then we have to be sure which test we can use, for that we have to check the normality and if its fine, the homoscedasticity.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf.clean <- df.clean %>% \n    mutate( ShareWomen_cat = ifelse(ShareWomen > 0.5, \"higher\", \"lower\") )\n\ndf.higher.w <- df.clean %>%\n    filter( ShareWomen_cat == \"higher\") %>%\n    select( Major, Major_category, ShareWomen, \n            ShareWomen_cat, Low_wage_jobs, Median)\n\ndf.lower.w <- df.clean %>%\n    filter( ShareWomen_cat == \"lower\") %>%\n    select( Major, Major_category, ShareWomen, \n            ShareWomen_cat, Low_wage_jobs, Median)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nks.test( df.higher.w$Low_wage_jobs, \"pnorm\") %>% suppressWarnings()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tExact one-sample Kolmogorov-Smirnov test\n\ndata:  df.higher.w$Low_wage_jobs\nD = 0.99, p-value = 0.000000000000001\nalternative hypothesis: two-sided\n```\n\n\n:::\n\n```{.r .cell-code}\nks.test( df.lower.w$Low_wage_jobs, \"pnorm\" ) %>% suppressWarnings()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tAsymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  df.lower.w$Low_wage_jobs\nD = 0.95, p-value <0.0000000000000002\nalternative hypothesis: two-sided\n```\n\n\n:::\n\n```{.r .cell-code}\nks.test( df.higher.w$Median, \"pnorm\" ) %>% suppressWarnings()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tAsymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  df.higher.w$Median\nD = 1, p-value <0.0000000000000002\nalternative hypothesis: two-sided\n```\n\n\n:::\n\n```{.r .cell-code}\nks.test( df.lower.w$Median, \"pnorm\" ) %>% suppressWarnings()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tAsymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  df.lower.w$Median\nD = 1, p-value <0.0000000000000002\nalternative hypothesis: two-sided\n```\n\n\n:::\n:::\n\n\nIn the las output we show that every test was lower than 0.05 thus there are no normality on these groups. Let's check homoscedasticity:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(df.clean, car::leveneTest(Median, ShareWomen_cat) )  %>% suppressWarnings()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value    Pr(>F)    \ngroup   1    25.4 0.0000012 ***\n      170                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\nwith(df.clean, car::leveneTest(Low_wage_jobs, ShareWomen_cat) )  %>% suppressWarnings()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value Pr(>F)\ngroup   1    1.86   0.17\n      170               \n```\n\n\n:::\n:::\n\n\nThe first result for Median on ShareWomen_cat levels have p \\< 0.05 then we reject the Null Hypothesis; nevertheless, the second result for Low_wage_jobs on ShareWomen_cat levels the p is 0.17, then we maintain the Null Hypothesis of Homogeneity of Variances.\n\nNow we know that for compare Median we will need a robust test, but with Low_wage_jobs we might use t-test because the sample is greater than 50 and that will not affect so much the results.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwilcox.test(df.higher.w$Median, df.lower.w$Median)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWilcoxon rank sum test with continuity correction\n\ndata:  df.higher.w$Median and df.lower.w$Median\nW = 1142, p-value = 0.00000000000001\nalternative hypothesis: true location shift is not equal to 0\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Median of Higher Share Woman: \", median(df.higher.w$Median),\n    \"\\nMedian of Lower Share Woman: \", median(df.lower.w$Median), \"\\n\" )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMedian of Higher Share Woman:  34000 \nMedian of Lower Share Woman:  45000 \n```\n\n\n:::\n\n```{.r .cell-code}\nt.test(df.higher.w$Low_wage_jobs, df.lower.w$Low_wage_jobs)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  df.higher.w$Low_wage_jobs and df.lower.w$Low_wage_jobs\nt = 1.8, df = 170, p-value = 0.07\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -162.2 3917.3\nsample estimates:\nmean of x mean of y \n     4708      2831 \n```\n\n\n:::\n:::\n\n\nFor Median there are differences between the median of the groups, but for Low Wage seem like there are not differences between the mean of the groups with this sample.\n\nLet's try another approach. Now using rep_sample_n() we are going to take 100 samples of 1000 observations allowing repetitions, and then taking the mean.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf.higher.samp <- df.higher.w %>%\n    rep_sample_n( size = 1000, reps = 200, replace = TRUE ) %>%\n    summarize( mu_Median = mean(Median), \n               mu_Low_wage = mean(Low_wage_jobs) )\n\ndf.lower.samp <- df.lower.w %>%\n    rep_sample_n( size = 1000, reps = 200, replace = TRUE ) %>%\n    summarize( mu_Median = mean(Median), \n               mu_Low_wage = mean(Low_wage_jobs) )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nshapiro.test(df.lower.samp$mu_Median)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  df.lower.samp$mu_Median\nW = 1, p-value = 0.8\n```\n\n\n:::\n\n```{.r .cell-code}\nshapiro.test(df.higher.samp$mu_Median)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  df.higher.samp$mu_Median\nW = 0.99, p-value = 0.1\n```\n\n\n:::\n\n```{.r .cell-code}\nshapiro.test(df.lower.samp$mu_Low_wage)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  df.lower.samp$mu_Low_wage\nW = 0.99, p-value = 0.2\n```\n\n\n:::\n\n```{.r .cell-code}\nshapiro.test(df.higher.samp$mu_Low_wage)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  df.higher.samp$mu_Low_wage\nW = 0.99, p-value = 0.5\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(df.higher.samp$mu_Median, df.lower.samp$mu_Median)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  df.higher.samp$mu_Median and df.lower.samp$mu_Median\nt = -420, df = 279, p-value <0.0000000000000002\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -12434 -12318\nsample estimates:\nmean of x mean of y \n    34619     46995 \n```\n\n\n:::\n\n```{.r .cell-code}\nt.test(df.higher.samp$mu_Low_wage, df.lower.samp$mu_Low_wage)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  df.higher.samp$mu_Low_wage and df.lower.samp$mu_Low_wage\nt = 87, df = 373, p-value <0.0000000000000002\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 1850 1935\nsample estimates:\nmean of x mean of y \n     4725      2832 \n```\n\n\n:::\n:::\n\n\nBoth p-value are close to zero, thus we can reject the null hypothesis of t-test. Then we can say the groups mean are different with this sample.\n\n> We can infer with 95% confidence that the majors with 50% or more of those enrolled being women have, on average, more low-wage jobs and less median salary.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf.salary.w <- df.clean %>%\n  group_by(ShareWomen_cat) %>%\n  summarize(Salary_mean = mean(Median) )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot( data = df.clean ) +\n  geom_boxplot( aes( x = ShareWomen_cat, y = Median), \n                fill = \"lightblue\", color = \"darkblue\" ) +\n  labs( title = \"Average Salary When \n        ShareWomen is lower/higher than 50%\",\n        subtitle = \"The average salary for majors with more women enrolled \\n\n        is lower than the majors with less women, reinforcing the perception \\n \n        that women are getting lower salaries.\",\n        x = \"50% Share of Women\",\n        y = \"Mean Salary\" ) +\n  geom_text( data = df.salary.w, aes( x = ShareWomen_cat,\n                                      y = Salary_mean,\n                                      label = round(Salary_mean)),\n             size = 2.2, vjust = 1, color = \"black\" ) +\n  theme(plot.subtitle = element_text( color = \"darkgray\", size = 9 ) )\n```\n\n::: {.cell-output-display}\n![](12_ExpDatAn_files/figure-html/unnamed-chunk-40-1.png){width=672}\n:::\n:::\n\n\nTo close this Exploratory Data Analysis we just need to compose the Analysis report.\n\n# Analysis Report\n\nThis Data Science project is about the EDA, composed of data wrangling and exploration to answer business questions and extract insights to support the decision-making process. When we write a Analysis Report we have to highlight the findings:\n\n------------------------------------------------------------------------\n\nDataset: College Majors (<https://github.com/fivethirtyeight/data/tree/master/college-majors>)\n\nDimensions: 173 observations, 21 variables\n\nData dictionary:\n\n| Header                 | Type        | Description                                                                |\n|-----------------|----------------|----------------------------------------|\n| `Rank`                 | Ordinal     | Rank by median earnings                                                    |\n| `Major_code`           | Categorical | Major code, FO1DP in ACS PUMS                                              |\n| `Major`                | Categorical | Major description                                                          |\n| `Major_category`       | Categorical | Category of major from Carnevale et al                                     |\n| `Total`                | Numeric     | Total number of people with major                                          |\n| `Sample_size`          | Numeric     | Sample size (unweighted) of full-time, year-round ONLY (used for earnings) |\n| `Men`                  | Numeric     | Male graduates                                                             |\n| `Women`                | Numeric     | Female graduates                                                           |\n| `ShareWomen`           | Numeric     | Women as share of total                                                    |\n| `Employed`             | Numeric     | Number employed (ESR == 1 or 2)                                            |\n| `Full_time`            | Numeric     | Employed 35 hours or more                                                  |\n| `Part_time`            | Numeric     | Employed less than 35 hours                                                |\n| `Full_time_year_round` | Numeric     | Employed at least 50 weeks (WKW == 1) and at least 35 hours (WKHP \\>= 35)  |\n| `Unemployed`           | Numeric     | Number unemployed (ESR == 3)                                               |\n| `Unemployment_rate`    | Numeric     | Unemployed / (Unemployed + Employed)                                       |\n| `Median`               | Numeric     | Median earnings of full-time, year-round workers                           |\n| `P25th`                | Numeric     | 25th percentile of earnings                                                |\n| `P75th`                | Numeric     | 75th percentile of earnings                                                |\n| `College_jobs`         | Numeric     | Number with job requiring a college degree                                 |\n| `Non_college_jobs`     | Numeric     | Number with job not requiring a college degree                             |\n| `Low_wage_jobs`        | Numeric     | Number in low-wage service jobs                                            |\n\nMissing Data: There is one observation with missing data on Men, Women, ShareWomen and Total variables. The Major with this missing observation is FOOD SCIENCE (code: 1104).\n\n## Report {.unnumbered}\n\nEDA of a complete dataset, the missing values are less than 1% which was excluded to preserve the original data only without adding calculated numbers.\n\nThe descriptive statistics show high variances due to the presence of outliers. The women in college represents the 52%. The unemployment rate is around 6% presenting high variance.\n\nAre jobs more specialized which require a degree such as Education, Maths, Engineering. The engineering is the top-paying category followed by Computer and Maths, Physical Science, and Business. Nevertheless, there are no statistical differences between categories on salary median except comparing Engineering which is statistically higher.\n\nComparing gender it can be shown that majors with more women enrolled have more low wage jobs and also majors with more share of women seems to have lower salaries. Moreover, the unemployment rate are lower in majors with more share of women.\n\n## Next steps {.unnumbered}\n\nIf further analysis are required which needs modeling, for linear regression attention has to be paid to multicollinear variables which can affect its reliability.\n\n---\n\nThe last text was an example of Analysis report, it could be filled with more content if its necessary. But this is the idea.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "supporting": [
      "12_ExpDatAn_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}